LangChain ä¸­çš„å„ç§é¢„è®¾ Chain èƒ½å¸®æˆ‘ä»¬è½»æ¾æ­å»ºå¤„ç†å¤æ‚ä»»åŠ¡çš„æµç¨‹ã€‚ä¸‹é¢é€šè¿‡ 5 ä¸ªå®é™…é¡¹ç›®æ¡ˆä¾‹ï¼Œå¸¦ä½ çœ‹çœ‹å®ƒä»¬æ€ä¹ˆç”¨ï¼Œå¹¶æä¾›ä»£ç ç¤ºä¾‹ã€‚

# ğŸ§© ä¸€ã€è·¯ç”±é“¾ï¼ˆRouterChainï¼‰ï¼šæ™ºèƒ½å®¢æœé—®é¢˜åˆ†ç±»

**åœºæ™¯æè¿°**ï¼š  
ä¸€ä¸ªé²œèŠ±ç”µå•†çš„æ™ºèƒ½å®¢æœç³»ç»Ÿéœ€è¦å¤„ç†ä¸¤ç±»é—®é¢˜ï¼š**é²œèŠ±å…»æŠ¤**ï¼ˆå¦‚æµ‡æ°´ã€æ–½è‚¥ï¼‰å’Œ**é²œèŠ±è£…é¥°**ï¼ˆå¦‚æ­é…ã€åœºåœ°å¸ƒç½®ï¼‰ã€‚éœ€è¦æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹ï¼Œè‡ªåŠ¨è·¯ç”±ç»™ä¸åŒçš„ä¸“ä¸šæ¨¡å‹å¤„ç†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š  
ä½¿ç”¨ `LLMRouterChain` å’Œ `MultiPromptChain` æ„å»ºè·¯ç”±é“¾ã€‚è·¯ç”±å™¨é“¾åˆ†æè¾“å…¥é—®é¢˜ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æç¤ºæ¨¡æ¿ï¼Œç„¶åå°†é—®é¢˜å‘é€ç»™å¯¹åº”çš„ç›®æ ‡é“¾å¤„ç†ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.llms import OpenAI
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

# 1. ä¸ºä¸åŒåœºæ™¯å®šä¹‰æç¤ºæ¨¡æ¿
flower_care_template = """ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å›­ä¸ï¼Œæ“…é•¿è§£ç­”å…³äºå…»èŠ±è‚²èŠ±çš„é—®é¢˜ã€‚
                        ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                        {input}"""

flower_deco_template = """ä½ æ˜¯ä¸€ä½ç½‘çº¢æ’èŠ±å¤§å¸ˆï¼Œæ“…é•¿è§£ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜ã€‚
                        ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                        {input}"""

# 2. æ„å»ºæç¤ºä¿¡æ¯åˆ—è¡¨
prompt_infos = [
    {
        "key": "flower_care",
        "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±æŠ¤ç†çš„é—®é¢˜",
        "template": flower_care_template,
    },
    {
        "key": "flower_decoration",
        "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜",
        "template": flower_deco_template,
    }]

# 3. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
llm = OpenAI(temperature=0)

# 4. æ„å»ºç›®æ ‡é“¾å­—å…¸
chain_map = {}
for info in prompt_infos:
    prompt = PromptTemplate(template=info['template'], input_variables=["input"])
    chain = LLMChain(llm=llm, prompt=prompt)
    chain_map[info["key"]] = chain

# 5. æ„å»ºè·¯ç”±é“¾
destinations = [f"{p['key']}: {p['description']}" for p in prompt_infos]
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations="\n".join(destinations))
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)
router_chain = LLMRouterChain.from_llm(llm, router_prompt)

# 6. æ„å»ºé»˜è®¤é“¾ï¼ˆå¤„ç†æœªåˆ†ç±»é—®é¢˜ï¼‰
default_chain = LLMChain(llm=llm, prompt=PromptTemplate(template="{input}", input_variables=["input"]))

# 7. ä½¿ç”¨è·¯ç”±é“¾ï¼ˆè¿™é‡Œç®€åŒ–äº†MultiPromptChainçš„è°ƒç”¨é€»è¾‘ï¼‰
# å®é™…é¡¹ç›®ä¸­ï¼Œå¯ä½¿ç”¨LangChainçš„MultiPromptChainç±»
def route_question(question):
    # è·¯ç”±é“¾å†³å®šç›®çš„åœ°
    destination_info = router_chain.invoke(question)
    destination = destination_info["destination"]
    # æ ¹æ®ç›®çš„åœ°é€‰æ‹©é“¾å¹¶è°ƒç”¨
    if destination in chain_map:
        return chain_map[destination].invoke(question)
    else:
        return default_chain.invoke(question)

# æµ‹è¯•
care_result = route_question("ç«ç‘°èŠ±åº”è¯¥å¤šä¹…æµ‡ä¸€æ¬¡æ°´ï¼Ÿ")
print(care_result)
deco_result = route_question("å©šç¤¼ç°åœºç”¨ç«ç‘°èŠ±å’Œæ»¡å¤©æ˜Ÿæ€ä¹ˆæ­é…ï¼Ÿ")
print(deco_result)
```

# ğŸ”„ äºŒã€é¡ºåºé“¾ï¼ˆSequentialChainï¼‰ï¼šç”¨æˆ·è¯„è®ºåˆ†æä¸å¤šè¯­è¨€å›å¤

**åœºæ™¯æè¿°**ï¼š  
ç”µå•†å¹³å°éœ€è¦è‡ªåŠ¨åŒ–å¤„ç†å¤šè¯­è¨€ç”¨æˆ·è¯„è®ºï¼šå…ˆç¿»è¯‘æˆè‹±è¯­ï¼Œæ€»ç»“æ‘˜è¦ï¼Œè¯†åˆ«åŸè¯­è¨€ï¼Œå†ç”¨åŸè¯­è¨€ç”Ÿæˆå›å¤ï¼Œæœ€åå°†å›å¤ç¿»è¯‘æˆä¸­æ–‡è®°å½•ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š  
ä½¿ç”¨ `SequentialChain` å°†å¤šä¸ª `LLMChain` è¿æ¥èµ·æ¥ï¼Œæ¯ä¸ªé“¾å¤„ç†ç‰¹å®šæ­¥éª¤ï¼Œå¹¶å°†è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’ç»™åç»­é“¾ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain, SequentialChain

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(temperature=0.7)

# å­é“¾1ï¼šå°†ä¸­æ–‡è¯„è®ºç¿»è¯‘æˆè‹±æ–‡
prompt_z2e = ChatPromptTemplate.from_template("å°†ä¸‹é¢çš„ä¸­æ–‡è¯„è®ºç¿»è¯‘ä¸ºè‹±æ–‡ï¼š\n\n{ch_review}")
chain_z2e = LLMChain(llm=llm, prompt=prompt_z2e, output_key="en_review")

# å­é“¾2ï¼šæ€»ç»“è‹±æ–‡è¯„è®º
prompt_es = ChatPromptTemplate.from_template("Can you summarize the following review in 1 sentence: \n\n{en_review}")
chain_es = LLMChain(llm=llm, prompt=prompt_es, output_key="summary")

# å­é“¾3ï¼šè¯†åˆ«è¯„è®ºåŸè¯­è¨€
prompt_lang = ChatPromptTemplate.from_template("ä¸‹é¢çš„è¯„è®ºä½¿ç”¨çš„æ˜¯ä»€ä¹ˆè¯­è¨€ï¼Ÿ:\n\n{ch_review}")
chain_lang = LLMChain(llm=llm, prompt=prompt_lang, output_key="language")

# å­é“¾4ï¼šç”¨åŸè¯­è¨€ç”Ÿæˆå›å¤
prompt_reply = ChatPromptTemplate.from_template(
    "ä½¿ç”¨æŒ‡å®šè¯­è¨€ç¼–å†™å¯¹ä»¥ä¸‹æ‘˜è¦çš„åç»­å›å¤ï¼š\n\næ‘˜è¦ï¼š{summary}\n\nè¯­è¨€ï¼š{language}"
)
chain_reply = LLMChain(llm=llm, prompt=prompt_reply, output_key="orig_reply")

# å­é“¾5ï¼šå°†å›å¤ç¿»è¯‘æˆä¸­æ–‡
prompt_e2z = ChatPromptTemplate.from_template("å°†ä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š\n\n{orig_reply}")
chain_e2z = LLMChain(llm=llm, prompt=prompt_e2z, output_key="ch_reply")

# æ„å»ºé¡ºåºé“¾
overall_chain = SequentialChain(
    chains=[chain_z2e, chain_es, chain_lang, chain_reply, chain_e2z],
    input_variables=["ch_review"],
    output_variables=["en_review", "summary", "language", "orig_reply", "ch_reply"],
    verbose=True  # æ˜¾ç¤ºè¯¦ç»†æ‰§è¡Œè¿‡ç¨‹
)

# æµ‹è¯•
chinese_review = "å®«å´éªä»¥å¾€çš„ä½œå“å‰§ä½œå·¥æ•´ã€å½¢å¼ç»Ÿä¸€ï¼Œè€Œä¸”å¤§å¤šèƒ½è®©è§‚ä¼—æç‚¼å‡ºå‘å–„å‘ç¾çš„ä¸­å¿ƒæ€æƒ³ã€‚å®ƒä»¬å½“ç„¶æ˜¯ç¾å¥½çš„ä½œå“ï¼Œä½†æˆ‘å´ä¸èƒ½ä¿¡ä»»çœŸç©ºçš„ã€è¿‡åº¦çš„ç¾å¥½ã€‚æ›´ä¸ä¿¡ä»»è¿™æ˜¯åˆ›ä½œè€…çµé­‚çš„çœŸå®é¢ã€‚"
result = overall_chain.invoke(chinese_review)
print("æœ€ç»ˆä¸­æ–‡å›å¤:", result['ch_reply'])
```

# ğŸ“Š ä¸‰ã€æ–‡æ¡£é—®ç­”é“¾ï¼ˆcreate_stuff_documents_chainï¼‰ï¼šåŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”

**åœºæ™¯æè¿°**ï¼š  
ä¼ä¸šæœ‰å¤§é‡å†…éƒ¨æ–‡æ¡£ï¼ˆå¦‚äº§å“æ‰‹å†Œã€å…¬å¸æ”¿ç­–ï¼‰ï¼Œå¸Œæœ›æ„å»ºä¸€ä¸ªé—®ç­”ç³»ç»Ÿï¼Œèƒ½æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”å‘˜å·¥çš„é—®é¢˜ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š  
ä½¿ç”¨ `create_stuff_documents_chain`ã€‚å®ƒå°†æä¾›çš„æ–‡æ¡£åˆ—è¡¨å…¨éƒ¨æ ¼å¼åŒ–æˆæç¤ºè¯ï¼Œç„¶åä¼ é€’ç»™ LLM ç”Ÿæˆç­”æ¡ˆã€‚**éœ€è¦æ³¨æ„**ç¡®ä¿æ‰€æœ‰æ–‡æ¡£å†…å®¹æ€»å’Œä¸è¶…è¿‡ LLM çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

# åˆå§‹åŒ–æ¨¡å‹å’Œæç¤ºæ¨¡æ¿
llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = ChatPromptTemplate.from_messages([
    ("system", "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡: {context} \n\n å›ç­”é—®é¢˜: {input}"),
])

# æ„å»ºæ–‡æ¡£é“¾
document_chain = create_stuff_documents_chain(llm, prompt)

# å‡†å¤‡æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿä»æ•°æ®åº“æˆ–æ–‡ä»¶åŠ è½½ï¼‰
docs = [
    Document(page_content="æ°è¥¿å–œæ¬¢çº¢è‰²ï¼Œä½†ä¸å–œæ¬¢é»„è‰²"),
    Document(page_content="è´¾é©¬å°”å–œæ¬¢ç»¿è‰²ï¼Œæœ‰ä¸€ç‚¹å–œæ¬¢çº¢è‰²"),
    Document(page_content="ç›ä¸½å–œæ¬¢ç²‰è‰²å’Œçº¢è‰²")
]

# æé—®å¹¶ä¼ å…¥æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡
question = "å¤§å®¶å–œæ¬¢ä»€ä¹ˆé¢œè‰²?"
answer = document_chain.invoke({"input": question, "context": docs})
print(answer)
```

# ğŸ” å››ã€ä¿¡æ¯æå–é“¾ï¼ˆcreate_extraction_chainï¼‰ï¼šä»æ–‡æœ¬ä¸­ç»“æ„åŒ–æå–ä¿¡æ¯

**åœºæ™¯æè¿°**ï¼š  
ä»æ–°é—»ç¨¿ã€äº§å“æè¿°æˆ–ç”¨æˆ·åé¦ˆç­‰éç»“æ„åŒ–æ–‡æœ¬ä¸­ï¼Œè‡ªåŠ¨åŒ–æå–é¢„å®šä¹‰çš„ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚äººç‰©å±æ€§ã€äº‹ä»¶è¦ç´ ã€äº§å“è§„æ ¼ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š  
ä½¿ç”¨ `create_extraction_chain` å¹¶é…åˆ OpenAI çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚å®šä¹‰ä¸€ä¸ª JSON Schema æ¥æŒ‡å®šè¦æå–çš„å±æ€§åŠå…¶ç±»å‹ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chains import create_extraction_chain
from langchain_openai import ChatOpenAI

# å®šä¹‰è¦æå–çš„ä¿¡æ¯æ¨¡å¼ï¼ˆJSON Schemaï¼‰
schema = {
    "properties": {
        "name": {"type": "string"},
        "height": {"type": "integer"},
        "hair_color": {"type": "string"},
    },
    "required": ["name", "height"],  # å¿…é¡»æå–çš„å­—æ®µ
}

# è¾“å…¥æ–‡æœ¬
input_text = """äºšå†å…‹æ–¯èº«é«˜ 5 è‹±å°ºã€‚å…‹åŠ³è¿ªå¨…æ¯”äºšå†å…‹æ–¯é«˜ 1 è‹±å°ºï¼Œå¹¶ä¸”è·³å¾—æ¯”ä»–æ›´é«˜ã€‚å…‹åŠ³è¿ªå¨…æ˜¯é»‘å‘å¥³éƒï¼Œäºšå†å…‹æ–¯æ˜¯é‡‘å‘å¥³éƒã€‚"""

# åˆå§‹åŒ–æ¨¡å‹å¹¶åˆ›å»ºæå–é“¾
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
extraction_chain = create_extraction_chain(schema, llm)

# æ‰§è¡Œæå–
result = extraction_chain.invoke(input_text)
print(result)
```

# ğŸ§® äº”ã€æ•°å­¦é“¾ï¼ˆLLMMathChainï¼‰ï¼šè§£å†³æ•°å­¦è®¡ç®—é—®é¢˜

**åœºæ™¯æè¿°**ï¼š  
æ•™è‚²ç±»åº”ç”¨æˆ–æ•°æ®åˆ†æå·¥å…·éœ€è¦å¤„ç†ç”¨æˆ·è¾“å…¥çš„æ•°å­¦é—®é¢˜ï¼Œå¹¶å°†è‡ªç„¶è¯­è¨€æè¿°çš„æ•°å­¦é—®é¢˜è½¬æ¢ä¸ºå¯è®¡ç®—çš„è¡¨è¾¾å¼ï¼Œæœ€åç»™å‡ºç­”æ¡ˆå’Œè§£æè¿‡ç¨‹ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š  
ä½¿ç”¨ `LLMMathChain`ã€‚å®ƒå°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ•°å­¦è¡¨è¾¾å¼ï¼Œç„¶åä½¿ç”¨ Python çš„ `numexpr` åº“å®‰å…¨åœ°è®¡ç®—è¡¨è¾¾å¼ç»“æœã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain_openai import OpenAI
from langchain.chains import LLMMathChain

# åˆå§‹åŒ–æ¨¡å‹å’Œæ•°å­¦é“¾
llm = OpenAI(temperature=0)  # æ•°å­¦é—®é¢˜é€šå¸¸è®¾ç½® temperature=0 ä»¥ä¿è¯å‡†ç¡®æ€§
llm_math_chain = LLMMathChain.from_llm(llm)

# æ‰§è¡Œæ•°å­¦è®¡ç®—
result = llm_math_chain.invoke("100 * 20 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ")
print(result)  # è¾“å‡º: {'question': '100 * 20 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ', 'answer': 'Answer: 2100'}

# ä¹Ÿå¯ä»¥å¤„ç†æ›´å¤æ‚çš„é—®é¢˜
complex_result = llm_math_chain.invoke("è®¡ç®—åœ†çš„é¢ç§¯ï¼Œå¦‚æœåŠå¾„æ˜¯5å˜ç±³ã€‚è¯·ä½¿ç”¨3.14ä½œä¸ºåœ†å‘¨ç‡ã€‚")
print(complex_result)
```

> âš ï¸ **æ³¨æ„**ï¼šä½¿ç”¨ `LLMMathChain` éœ€è¦å…ˆå®‰è£… `numexpr` åº“ï¼š
> ```bash
> pip install numexpr
> ```

# ğŸ”„ å…­ã€è½¬æ¢é“¾ï¼ˆTransformChainï¼‰ï¼šæ•°æ®æ ¼å¼è½¬æ¢ä¸å¤„ç†

**åœºæ™¯æè¿°**ï¼š
åœ¨AIåº”ç”¨ä¸­ç»å¸¸éœ€è¦å°†ä¸€ç§æ•°æ®æ ¼å¼è½¬æ¢ä¸ºå¦ä¸€ç§æ ¼å¼ï¼Œæˆ–è€…å¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ã€æ ‡å‡†åŒ–å¤„ç†ã€‚ä¾‹å¦‚å°†ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢è¯­å¥ï¼Œå°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†JSONæ ¼å¼ï¼Œæˆ–è€…å°†å¤æ‚çš„æ•°æ®ç»“æ„ç®€åŒ–ä¸ºæ›´æ˜“å¤„ç†çš„å½¢å¼ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ `TransformChain` å¯ä»¥åœ¨é“¾å¼å¤„ç†è¿‡ç¨‹ä¸­æ’å…¥è‡ªå®šä¹‰çš„æ•°æ®è½¬æ¢é€»è¾‘ã€‚å®ƒæ¥æ”¶è¾“å…¥æ•°æ®ï¼Œåº”ç”¨è½¬æ¢å‡½æ•°ï¼Œç„¶åè¾“å‡ºè½¬æ¢åçš„æ•°æ®ï¼Œæ— ç¼é›†æˆåˆ°æ•´ä¸ªå¤„ç†æµç¨‹ä¸­ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chains import TransformChain, SequentialChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import json
import re

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(temperature=0.3)

# è½¬æ¢é“¾1ï¼šæ¸…ç†ç”¨æˆ·è¾“å…¥
def clean_input(inputs):
    """æ¸…ç†å’Œæ ‡å‡†åŒ–ç”¨æˆ·è¾“å…¥"""
    text = inputs["raw_input"]
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    cleaned = re.sub(r'\s+', ' ', text.strip())
    return {"cleaned_input": cleaned}

cleaning_chain = TransformChain(
    input_variables=["raw_input"],
    output_variables=["cleaned_input"],
    transform=clean_input
)

# è½¬æ¢é“¾2ï¼šå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºJSONæ ¼å¼
def text_to_json(inputs):
    """å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºç»“æ„åŒ–JSON"""
    text = inputs["cleaned_input"]
    # è¿™é‡Œå¯ä»¥æ·»åŠ å¤æ‚çš„è½¬æ¢é€»è¾‘
    # ç¤ºä¾‹ï¼šæå–å…³é”®ä¿¡æ¯å¹¶æ„å»ºJSON
    json_structure = {
        "original_text": text,
        "length": len(text),
        "word_count": len(text.split()),
        "processed": True
    }
    return {"json_output": json.dumps(json_structure, ensure_ascii=False)}

json_transform_chain = TransformChain(
    input_variables=["cleaned_input"],
    output_variables=["json_output"],
    transform=text_to_json
)

# ä½¿ç”¨è½¬æ¢é“¾æ„å»ºå®Œæ•´æµç¨‹
full_chain = SequentialChain(
    chains=[cleaning_chain, json_transform_chain],
    input_variables=["raw_input"],
    output_variables=["json_output"],
    verbose=True
)

# æµ‹è¯•è½¬æ¢é“¾
result = full_chain.invoke({
    "raw_input": "  è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ¸…ç†çš„  ç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼ŒåŒ…å«  å¤šä½™ç©ºæ ¼å’Œæ ¼å¼é—®é¢˜  "
})
print("è½¬æ¢ç»“æœ:", result["json_output"])
```

**é«˜çº§åº”ç”¨ç¤ºä¾‹**ï¼šSQLæŸ¥è¯¢ç”Ÿæˆ

```python
# è½¬æ¢é“¾ï¼šå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢
def nl_to_sql(inputs):
    """è‡ªç„¶è¯­è¨€è½¬SQLæŸ¥è¯¢"""
    nl_query = inputs["user_query"]
    table_schema = inputs["table_schema"]
    
    # æ„å»ºè½¬æ¢åçš„è¾“å…¥
    sql_prompt = f"""
    æ ¹æ®ä»¥ä¸‹æ•°æ®åº“è¡¨ç»“æ„ï¼Œå°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¯­å¥ï¼š
    
    è¡¨ç»“æ„ï¼š{table_schema}
    ç”¨æˆ·æŸ¥è¯¢ï¼š{nl_query}
    
    è¯·åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚
    """
    
    return {"sql_prompt": sql_prompt}

sql_transform_chain = TransformChain(
    input_variables=["user_query", "table_schema"],
    output_variables=["sql_prompt"],
    transform=nl_to_sql
)

# æ„å»ºå®Œæ•´çš„SQLç”Ÿæˆé“¾
sql_generation_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        template="{sql_prompt}",
        input_variables=["sql_prompt"]
    ),
    output_key="sql_query"
)

# ç»„åˆæˆå®Œæ•´æµç¨‹
sql_workflow = SequentialChain(
    chains=[sql_transform_chain, sql_generation_chain],
    input_variables=["user_query", "table_schema"],
    output_variables=["sql_query"],
    verbose=True
)

# æµ‹è¯•SQLç”Ÿæˆ
result = sql_workflow.invoke({
    "user_query": "æŸ¥æ‰¾æ‰€æœ‰ä»·æ ¼å¤§äº100çš„å•†å“",
    "table_schema": "products(id, name, price, category)"
})
print("ç”Ÿæˆçš„SQL:", result["sql_query"])
```

# ğŸ“ ä¸ƒã€æ€»ç»“é“¾ï¼ˆSummarizeChainï¼‰ï¼šé•¿æ–‡æœ¬æ™ºèƒ½æ‘˜è¦

**åœºæ™¯æè¿°**ï¼š
åœ¨å¤„ç†å¤§é‡æ–‡æœ¬å†…å®¹æ—¶ï¼ˆå¦‚æ–°é—»æ–‡ç« ã€ç ”ç©¶æŠ¥å‘Šã€ä¼šè®®è®°å½•ã€é•¿ç¯‡æ–‡æ¡£ï¼‰ï¼Œéœ€è¦å¿«é€Ÿæå–æ ¸å¿ƒè¦ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´å‡†ç¡®çš„æ‘˜è¦ã€‚è¿™åœ¨å†…å®¹ç®¡ç†ã€ä¿¡æ¯æ£€ç´¢ã€çŸ¥è¯†ç®¡ç†ç­‰åœºæ™¯ä¸­å°¤ä¸ºé‡è¦ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨æ€»ç»“é“¾å¯ä»¥å°†å†—é•¿çš„æ–‡æœ¬å†…å®¹å‹ç¼©ä¸ºç²¾ç‚¼çš„æ‘˜è¦ã€‚æ”¯æŒå¤šç§æ€»ç»“ç­–ç•¥ï¼š
- **MapReduce**ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—ï¼Œåˆ†åˆ«æ€»ç»“åå†åˆå¹¶
- **Refine**ï¼šé€æ­¥ç²¾ç‚¼ï¼Œæ¯æ¬¡åŸºäºå‰ä¸€æ­¥çš„æ€»ç»“ç»§ç»­ä¼˜åŒ–
- **Stuff**ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼ˆé€‚ç”¨äºè¾ƒçŸ­çš„æ–‡æœ¬ï¼‰

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chains.summarize import load_summarize_chain
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(temperature=0.3, model="gpt-3.5-turbo")

# ç¤ºä¾‹ï¼šé•¿æ–‡æœ¬æ€»ç»“
long_text = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚
å¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚
äººå·¥æ™ºèƒ½ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ ·æ€è€ƒã€ä¹Ÿå¯èƒ½è¶…è¿‡äººçš„æ™ºèƒ½ã€‚äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨æå¯ŒæŒ‘æˆ˜æ€§çš„ç§‘å­¦ï¼Œä»äº‹è¿™é¡¹å·¥ä½œçš„äººå¿…é¡»æ‡‚å¾—è®¡ç®—æœºçŸ¥è¯†ã€å¿ƒç†å­¦å’Œå“²å­¦ã€‚
äººå·¥æ™ºèƒ½æ˜¯åŒ…æ‹¬ååˆ†å¹¿æ³›çš„ç§‘å­¦ï¼Œå®ƒç”±ä¸åŒçš„é¢†åŸŸç»„æˆï¼Œå¦‚æœºå™¨å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ç­‰ç­‰ã€‚æ€»çš„è¯´æ¥ï¼Œäººå·¥æ™ºèƒ½ç ”ç©¶çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿèƒœä»»ä¸€äº›é€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„å¤æ‚å·¥ä½œã€‚
"""

# åˆ›å»ºæ–‡æ¡£å¯¹è±¡
doc = Document(page_content=long_text)

# æ–¹æ³•1ï¼šä½¿ç”¨Stuffé“¾ï¼ˆé€‚åˆçŸ­æ–‡æœ¬ï¼‰
chain_stuff = load_summarize_chain(llm, chain_type="stuff")
summary_stuff = chain_stuff.invoke([doc])
print("Stuffæ€»ç»“:", summary_stuff["output_text"])

# æ–¹æ³•2ï¼šä½¿ç”¨MapReduceé“¾ï¼ˆé€‚åˆé•¿æ–‡æœ¬ï¼‰
chain_mapreduce = load_summarize_chain(llm, chain_type="map_reduce")
summary_mapreduce = chain_mapreduce.invoke([doc])
print("MapReduceæ€»ç»“:", summary_mapreduce["output_text"])

# æ–¹æ³•3ï¼šä½¿ç”¨Refineé“¾ï¼ˆé€æ­¥ç²¾ç‚¼ï¼‰
chain_refine = load_summarize_chain(llm, chain_type="refine")
summary_refine = chain_refine.invoke([doc])
print("Refineæ€»ç»“:", summary_refine["output_text"])
```

**é«˜çº§åº”ç”¨ï¼šå¤šæ–‡æ¡£æ€»ç»“**

```python
# å¤„ç†å¤šä¸ªç›¸å…³æ–‡æ¡£
documents = [
    Document(page_content="è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°ä¸€ä»£iPhoneï¼Œé‡‡ç”¨äº†å…¨æ–°çš„A17èŠ¯ç‰‡ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚"),
    Document(page_content="æ–°æ¬¾iPhoneçš„ç›¸æœºç³»ç»Ÿå¾—åˆ°å…¨é¢å‡çº§ï¼Œæ”¯æŒæ›´å…ˆè¿›çš„å¤œé—´æ‹æ‘„æ¨¡å¼ã€‚"),
    Document(page_content="ç”µæ± ç»­èˆªèƒ½åŠ›ä¹Ÿæœ‰æ‰€æå‡ï¼Œæ»¡è¶³ç”¨æˆ·ä¸€æ•´å¤©çš„ä½¿ç”¨éœ€æ±‚ã€‚"),
    Document(page_content="å”®ä»·æ–¹é¢ï¼Œæ–°æ¬¾iPhoneèµ·å”®ä»·ä¸º999ç¾å…ƒï¼Œæ¯”ä¸Šä¸€ä»£ç•¥æœ‰ä¸Šæ¶¨ã€‚")
]

# åˆ›å»ºæ€»ç»“é“¾å¤„ç†å¤šä¸ªæ–‡æ¡£
multi_doc_chain = load_summarize_chain(llm, chain_type="map_reduce")
combined_summary = multi_doc_chain.invoke(documents)
print("å¤šæ–‡æ¡£ç»¼åˆæ‘˜è¦:", combined_summary["output_text"])

# è‡ªå®šä¹‰æ€»ç»“æç¤ºæ¨¡æ¿
custom_prompt = """
è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š

{context}

è¦æ±‚ï¼š
1. æå–å…³é”®ä¿¡æ¯ç‚¹
2. ä¿æŒå®¢è§‚ä¸­ç«‹
3. æ§åˆ¶åœ¨50å­—ä»¥å†…

æ‘˜è¦ï¼š"""

prompt = PromptTemplate(template=custom_prompt, input_variables=["context"])
custom_chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt)
custom_summary = custom_chain.invoke([doc])
print("è‡ªå®šä¹‰æ‘˜è¦:", custom_summary["output_text"])
```

# ğŸ’¡ ä½¿ç”¨ LangChain Chain çš„å®ç”¨å»ºè®®ï¼ˆ2024å‡çº§ç‰ˆï¼‰

## ğŸ¯ é€‰å‹æŒ‡å—
1. **ä»»åŠ¡å¤æ‚åº¦åŒ¹é…**ï¼š
   - ç®€å•ä»»åŠ¡ï¼šå•ä¸ªLLMChain
   - ä¸­ç­‰å¤æ‚åº¦ï¼šSequentialChainæˆ–TransformChain
   - å¤æ‚å†³ç­–ï¼šRouterChain
   - æ–‡æ¡£å¤„ç†ï¼šæ–‡æ¡£é—®ç­”é“¾+æ€»ç»“é“¾ç»„åˆ

2. **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š
   - **æ‰¹å¤„ç†**ï¼šå¯¹ç›¸ä¼¼ä»»åŠ¡ä½¿ç”¨æ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨
   - **ç¼“å­˜**ï¼šä½¿ç”¨LangChainçš„ç¼“å­˜æœºåˆ¶é¿å…é‡å¤è®¡ç®—
   - **å¼‚æ­¥å¤„ç†**ï¼šä½¿ç”¨å¼‚æ­¥é“¾æé«˜å¹¶å‘æ€§èƒ½
   - **æµå¼å“åº”**ï¼šå¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆä½¿ç”¨æµå¼è¾“å‡ºæå‡ç”¨æˆ·ä½“éªŒ

## ğŸ› ï¸ å¼€å‘æœ€ä½³å®è·µ

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼š
   ```python
   # å°†å¸¸ç”¨é“¾å°è£…ä¸ºå¯é‡ç”¨ç»„ä»¶
   class TextProcessor:
       def __init__(self, llm):
           self.summarizer = load_summarize_chain(llm, chain_type="map_reduce")
           self.translator = LLMChain(...)  # ç¿»è¯‘é“¾
           
       def process_document(self, text):
           # ç»„åˆå¤šä¸ªé“¾å®Œæˆå¤æ‚ä»»åŠ¡
           summary = self.summarizer.invoke([Document(page_content=text)])
           translation = self.translator.invoke({"text": summary})
           return translation
   ```

2. **é”™è¯¯å¤„ç†ä¸é‡è¯•**ï¼š
   ```python
   from langchain.chains.base import Chain
   from tenacity import retry, stop_after_attempt, wait_exponential
   
   @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
   def safe_chain_invoke(chain, inputs):
       return chain.invoke(inputs)
   ```

3. **ç›‘æ§ä¸æ—¥å¿—**ï¼š
   - ä½¿ç”¨LangSmithè¿›è¡Œé“¾æ‰§è¡Œè·Ÿè¸ª
   - è®°å½•å…³é”®æŒ‡æ ‡ï¼šå»¶è¿Ÿã€tokenç”¨é‡ã€æˆåŠŸç‡
   - è®¾ç½®å‘Šè­¦æœºåˆ¶ç›‘æ§å¼‚å¸¸

4. **ç‰ˆæœ¬ç®¡ç†**ï¼š
   - ä½¿ç”¨LangChainçš„åºåˆ—åŒ–åŠŸèƒ½ä¿å­˜é“¾é…ç½®
   - å»ºç«‹æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
   - å®æ–½A/Bæµ‹è¯•è¯„ä¼°ä¸åŒé“¾é…ç½®æ•ˆæœ

## ğŸ”§ å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

1. **Tokené™åˆ¶**ï¼š
   - é—®é¢˜ï¼šæ–‡æ¡£é“¾è¶…å‡ºä¸Šä¸‹æ–‡é•¿åº¦
   - è§£å†³ï¼šä½¿ç”¨MapReduceç­–ç•¥æˆ–æ–‡æœ¬åˆ†å‰²å™¨

2. **å“åº”æ ¼å¼**ï¼š
   - é—®é¢˜ï¼šè¾“å‡ºæ ¼å¼ä¸ä¸€è‡´
   - è§£å†³ï¼šä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè§£æå™¨æˆ–å‡½æ•°è°ƒç”¨

3. **æ€§èƒ½ç“¶é¢ˆ**ï¼š
   - é—®é¢˜ï¼šé¡ºåºé“¾å¤„ç†æ—¶é—´è¿‡é•¿
   - è§£å†³ï¼šå¹¶è¡ŒåŒ–ç‹¬ç«‹ä»»åŠ¡ï¼Œä½¿ç”¨å¼‚æ­¥å¤„ç†

4. **æˆæœ¬æ§åˆ¶**ï¼š
   - é—®é¢˜ï¼šAPIè°ƒç”¨è´¹ç”¨è¿‡é«˜
   - è§£å†³ï¼šå®æ–½ç¼“å­˜ç­–ç•¥ï¼Œä¼˜åŒ–æç¤ºè¯å‡å°‘tokenç”¨é‡

## ğŸš€ è¿›é˜¶ç»„åˆæ¨¡å¼

```python
# æ„å»ºå¤æ‚å·¥ä½œæµçš„ç¤ºä¾‹
class IntelligentDocumentProcessor:
    """æ™ºèƒ½æ–‡æ¡£å¤„ç†å·¥ä½œæµ"""
    
    def __init__(self, llm):
        self.router = LLMRouterChain(...)  # è·¯ç”±ä¸åŒç±»å‹æ–‡æ¡£
        self.extractor = create_extraction_chain(...)  # æå–å…³é”®ä¿¡æ¯
        self.summarizer = load_summarize_chain(...)  # ç”Ÿæˆæ‘˜è¦
        self.transformer = TransformChain(...)  # æ ¼å¼è½¬æ¢
        
    def process_document(self, document, query=None):
        # 1. è·¯ç”±åˆ°åˆé€‚çš„å¤„ç†é“¾
        route = self.router.invoke({"input": document})
        
        # 2. æå–ç»“æ„åŒ–ä¿¡æ¯
        extracted = self.extractor.invoke(document)
        
        # 3. ç”Ÿæˆæ‘˜è¦
        summary = self.summarizer.invoke([Document(page_content=document)])
        
        # 4. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        final_result = self.transformer.invoke({
            "extracted": extracted,
            "summary": summary,
            "query": query
        })
        
        return final_result
```

# ğŸ”— å…«ã€APIé“¾ï¼ˆAPIChainï¼‰ï¼šæ™ºèƒ½APIè°ƒç”¨ä¸æ•°æ®è·å–

**åœºæ™¯æè¿°**ï¼š
åœ¨AIåº”ç”¨ä¸­ï¼Œç»å¸¸éœ€è¦è®©å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ™ºèƒ½åœ°è°ƒç”¨å¤–éƒ¨APIæ¥è·å–å®æ—¶æ•°æ®ã€æ‰§è¡Œæ“ä½œæˆ–ä¸å¤–éƒ¨ç³»ç»Ÿé›†æˆã€‚ä¾‹å¦‚æŸ¥è¯¢å¤©æ°”ä¿¡æ¯ã€è·å–è‚¡ç¥¨æ•°æ®ã€è°ƒç”¨ä¼ä¸šå†…éƒ¨æœåŠ¡ã€æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯ç­‰ã€‚ä¼ ç»Ÿæ–¹å¼éœ€è¦æ‰‹åŠ¨ç¼–å†™APIè°ƒç”¨é€»è¾‘ï¼Œè€ŒAPIé“¾å¯ä»¥è®©LLMè‡ªåŠ¨å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨APIã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ `APIChain` å¯ä»¥è®©LLMæ ¹æ®ç”¨æˆ·æ„å›¾è‡ªåŠ¨ç”ŸæˆAPIè°ƒç”¨ï¼Œå¤„ç†å“åº”æ•°æ®ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€å½¢å¼è¿”å›ç»“æœã€‚å®ƒæ”¯æŒRESTful APIè°ƒç”¨ï¼Œå¯ä»¥å¤„ç†è®¤è¯ã€å‚æ•°æ„å»ºã€å“åº”è§£æç­‰å¤æ‚é€»è¾‘ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chains import APIChain
from langchain_openai import OpenAI

# åˆå§‹åŒ–æ¨¡å‹å’ŒAPIé“¾
llm = OpenAI(temperature=0)

# ç¤ºä¾‹1ï¼šæŸ¥è¯¢å¤©æ°”ä¿¡æ¯
weather_api_docs = """
BASE URL: https://api.openweathermap.org/data/2.5/

APIæ–‡æ¡£:
- è·å–å½“å‰å¤©æ°”: GET /weather?q={city}&appid={api_key}&units=metric
- è·å–å¤©æ°”é¢„æŠ¥: GET /forecast?q={city}&appid={api_key}&units=metric
- å“åº”æ ¼å¼: JSONåŒ…å«æ¸©åº¦ã€æ¹¿åº¦ã€å¤©æ°”æè¿°ç­‰ä¿¡æ¯
"""

weather_chain = APIChain.from_llm_and_api_docs(
    llm=llm,
    api_docs=weather_api_docs,
    verbose=True
)

# æµ‹è¯•å¤©æ°”æŸ¥è¯¢
result = weather_chain.invoke("åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
print("å¤©æ°”ä¿¡æ¯:", result)
```

**é«˜çº§åº”ç”¨ï¼šè‡ªå®šä¹‰APIé›†æˆ**

```python
# ç¤ºä¾‹2ï¼šé›†æˆå¤šä¸ªAPIæœåŠ¡
class SmartAPIChain:
    def __init__(self, llm):
        self.llm = llm
        self.setup_chains()
    
    def setup_chains(self):
        # ä¾›åº”å•†æŸ¥è¯¢API
        supplier_docs = """
        BASE URL: http://localhost:8000/api
        
        ç«¯ç‚¹è¯´æ˜:
        - GET /suppliers?name={name} - æŒ‰åç§°æœç´¢ä¾›åº”å•†
        - GET /suppliers/{id} - è·å–ä¾›åº”å•†è¯¦ç»†ä¿¡æ¯
        - GET /suppliers/category/{category} - æŒ‰ç±»åˆ«æŸ¥è¯¢ä¾›åº”å•†
        """
        
        self.supplier_chain = APIChain.from_llm_and_api_docs(
            llm=self.llm,
            api_docs=supplier_docs,
            headers={"Authorization": "Bearer mock-token"},
            verbose=True
        )
    
    def query_suppliers(self, query):
        return self.supplier_chain.invoke(query)

# ä½¿ç”¨ç¤ºä¾‹
api_processor = SmartAPIChain(llm)
result = api_processor.query_suppliers("æŸ¥æ‰¾ç”µå­äº§å“ç±»åˆ«çš„ä¾›åº”å•†")
```

# ğŸŒ ä¹ã€LLMRequestsChainï¼šæ™ºèƒ½HTTPè¯·æ±‚é“¾

**åœºæ™¯æè¿°**ï¼š
å½“éœ€è¦è®©AIæ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒHTTPè¯·æ±‚ï¼Œå¤„ç†å¤æ‚çš„WebæœåŠ¡äº¤äº’æ—¶ï¼ŒLLMRequestsChainæä¾›äº†æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒå¯ä»¥å¤„ç†GETã€POSTç­‰å„ç§HTTPæ–¹æ³•ï¼Œæ”¯æŒè‡ªå®šä¹‰headersã€è®¤è¯ã€å‚æ•°å¤„ç†ç­‰ï¼Œé€‚ç”¨äºæ„å»ºæ™ºèƒ½å®¢æœã€æ•°æ®æŸ¥è¯¢åŠ©æ‰‹ç­‰åº”ç”¨ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä½¿ç”¨ `LLMRequestsChain` å¯ä»¥è®©ç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€æè¿°ä»–ä»¬æƒ³è¦çš„æ“ä½œï¼ŒAIä¼šè‡ªåŠ¨æ„å»ºåˆé€‚çš„HTTPè¯·æ±‚ï¼Œè°ƒç”¨APIï¼Œå¹¶è§£æè¿”å›çš„æ•°æ®ä»¥è‡ªç„¶è¯­è¨€å½¢å¼å‘ˆç°ç»™ç”¨æˆ·ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.chains import LLMRequestsChain
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate

# åˆå§‹åŒ–æ¨¡å‹
llm = OpenAI(temperature=0)

# åˆ›å»ºè¯·æ±‚é“¾æ¨¡æ¿
template = """åœ¨ >>> å’Œ <<< ä¹‹é—´æ˜¯APIçš„å“åº”ã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚{query}ï¼Œä»å“åº”ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶ä»¥è‡ªç„¶è¯­è¨€å›ç­”ã€‚

>>> {requests_result} <<<
è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""

PROMPT = PromptTemplate(
    input_variables=["query", "requests_result"],
    template=template,
)

# åˆ›å»ºLLMRequestsChain
requests_chain = LLMRequestsChain(
    llm_chain=LLMChain(llm=llm, prompt=PROMPT),
    verbose=True
)

# ç¤ºä¾‹ï¼šæŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯
result = requests_chain.invoke({
    "query": "æŸ¥è¯¢å‘˜å·¥å¼ ä¸‰çš„å‰©ä½™å¹´å‡å¤©æ•°",
    "url": "http://localhost:8000/api/employees/å¼ ä¸‰/leave-balance"
})
print("æŸ¥è¯¢ç»“æœ:", result["output"])
```

**ç»¼åˆåº”ç”¨ï¼šæ™ºèƒ½ä¼ä¸šåŠ©æ‰‹**

```python
class EnterpriseSmartAssistant:
    """ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹ - æ•´åˆå¤šç§APIæœåŠ¡"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.llm = OpenAI(temperature=0.3)
        self.setup_chains()
    
    def setup_chains(self):
        """è®¾ç½®å„ç§APIè°ƒç”¨é“¾"""
        
        # å‘˜å·¥ä¿¡æ¯æŸ¥è¯¢é“¾
        employee_template = """æ ¹æ®APIå“åº”å›ç­”å…³äºå‘˜å·¥çš„é—®é¢˜ã€‚
        
        ç”¨æˆ·é—®é¢˜: {query}
        APIå“åº”: {requests_result}
        
        è¯·ä»¥ç®€æ´æ˜äº†çš„ä¸­æ–‡å›ç­”ã€‚"""
        
        self.employee_chain = LLMRequestsChain(
            llm_chain=LLMChain(
                llm=self.llm,
                prompt=PromptTemplate(
                    input_variables=["query", "requests_result"],
                    template=employee_template
                )
            )
        )
        
        # æ”¿ç­–æŸ¥è¯¢é“¾
        policy_template = """æ ¹æ®APIå“åº”è§£é‡Šç›¸å…³æ”¿ç­–ã€‚
        
        ç”¨æˆ·é—®é¢˜: {query}
        æ”¿ç­–å†…å®¹: {requests_result}
        
        è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£é‡Šæ”¿ç­–è¦ç‚¹ã€‚"""
        
        self.policy_chain = LLMRequestsChain(
            llm_chain=LLMChain(
                llm=self.llm,
                prompt=PromptTemplate(
                    input_variables=["query", "requests_result"],
                    template=policy_template
                )
            )
        )
    
    def query_employee_leave(self, employee_name):
        """æŸ¥è¯¢å‘˜å·¥å‡æœŸä¿¡æ¯"""
        return self.employee_chain.invoke({
            "query": f"æŸ¥è¯¢{employee_name}çš„å‡æœŸä½™é¢",
            "url": f"{self.base_url}/api/employees/{employee_name}/leave-balance"
        })
    
    def query_maternity_policy(self):
        """æŸ¥è¯¢äº§å‡æ”¿ç­–"""
        return self.policy_chain.invoke({
            "query": "æŸ¥è¯¢å…¬å¸çš„äº§å‡æ”¿ç­–",
            "url": f"{self.base_url}/api/policies/maternity"
        })
    
    def query_user_permissions(self, user_id):
        """æŸ¥è¯¢ç”¨æˆ·æƒé™åˆ—è¡¨"""
        return self.employee_chain.invoke({
            "query": f"æŸ¥è¯¢ç”¨æˆ·{user_id}çš„æƒé™åˆ—è¡¨",
            "url": f"{self.base_url}/api/users/{user_id}/permissions"
        })
    
    def query_suppliers(self, category=None, name=None):
        """æŸ¥è¯¢ä¾›åº”å•†ä¿¡æ¯"""
        if name:
            url = f"{self.base_url}/api/suppliers?name={name}"
        elif category:
            url = f"{self.base_url}/api/suppliers/category/{category}"
        else:
            url = f"{self.base_url}/api/suppliers"
        
        return self.employee_chain.invoke({
            "query": "æŸ¥è¯¢ä¾›åº”å•†ä¿¡æ¯",
            "url": url
        })

# ä½¿ç”¨ç¤ºä¾‹
assistant = EnterpriseSmartAssistant()

# å„ç§æŸ¥è¯¢ç¤ºä¾‹
print("=== ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹æ¼”ç¤º ===")
print("å‘˜å·¥å‡æœŸæŸ¥è¯¢:", assistant.query_employee_leave("å¼ ä¸‰"))
print("äº§å‡æ”¿ç­–:", assistant.query_maternity_policy())
print("ç”¨æˆ·æƒé™:", assistant.query_user_permissions("user123"))
print("ä¾›åº”å•†æŸ¥è¯¢:", assistant.query_suppliers(category="electronics"))
```

# ğŸ“‹ é™„å½•ï¼šå®Œæ•´ä»£ç å®ç°

## A.1 80_chain_examples.py - å®Œæ•´å®ç°

```python
#!/usr/bin/env python3
"""
LangChain Chain ç¤ºä¾‹è„šæœ¬ - åŸºäºOllamaæœ¬åœ°æ¨¡å‹
åŸºäº79_chain.mdæ–‡æ¡£æ„å»ºçš„5ç§Chainæ¼”ç¤º
ä½¿ç”¨æœ¬åœ°Ollamaçš„gpt-3.5-turbo:latestæ¨¡å‹
"""

import os
import json
from typing import List, Dict, Any
from langchain_ollama import OllamaLLM as Ollama
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chains import LLMChain, SequentialChain
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_extraction_chain
from langchain.chains import LLMMathChain

def setup_ollama_model():
    """åˆå§‹åŒ–Ollamaæ¨¡å‹"""
    return Ollama(
        model="gpt-3.5-turbo:latest",
        base_url="http://localhost:11434",
        temperature=0.7
    )

class LangChainExamples:
    """LangChainç¤ºä¾‹ç±»"""
    
    def __init__(self):
        self.llm = setup_ollama_model()
        
    def example_1_router_chain(self):
        """ç¤ºä¾‹1ï¼šè·¯ç”±é“¾ - æ™ºèƒ½å®¢æœé—®é¢˜åˆ†ç±»"""
        print("\nğŸ§© ç¤ºä¾‹1ï¼šè·¯ç”±é“¾ - æ™ºèƒ½å®¢æœé—®é¢˜åˆ†ç±»")
        print("-" * 50)
        
        # å®šä¹‰æç¤ºæ¨¡æ¿
        flower_care_template = """ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å›­ä¸ï¼Œæ“…é•¿è§£ç­”å…³äºå…»èŠ±è‚²èŠ±çš„é—®é¢˜ã€‚
                                ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                                {input}"""

        flower_deco_template = """ä½ æ˜¯ä¸€ä½ç½‘çº¢æ’èŠ±å¤§å¸ˆï¼Œæ“…é•¿è§£ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜ã€‚
                                ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                                {input}"""

        # æ„å»ºæç¤ºä¿¡æ¯åˆ—è¡¨
        prompt_infos = [
            {
                "key": "flower_care",
                "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±æŠ¤ç†çš„é—®é¢˜",
                "template": flower_care_template,
            },
            {
                "key": "flower_decoration",
                "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜",
                "template": flower_deco_template,
            }
        ]

        # æ„å»ºç›®æ ‡é“¾å­—å…¸
        chain_map = {}
        for info in prompt_infos:
            prompt = PromptTemplate(template=info['template'], input_variables=["input"])
            chain = LLMChain(llm=self.llm, prompt=prompt)
            chain_map[info["key"]] = chain

        # æ„å»ºè·¯ç”±é“¾
        destinations = [f"{p['key']}: {p['description']}" for p in prompt_infos]
        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations="\n".join(destinations))
        router_prompt = PromptTemplate(
            template=router_template,
            input_variables=["input"],
            output_parser=RouterOutputParser(),
        )
        router_chain = LLMRouterChain.from_llm(self.llm, router_prompt)

        # æ„å»ºé»˜è®¤é“¾
        default_prompt = PromptTemplate(template="{input}", input_variables=["input"])
        default_chain = LLMChain(llm=self.llm, prompt=default_prompt)

        # è·¯ç”±å‡½æ•°
        def route_question(question):
            try:
                destination_info = router_chain.invoke({"input": question})
                destination = destination_info.get("destination", "default")
                
                if destination in chain_map:
                    return chain_map[destination].invoke({"input": question})
                else:
                    return default_chain.invoke({"input": question})
            except Exception as e:
                print(f"è·¯ç”±é“¾æ‰§è¡Œé”™è¯¯: {e}")
                return default_chain.invoke({"input": question})

        # æµ‹è¯•ç”¨ä¾‹
        test_questions = [
            "ç«ç‘°èŠ±åº”è¯¥å¤šä¹…æµ‡ä¸€æ¬¡æ°´ï¼Ÿ",
            "å©šç¤¼ç°åœºç”¨ç«ç‘°èŠ±å’Œæ»¡å¤©æ˜Ÿæ€ä¹ˆæ­é…ï¼Ÿ",
            "å‘æ—¥è‘µé€‚åˆæ”¾åœ¨å§å®¤å—ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = route_question(question)
                print(f"å›ç­”: {result.get('text', str(result))}")
            except Exception as e:
                print(f"æ‰§è¡Œå¤±è´¥: {e}")

    def example_2_sequential_chain(self):
        """ç¤ºä¾‹2ï¼šé¡ºåºé“¾ - ç”¨æˆ·è¯„è®ºåˆ†æä¸å¤šè¯­è¨€å›å¤"""
        print("\nğŸ”„ ç¤ºä¾‹2ï¼šé¡ºåºé“¾ - ç”¨æˆ·è¯„è®ºåˆ†æä¸å¤šè¯­è¨€å›å¤")
        print("-" * 50)
        
        # è®¾ç½®è¾ƒä½temperatureä»¥ä¿è¯å‡†ç¡®æ€§
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0.3
        )
        
        # å­é“¾1ï¼šå°†ä¸­æ–‡è¯„è®ºç¿»è¯‘æˆè‹±æ–‡
        prompt_z2e = PromptTemplate.from_template("å°†ä¸‹é¢çš„ä¸­æ–‡è¯„è®ºç¿»è¯‘ä¸ºè‹±æ–‡ï¼š\n\n{ch_review}")
        chain_z2e = LLMChain(llm=llm, prompt=prompt_z2e, output_key="en_review")

        # å­é“¾2ï¼šæ€»ç»“è‹±æ–‡è¯„è®º
        prompt_es = PromptTemplate.from_template("Can you summarize the following review in 1 sentence: \n\n{en_review}")
        chain_es = LLMChain(llm=llm, prompt=prompt_es, output_key="summary")

        # å­é“¾3ï¼šè¯†åˆ«è¯„è®ºåŸè¯­è¨€
        prompt_lang = PromptTemplate.from_template("ä¸‹é¢çš„è¯„è®ºä½¿ç”¨çš„æ˜¯ä»€ä¹ˆè¯­è¨€ï¼Ÿ:\n\n{ch_review}")
        chain_lang = LLMChain(llm=llm, prompt=prompt_lang, output_key="language")

        # å­é“¾4ï¼šç”¨åŸè¯­è¨€ç”Ÿæˆå›å¤
        prompt_reply = PromptTemplate.from_template(
            "ä½¿ç”¨æŒ‡å®šè¯­è¨€ç¼–å†™å¯¹ä»¥ä¸‹æ‘˜è¦çš„åç»­å›å¤ï¼š\n\næ‘˜è¦ï¼š{summary}\n\nè¯­è¨€ï¼š{language}"
        )
        chain_reply = LLMChain(llm=llm, prompt=prompt_reply, output_key="orig_reply")

        # å­é“¾5ï¼šå°†å›å¤ç¿»è¯‘æˆä¸­æ–‡
        prompt_e2z = PromptTemplate.from_template("å°†ä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š\n\n{orig_reply}")
        chain_e2z = LLMChain(llm=llm, prompt=prompt_e2z, output_key="ch_reply")

        # æ„å»ºé¡ºåºé“¾
        overall_chain = SequentialChain(
            chains=[chain_z2e, chain_es, chain_lang, chain_reply, chain_e2z],
            input_variables=["ch_review"],
            output_variables=["en_review", "summary", "language", "orig_reply", "ch_reply"],
            verbose=True
        )

        # æµ‹è¯•
        chinese_review = "å®«å´éªä»¥å¾€çš„ä½œå“å‰§ä½œå·¥æ•´ã€å½¢å¼ç»Ÿä¸€ï¼Œè€Œä¸”å¤§å¤šèƒ½è®©è§‚ä¼—æç‚¼å‡ºå‘å–„å‘ç¾çš„ä¸­å¿ƒæ€æƒ³ã€‚"
        
        try:
            result = overall_chain.invoke({"ch_review": chinese_review})
            print(f"åŸå§‹è¯„è®º: {chinese_review}")
            print(f"è‹±æ–‡ç¿»è¯‘: {result['en_review']}")
            print(f"æ‘˜è¦: {result['summary']}")
            print(f"è¯­è¨€: {result['language']}")
            print(f"åŸè¯­è¨€å›å¤: {result['orig_reply']}")
            print(f"ä¸­æ–‡å›å¤: {result['ch_reply']}")
        except Exception as e:
            print(f"é¡ºåºé“¾æ‰§è¡Œé”™è¯¯: {e}")

    def example_3_document_chain(self):
        """ç¤ºä¾‹3ï¼šæ–‡æ¡£é—®ç­”é“¾ - åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”"""
        print("\nğŸ“Š ç¤ºä¾‹3ï¼šæ–‡æ¡£é—®ç­”é“¾ - åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”")
        print("-" * 50)
        
        # åˆå§‹åŒ–æ¨¡å‹
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0.1
        )
        
        # æ„å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡: {context} \n\n å›ç­”é—®é¢˜: {input}"),
        ])

        # æ„å»ºæ–‡æ¡£é“¾
        document_chain = create_stuff_documents_chain(llm, prompt)

        # å‡†å¤‡æ–‡æ¡£
        docs = [
            Document(page_content="æ°è¥¿å–œæ¬¢çº¢è‰²ï¼Œä½†ä¸å–œæ¬¢é»„è‰²"),
            Document(page_content="è´¾é©¬å°”å–œæ¬¢ç»¿è‰²ï¼Œæœ‰ä¸€ç‚¹å–œæ¬¢çº¢è‰²"),
            Document(page_content="ç›ä¸½å–œæ¬¢ç²‰è‰²å’Œçº¢è‰²")
        ]

        # æµ‹è¯•ç”¨ä¾‹
        questions = [
            "å¤§å®¶å–œæ¬¢ä»€ä¹ˆé¢œè‰²?",
            "è°å–œæ¬¢çº¢è‰²ï¼Ÿ",
            "æ°è¥¿å–œæ¬¢ä»€ä¹ˆé¢œè‰²ï¼Ÿ"
        ]
        
        for question in questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = document_chain.invoke({"input": question, "context": docs})
                print(f"å›ç­”: {result}")
            except Exception as e:
                print(f"æ‰§è¡Œå¤±è´¥: {e}")

    def example_4_extraction_chain(self):
        """ç¤ºä¾‹4ï¼šä¿¡æ¯æå–é“¾ - ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        print("\nğŸ” ç¤ºä¾‹4ï¼šä¿¡æ¯æå–é“¾ - ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯")
        print("-" * 50)
        
        # è®¾ç½®è¾ƒä½temperatureä»¥ä¿è¯å‡†ç¡®æ€§
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0
        )
        
        # å®šä¹‰æå–æ¨¡å¼
        schema = {
            "properties": {
                "name": {"type": "string"},
                "height": {"type": "integer"},
                "hair_color": {"type": "string"},
            },
            "required": ["name", "height"],
        }

        # åˆ›å»ºæå–é“¾
        extraction_chain = create_extraction_chain(schema, llm)

        # æµ‹è¯•ç”¨ä¾‹
        test_texts = [
            "äºšå†å…‹æ–¯èº«é«˜ 5 è‹±å°ºã€‚å…‹åŠ³è¿ªå¨…æ¯”äºšå†å…‹æ–¯é«˜ 1 è‹±å°ºï¼Œå¹¶ä¸”è·³å¾—æ¯”ä»–æ›´é«˜ã€‚å…‹åŠ³è¿ªå¨…æ˜¯é»‘å‘å¥³éƒï¼Œäºšå†å…‹æ–¯æ˜¯é‡‘å‘å¥³éƒã€‚",
            "å°æ˜èº«é«˜180å˜ç±³ï¼Œå°çº¢èº«é«˜165å˜ç±³ï¼Œå°æ˜çš„å¤´å‘æ˜¯é»‘è‰²çš„ã€‚",
            "å¼ ä¸‰å’Œæå››éƒ½æ˜¯å­¦ç”Ÿï¼Œå¼ ä¸‰èº«é«˜175å˜ç±³ï¼Œæå››èº«é«˜170å˜ç±³ï¼Œå¼ ä¸‰çš„å¤´å‘æ˜¯æ£•è‰²çš„ã€‚"
        ]
        
        for text in test_texts:
            print(f"\næ–‡æœ¬: {text}")
            try:
                result = extraction_chain.invoke(text)
                print(f"æå–ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
            except Exception as e:
                print(f"æå–å¤±è´¥: {e}")

    def example_5_math_chain(self):
        """ç¤ºä¾‹5ï¼šæ•°å­¦é“¾ - è§£å†³æ•°å­¦è®¡ç®—é—®é¢˜"""
        print("\nğŸ§® ç¤ºä¾‹5ï¼šæ•°å­¦é“¾ - è§£å†³æ•°å­¦è®¡ç®—é—®é¢˜")
        print("-" * 50)
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ•°å­¦é—®é¢˜éœ€è¦å‡†ç¡®æ€§ï¼‰
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0
        )
        
        # åˆ›å»ºæ•°å­¦é“¾
        llm_math_chain = LLMMathChain.from_llm(llm)

        # æµ‹è¯•ç”¨ä¾‹
        math_questions = [
            "100 * 20 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ",
            "è®¡ç®—åœ†çš„é¢ç§¯ï¼Œå¦‚æœåŠå¾„æ˜¯5å˜ç±³ã€‚è¯·ä½¿ç”¨3.14ä½œä¸ºåœ†å‘¨ç‡ã€‚",
            "ä¸€ä¸ªé•¿æ–¹å½¢çš„é•¿æ˜¯12ç±³ï¼Œå®½æ˜¯8ç±³ï¼Œæ±‚é¢ç§¯å’Œå‘¨é•¿ã€‚",
            "è§£æ–¹ç¨‹ï¼š2x + 5 = 15"
        ]
        
        for question in math_questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = llm_math_chain.invoke(question)
                print(f"ç­”æ¡ˆ: {result.get('answer', str(result))}")
            except Exception as e:
                print(f"è®¡ç®—å¤±è´¥: {e}")

    def run_all_examples(self):
        """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
        print("ğŸš€ LangChain Chain ç¤ºä¾‹æ¼”ç¤º")
        print("=" * 60)
        print("ä½¿ç”¨æœ¬åœ°Ollamaçš„gpt-3.5-turbo:latestæ¨¡å‹")
        print("=" * 60)
        
        # æ£€æŸ¥OllamaæœåŠ¡
        try:
            test_response = self.llm.invoke("Hello")
            print("âœ… OllamaæœåŠ¡è¿æ¥æ­£å¸¸")
        except Exception as e:
            print(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨: ollama serve")
            return
        
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        try:
            self.example_1_router_chain()
        except Exception as e:
            print(f"è·¯ç”±é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_2_sequential_chain()
        except Exception as e:
            print(f"é¡ºåºé“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_3_document_chain()
        except Exception as e:
            print(f"æ–‡æ¡£é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_4_extraction_chain()
        except Exception as e:
            print(f"æå–é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_5_math_chain()
        except Exception as e:
            print(f"æ•°å­¦é“¾ç¤ºä¾‹å¤±è´¥: {e}")

if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„ä¾èµ–
    try:
        import langchain_ollama
        import langchain_core
        import langchain
    except ImportError:
        print("è¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
        print("pip install langchain-ollama langchain-core langchain")
        exit(1)
    
    # è¿è¡Œç¤ºä¾‹
    examples = LangChainExamples()
    examples.run_all_examples()
```

## A.2 82_api_requests_ollama.py - å®Œæ•´å®ç°

```python
#!/usr/bin/env python3
"""
ä½¿ç”¨Ollamaçš„LLMRequestsChainå®Œæ•´æ¼”ç¤º
ä¸“ä¸ºOllama gpt-3.5-turbo:latestä¼˜åŒ–
"""

import json
import time
import threading
import requests
from flask import Flask, jsonify, request
from langchain.chains import LLMChain
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate

# ==================== Mock RESTfulæœåŠ¡ ====================

class MockEnterpriseAPI:
    """ä¼ä¸šçº§Mock APIæœåŠ¡ - å®Œæ•´å®ç°"""
    
    def __init__(self, port=8000):
        self.app = Flask(__name__)
        self.port = port
        self.setup_routes()
        self.mock_data = self._generate_mock_data()
    
    def _generate_mock_data(self):
        """ç”Ÿæˆå®Œæ•´çš„æ¨¡æ‹Ÿæ•°æ®"""
        return {
            "employees": {
                "å¼ ä¸‰": {
                    "name": "å¼ ä¸‰",
                    "department": "æŠ€æœ¯éƒ¨",
                    "position": "é«˜çº§å¼€å‘å·¥ç¨‹å¸ˆ",
                    "annual_leave": 15,
                    "used_leave": 7,
                    "remaining_leave": 8,
                    "sick_leave": 5,
                    "maternity_leave_eligible": True,
                    "join_date": "2020-03-15",
                    "salary": 25000
                },
                "æå››": {
                    "name": "æå››",
                    "department": "å¸‚åœºéƒ¨",
                    "position": "å¸‚åœºç»ç†", 
                    "annual_leave": 12,
                    "used_leave": 3,
                    "remaining_leave": 9,
                    "sick_leave": 5,
                    "maternity_leave_eligible": False,
                    "join_date": "2019-08-20",
                    "salary": 18000
                },
                "ç‹äº”": {
                    "name": "ç‹äº”",
                    "department": "äººäº‹éƒ¨",
                    "position": "HRä¸“å‘˜",
                    "annual_leave": 10,
                    "used_leave": 2,
                    "remaining_leave": 8,
                    "sick_leave": 3,
                    "maternity_leave_eligible": True,
                    "join_date": "2021-05-10",
                    "salary": 15000
                }
            },
            "suppliers": {
                "electronics": [
                    {
                        "id": 1,
                        "name": "åå¼ºåŒ—ç”µå­",
                        "contact": "13800138001",
                        "email": "huang@example.com",
                        "rating": 4.8,
                        "products": ["æ‰‹æœºé…ä»¶", "ç”µè„‘é…ä»¶", "æ•°æ®çº¿"],
                        "status": "active",
                        "address": "æ·±åœ³å¸‚ç¦ç”°åŒºåå¼ºåŒ—è·¯",
                        "established": "2010"
                    },
                    {
                        "id": 2,
                        "name": "äº¬ä¸œç”µå­",
                        "contact": "400-606-5500",
                        "email": "jd@jd.com",
                        "rating": 4.9,
                        "products": ["æ‰‹æœº", "ç”µè„‘", "å®¶ç”µ"],
                        "status": "active",
                        "address": "åŒ—äº¬å¸‚å¤§å…´åŒº",
                        "established": "2004"
                    }
                ],
                "food": [
                    {
                        "id": 3,
                        "name": "ä¸­ç²®é›†å›¢",
                        "contact": "400-698-6666",
                        "email": "info@cofco.com",
                        "rating": 4.7,
                        "products": ["å¤§ç±³", "é£Ÿç”¨æ²¹", "é¢ç²‰"],
                        "status": "active",
                        "address": "åŒ—äº¬å¸‚æœé˜³åŒº",
                        "established": "1949"
                    }
                ]
            },
            "policies": {
                "maternity": {
                    "title": "äº§å‡æ”¿ç­–",
                    "content": """
                    1. åŸºç¡€äº§å‡ï¼šå¥³èŒå·¥ç”Ÿè‚²äº«å—98å¤©äº§å‡
                    2. éš¾äº§å¢åŠ ï¼šéš¾äº§å¢åŠ 15å¤©
                    3. å¤šèƒèƒï¼šæ¯å¤š1ä¸ªå©´å„¿å¢åŠ 15å¤©
                    4. é™ªäº§å‡ï¼šç”·èŒå·¥äº«å—15å¤©é™ªäº§å‡
                    5. äº§å‰æ£€æŸ¥ï¼šæ€€å­•å¥³èŒå·¥äº§å‰æ£€æŸ¥ç®—ä½œåŠ³åŠ¨æ—¶é—´
                    """,
                    "effective_date": "2024-01-01",
                    "department": "äººäº‹éƒ¨"
                },
                "annual_leave": {
                    "title": "å¹´å‡æ”¿ç­–",
                    "content": """
                    1. å·¥ä½œæ»¡1-10å¹´ï¼š5å¤©å¹´å‡
                    2. å·¥ä½œæ»¡10-20å¹´ï¼š10å¤©å¹´å‡
                    3. å·¥ä½œæ»¡20å¹´ä»¥ä¸Šï¼š15å¤©å¹´å‡
                    4. å¹´å‡å¯è·¨å¹´ç´¯ç§¯ï¼Œæœ€å¤šç´¯ç§¯2å¹´
                    """,
                    "effective_date": "2024-01-01",
                    "department": "äººäº‹éƒ¨"
                }
            },
            "users": {
                "admin": {
                    "username": "admin",
                    "permissions": ["read", "write", "delete", "admin"],
                    "role": "ç®¡ç†å‘˜",
                    "last_login": "2024-01-15 09:30:00"
                },
                "user1": {
                    "username": "user1",
                    "permissions": ["read", "write"],
                    "role": "æ™®é€šç”¨æˆ·",
                    "last_login": "2024-01-14 16:45:00"
                }
            }
        }
    
    def setup_routes(self):
        """è®¾ç½®å®Œæ•´çš„APIè·¯ç”±"""
        
        # å¥åº·æ£€æŸ¥
        @self.app.route('/health', methods=['GET'])
        def health_check():
            return jsonify({"status": "healthy", "timestamp": time.time()})
        
        # å‘˜å·¥ç›¸å…³API
        @self.app.route('/api/employees/<employee_name>/leave-balance', methods=['GET'])
        def get_employee_leave(employee_name):
            employee = self.mock_data["employees"].get(employee_name)
            if not employee:
                return jsonify({"error": "å‘˜å·¥ä¸å­˜åœ¨"}), 404
            return jsonify({
                "success": True,
                "data": {
                    "name": employee["name"],
                    "department": employee["department"],
                    "annual_leave": employee["annual_leave"],
                    "used_leave": employee["used_leave"],
                    "remaining_leave": employee["remaining_leave"],
                    "sick_leave": employee["sick_leave"]
                }
            })
        
        @self.app.route('/api/employees/<employee_name>/profile', methods=['GET'])
        def get_employee_profile(employee_name):
            employee = self.mock_data["employees"].get(employee_name)
            if not employee:
                return jsonify({"error": "å‘˜å·¥ä¸å­˜åœ¨"}), 404
            return jsonify({"success": True, "data": employee})
        
        # ä¾›åº”å•†ç›¸å…³API
        @self.app.route('/api/suppliers', methods=['GET'])
        def get_suppliers():
            category = request.args.get('category')
            name = request.args.get('name')
            
            suppliers = []
            if category and category in self.mock_data["suppliers"]:
                suppliers = self.mock_data["suppliers"][category]
            elif name:
                # æœç´¢æ‰€æœ‰ä¾›åº”å•†
                for category_suppliers in self.mock_data["suppliers"].values():
                    suppliers.extend([s for s in category_suppliers if name.lower() in s["name"].lower()])
            else:
                # è¿”å›æ‰€æœ‰ä¾›åº”å•†
                for category_suppliers in self.mock_data["suppliers"].values():
                    suppliers.extend(category_suppliers)
            
            return jsonify({
                "success": True,
                "data": suppliers,
                "count": len(suppliers)
            })
        
        @self.app.route('/api/suppliers/category/<category>', methods=['GET'])
        def get_suppliers_by_category(category):
            suppliers = self.mock_data["suppliers"].get(category, [])
            return jsonify({
                "success": True,
                "data": suppliers,
                "category": category,
                "count": len(suppliers)
            })
        
        # æ”¿ç­–ç›¸å…³API
        @self.app.route('/api/policies/<policy_type>', methods=['GET'])
        def get_policy(policy_type):
            policy = self.mock_data["policies"].get(policy_type)
            if not policy:
                return jsonify({"error": "æ”¿ç­–ä¸å­˜åœ¨"}), 404
            return jsonify({"success": True, "data": policy})
        
        @self.app.route('/api/policies', methods=['GET'])
        def get_all_policies():
            return jsonify({
                "success": True,
                "data": self.mock_data["policies"],
                "count": len(self.mock_data["policies"])
            })
        
        # ç”¨æˆ·æƒé™API
        @self.app.route('/api/users/<username>/permissions', methods=['GET'])
        def get_user_permissions(username):
            user = self.mock_data["users"].get(username)
            if not user:
                return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404
            return jsonify({
                "success": True,
                "data": {
                    "username": username,
                    "permissions": user["permissions"],
                    "role": user["role"]
                }
            })
    
    def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

# ==================== LLMRequestsChainå®ç° ====================

class OllamaLLMRequestsChain:
    """å…¼å®¹Ollamaçš„LLMRequestsChainå®ç°"""
    
    def __init__(self, llm, prompt_template):
        self.llm = llm
        self.prompt_template = prompt_template
        self.llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    def invoke(self, inputs):
        """æ‰§è¡Œé“¾å¼è°ƒç”¨"""
        query = inputs["query"]
        url = inputs["url"]
        
        try:
            # è·å–APIæ•°æ®
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            api_data = response.json()
            
            # æ„å»ºæç¤º
            prompt = self.prompt_template.format(
                query=query,
                api_response=json.dumps(api_data, ensure_ascii=False, indent=2)
            )
            
            # è°ƒç”¨LLM
            result = self.llm.invoke(prompt)
            return {"output": result, "success": True}
            
        except requests.exceptions.RequestException as e:
            return {
                "output": f"APIè¯·æ±‚å¤±è´¥: {str(e)}",
                "success": False,
                "error": str(e)
            }
        except Exception as e:
            return {
                "output": f"å¤„ç†å¤±è´¥: {str(e)}",
                "success": False,
                "error": str(e)
            }

class EnterpriseSmartAssistant:
    """ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹ - å®Œæ•´å®ç°"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0.3
        )
        self.setup_chains()
    
    def setup_chains(self):
        """è®¾ç½®å„ç§æŸ¥è¯¢é“¾"""
        
        # å‘˜å·¥æŸ¥è¯¢é“¾
        employee_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”å›ç­”å‘˜å·¥å‡æœŸé—®é¢˜ï¼š

é—®é¢˜ï¼š{query}
APIå“åº”ï¼š{api_response}

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼ŒåŒ…å«å…³é”®æ•°å­—å’Œéƒ¨é—¨ä¿¡æ¯ã€‚

å›ç­”æ ¼å¼ï¼š
å‘˜å·¥[å§“å]åœ¨[éƒ¨é—¨]éƒ¨é—¨ï¼Œ
- å¹´å‡æ€»å¤©æ•°ï¼š[X]å¤©
- å·²ä½¿ç”¨ï¼š[X]å¤©  
- å‰©ä½™ï¼š[X]å¤©
- ç—…å‡ï¼š[X]å¤©

å…¶ä»–ç›¸å…³ä¿¡æ¯è¯·ä¸€å¹¶è¯´æ˜ã€‚"""
        )
        self.employee_chain = OllamaLLMRequestsChain(self.llm, employee_prompt)
        
        # æ”¿ç­–æŸ¥è¯¢é“¾  
        policy_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”è§£é‡Šæ”¿ç­–ï¼š

é—®é¢˜ï¼š{query}
æ”¿ç­–å†…å®¹ï¼š{api_response}

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£é‡Šæ”¿ç­–è¦ç‚¹ï¼Œåˆ†ç‚¹è¯´æ˜ï¼š

1. æ”¿ç­–é€‚ç”¨èŒƒå›´
2. å…·ä½“è§„å®šå†…å®¹
3. ç”³è¯·æµç¨‹ï¼ˆå¦‚æœ‰ï¼‰
4. æ³¨æ„äº‹é¡¹

æ”¿ç­–ç”Ÿæ•ˆæ—¶é—´ä¹Ÿè¯·ä¸€å¹¶è¯´æ˜ã€‚"""
        )
        self.policy_chain = OllamaLLMRequestsChain(self.llm, policy_prompt)
        
        # ä¾›åº”å•†æŸ¥è¯¢é“¾
        supplier_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”æä¾›ä¾›åº”å•†ä¿¡æ¯ï¼š

é—®é¢˜ï¼š{query}
ä¾›åº”å•†ä¿¡æ¯ï¼š{api_response}

è¯·ç”¨ä¸­æ–‡åˆ—å‡ºä¾›åº”å•†è¯¦ç»†ä¿¡æ¯ï¼š

ğŸ“‹ ä¾›åº”å•†åˆ—è¡¨ï¼š
{ä¾›åº”å•†ä¿¡æ¯}

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š
- æ€»ä¾›åº”å•†æ•°é‡ï¼š[X]å®¶
- å¹³å‡è¯„åˆ†ï¼š[X]åˆ†
- ä¸»è¦ç±»åˆ«ï¼š[åˆ—å‡ºç±»åˆ«]

å¦‚æœ‰å…·ä½“ä¾›åº”å•†ï¼Œè¯·æä¾›è”ç³»æ–¹å¼å’Œä¸»è¥äº§å“ã€‚"""
        )
        self.supplier_chain = OllamaLLMRequestsChain(self.llm, supplier_prompt)
        
        # æƒé™æŸ¥è¯¢é“¾
        permission_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”å›ç­”æƒé™é—®é¢˜ï¼š

é—®é¢˜ï¼š{query}
æƒé™ä¿¡æ¯ï¼š{api_response}

ç”¨æˆ·æƒé™è¯¦æƒ…ï¼š
- ç”¨æˆ·åï¼š[ç”¨æˆ·å]
- è§’è‰²ï¼š[è§’è‰²]
- æƒé™åˆ—è¡¨ï¼š
{æƒé™åˆ—è¡¨}

æƒé™è¯´æ˜ï¼š
- read: è¯»å–æƒé™
- write: å†™å…¥æƒé™  
- delete: åˆ é™¤æƒé™
- admin: ç®¡ç†æƒé™

è¯·æ ¹æ®å®é™…æƒé™ç»™å‡ºä½¿ç”¨å»ºè®®ã€‚"""
        )
        self.permission_chain = OllamaLLMRequestsChain(self.llm, permission_prompt)
    
    def query_employee_leave(self, employee_name: str) -> Dict[str, Any]:
        """æŸ¥è¯¢å‘˜å·¥å‡æœŸ"""
        return self.employee_chain.invoke({
            "query": f"æŸ¥è¯¢{employee_name}çš„å‡æœŸä½™é¢",
            "url": f"{self.base_url}/api/employees/{employee_name}/leave-balance"
        })
    
    def query_employee_profile(self, employee_name: str) -> Dict[str, Any]:
        """æŸ¥è¯¢å‘˜å·¥æ¡£æ¡ˆ"""
        return self.employee_chain.invoke({
            "query": f"æŸ¥è¯¢{employee_name}çš„è¯¦ç»†ä¿¡æ¯",
            "url": f"{self.base_url}/api/employees/{employee_name}/profile"
        })
    
    def query_maternity_policy(self) -> Dict[str, Any]:
        """æŸ¥è¯¢äº§å‡æ”¿ç­–"""
        return self.policy_chain.invoke({
            "query": "äº§å‡æ”¿ç­–æ˜¯ä»€ä¹ˆ",
            "url": f"{self.base_url}/api/policies/maternity"
        })
    
    def query_all_policies(self) -> Dict[str, Any]:
        """æŸ¥è¯¢æ‰€æœ‰æ”¿ç­–"""
        return self.policy_chain.invoke({
            "query": "æŸ¥è¯¢æ‰€æœ‰æ”¿ç­–",
            "url": f"{self.base_url}/api/policies"
        })
    
    def query_suppliers(self, category: str = None, name: str = None) -> Dict[str, Any]:
        """æŸ¥è¯¢ä¾›åº”å•†"""
        if name:
            return self.supplier_chain.invoke({
                "query": f"æŸ¥è¯¢åç§°ä¸º{name}çš„ä¾›åº”å•†",
                "url": f"{self.base_url}/api/suppliers?name={name}"
            })
        elif category:
            return self.supplier_chain.invoke({
                "query": f"æŸ¥è¯¢{category}ç±»åˆ«çš„ä¾›åº”å•†",
                "url": f"{self.base_url}/api/suppliers/category/{category}"
            })
        else:
            return self.supplier_chain.invoke({
                "query": "æŸ¥è¯¢æ‰€æœ‰ä¾›åº”å•†",
                "url": f"{self.base_url}/api/suppliers"
            })
    
    def query_user_permissions(self, username: str) -> Dict[str, Any]:
        """æŸ¥è¯¢ç”¨æˆ·æƒé™"""
        return self.permission_chain.invoke({
            "query": f"æŸ¥è¯¢ç”¨æˆ·{username}çš„æƒé™",
            "url": f"{self.base_url}/api/users/{username}/permissions"
        })

# ==================== å®Œæ•´æ¼”ç¤ºç¨‹åº ====================

def run_complete_demo():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    
    print("ğŸš€ LLMRequestsChainä¼ä¸šæ™ºèƒ½åŠ©æ‰‹å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # å¯åŠ¨MockæœåŠ¡å™¨
    mock_api = MockEnterpriseAPI()
    server_thread = threading.Thread(target=mock_api.start_server, daemon=True)
    server_thread.start()
    time.sleep(3)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    print("âœ… Mockä¼ä¸šAPIæœåŠ¡å™¨å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–åŠ©æ‰‹
    assistant = EnterpriseSmartAssistant()
    
    # æ¼”ç¤ºå„ç§æŸ¥è¯¢
    demo_queries = [
        # å‘˜å·¥ç›¸å…³
        ("å‘˜å·¥å‡æœŸ", lambda: assistant.query_employee_leave("å¼ ä¸‰")),
        ("å‘˜å·¥æ¡£æ¡ˆ", lambda: assistant.query_employee_profile("æå››")),
        
        # æ”¿ç­–ç›¸å…³
        ("äº§å‡æ”¿ç­–", lambda: assistant.query_maternity_policy()),
        ("æ‰€æœ‰æ”¿ç­–", lambda: assistant.query_all_policies()),
        
        # ä¾›åº”å•†ç›¸å…³
        ("ç”µå­ä¾›åº”å•†", lambda: assistant.query_suppliers(category="electronics")),
        ("é£Ÿå“ä¾›åº”å•†", lambda: assistant.query_suppliers(category="food")),
        ("æ‰€æœ‰ä¾›åº”å•†", lambda: assistant.query_suppliers()),
        
        # æƒé™ç›¸å…³
        ("ç®¡ç†å‘˜æƒé™", lambda: assistant.query_user_permissions("admin")),
        ("æ™®é€šç”¨æˆ·æƒé™", lambda: assistant.query_user_permissions("user1"))
    ]
    
    for query_name, query_func in demo_queries:
        print(f"\nğŸ“‹ {query_name}æŸ¥è¯¢:")
        try:
            result = query_func()
            if result["success"]:
                print(f"âœ… ç»“æœ: {result['output']}")
            else:
                print(f"âŒ å¤±è´¥: {result['output']}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
        
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

if __name__ == "__main__":
    run_complete_demo()
```

é€šè¿‡ä»¥ä¸Šå‡çº§å†…å®¹å’Œå®Œæ•´ä»£ç å®ç°ï¼Œä½ ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªåŒ…å«9ç§æ ¸å¿ƒé“¾ç±»å‹çš„å®Œæ•´LangChainå®æˆ˜æŒ‡å—ã€‚æ¯ä¸ªç¤ºä¾‹éƒ½æä¾›äº†å®Œæ•´çš„å¯è¿è¡Œä»£ç ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨å¹¶æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

è®°ä½ï¼Œé“¾çš„çœŸæ­£å¨åŠ›åœ¨äºå®ƒä»¬çš„ç»„åˆèƒ½åŠ› - é€šè¿‡å·§å¦™åœ°ç»„åˆä¸åŒçš„é“¾ï¼Œå¯ä»¥è§£å†³å‡ ä¹ä»»ä½•å¤æ‚çš„AIåº”ç”¨åœºæ™¯ã€‚å»ºè®®å…ˆè¿è¡ŒåŸºç¡€ç¤ºä¾‹ï¼Œç†è§£æ¯ç§é“¾çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç„¶åæ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œç»„åˆå’Œåˆ›æ–°ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å—

1. **å®‰è£…ä¾èµ–**:
```bash
pip install langchain langchain-ollama langchain-core flask requests
```

2. **å¯åŠ¨OllamaæœåŠ¡**:
```bash
ollama serve
ollama pull gpt-3.5-turbo:latest
```

3. **è¿è¡Œç¤ºä¾‹**:
```bash
# è¿è¡ŒåŸºç¡€é“¾ç¤ºä¾‹
python 80_chain_examples.py

# è¿è¡ŒAPIé“¾å’ŒLLMRequestsChainç¤ºä¾‹
python 82_api_requests_ollama.py
```

å¸Œæœ›è¿™äº›è¯¦ç»†çš„ä»£ç å’Œå®ç°èƒ½å¸®åŠ©ä½ å¿«é€ŸæŒæ¡LangChainçš„å„ç§é“¾å¼åº”ç”¨ï¼