#!/usr/bin/env python3
"""
LangChain Chain ç¤ºä¾‹è„šæœ¬ - åŸºäºOllamaæœ¬åœ°æ¨¡å‹
åŸºäº79_chain.mdæ–‡æ¡£æ„å»ºçš„5ç§Chainæ¼”ç¤º
ä½¿ç”¨æœ¬åœ°Ollamaçš„gpt-3.5-turbo:latestæ¨¡å‹
"""

import os
import json
from typing import List, Dict, Any
from langchain_ollama import OllamaLLM as Ollama
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chains import LLMChain, SequentialChain
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_extraction_chain
from langchain.chains import LLMMathChain

def setup_ollama_model():
    """åˆå§‹åŒ–Ollamaæ¨¡å‹"""
    return Ollama(
        model="gpt-3.5-turbo:latest",
        base_url="http://localhost:11434",
        temperature=0.7
    )

class LangChainExamples:
    """LangChainç¤ºä¾‹ç±»"""
    
    def __init__(self):
        self.llm = setup_ollama_model()
        
    def example_1_router_chain(self):
        """ç¤ºä¾‹1ï¼šè·¯ç”±é“¾ - æ™ºèƒ½å®¢æœé—®é¢˜åˆ†ç±»"""
        print("\nğŸ§© ç¤ºä¾‹1ï¼šè·¯ç”±é“¾ - æ™ºèƒ½å®¢æœé—®é¢˜åˆ†ç±»")
        print("-" * 50)
        
        # å®šä¹‰æç¤ºæ¨¡æ¿
        flower_care_template = """ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å›­ä¸ï¼Œæ“…é•¿è§£ç­”å…³äºå…»èŠ±è‚²èŠ±çš„é—®é¢˜ã€‚
                                ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                                {input}"""

        flower_deco_template = """ä½ æ˜¯ä¸€ä½ç½‘çº¢æ’èŠ±å¤§å¸ˆï¼Œæ“…é•¿è§£ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜ã€‚
                                ä¸‹é¢æ˜¯éœ€è¦ä½ æ¥å›ç­”çš„é—®é¢˜:
                                {input}"""

        # æ„å»ºæç¤ºä¿¡æ¯åˆ—è¡¨
        prompt_infos = [
            {
                "key": "flower_care",
                "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±æŠ¤ç†çš„é—®é¢˜",
                "template": flower_care_template,
            },
            {
                "key": "flower_decoration",
                "description": "é€‚åˆå›ç­”å…³äºé²œèŠ±è£…é¥°çš„é—®é¢˜",
                "template": flower_deco_template,
            }
        ]

        # æ„å»ºç›®æ ‡é“¾å­—å…¸
        chain_map = {}
        for info in prompt_infos:
            prompt = PromptTemplate(template=info['template'], input_variables=["input"])
            chain = LLMChain(llm=self.llm, prompt=prompt)
            chain_map[info["key"]] = chain

        # æ„å»ºè·¯ç”±é“¾
        destinations = [f"{p['key']}: {p['description']}" for p in prompt_infos]
        router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations="\n".join(destinations))
        router_prompt = PromptTemplate(
            template=router_template,
            input_variables=["input"],
            output_parser=RouterOutputParser(),
        )
        router_chain = LLMRouterChain.from_llm(self.llm, router_prompt)

        # æ„å»ºé»˜è®¤é“¾
        default_prompt = PromptTemplate(template="{input}", input_variables=["input"])
        default_chain = LLMChain(llm=self.llm, prompt=default_prompt)

        # è·¯ç”±å‡½æ•°
        def route_question(question):
            try:
                destination_info = router_chain.invoke({"input": question})
                destination = destination_info.get("destination", "default")
                
                if destination in chain_map:
                    return chain_map[destination].invoke({"input": question})
                else:
                    return default_chain.invoke({"input": question})
            except Exception as e:
                print(f"è·¯ç”±é“¾æ‰§è¡Œé”™è¯¯: {e}")
                return default_chain.invoke({"input": question})

        # æµ‹è¯•ç”¨ä¾‹
        test_questions = [
            "ç«ç‘°èŠ±åº”è¯¥å¤šä¹…æµ‡ä¸€æ¬¡æ°´ï¼Ÿ",
            "å©šç¤¼ç°åœºç”¨ç«ç‘°èŠ±å’Œæ»¡å¤©æ˜Ÿæ€ä¹ˆæ­é…ï¼Ÿ",
            "å‘æ—¥è‘µé€‚åˆæ”¾åœ¨å§å®¤å—ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = route_question(question)
                print(f"å›ç­”: {result.get('text', str(result))}")
            except Exception as e:
                print(f"æ‰§è¡Œå¤±è´¥: {e}")

    def example_2_sequential_chain(self):
        """ç¤ºä¾‹2ï¼šé¡ºåºé“¾ - ç”¨æˆ·è¯„è®ºåˆ†æä¸å¤šè¯­è¨€å›å¤"""
        print("\nğŸ”„ ç¤ºä¾‹2ï¼šé¡ºåºé“¾ - ç”¨æˆ·è¯„è®ºåˆ†æä¸å¤šè¯­è¨€å›å¤")
        print("-" * 50)
        
        # è®¾ç½®è¾ƒä½temperatureä»¥ä¿è¯å‡†ç¡®æ€§
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0.3
        )
        
        # å­é“¾1ï¼šå°†ä¸­æ–‡è¯„è®ºç¿»è¯‘æˆè‹±æ–‡
        prompt_z2e = PromptTemplate.from_template("å°†ä¸‹é¢çš„ä¸­æ–‡è¯„è®ºç¿»è¯‘ä¸ºè‹±æ–‡ï¼š\n\n{ch_review}")
        chain_z2e = LLMChain(llm=llm, prompt=prompt_z2e, output_key="en_review")

        # å­é“¾2ï¼šæ€»ç»“è‹±æ–‡è¯„è®º
        prompt_es = PromptTemplate.from_template("Can you summarize the following review in 1 sentence: \n\n{en_review}")
        chain_es = LLMChain(llm=llm, prompt=prompt_es, output_key="summary")

        # å­é“¾3ï¼šè¯†åˆ«è¯„è®ºåŸè¯­è¨€
        prompt_lang = PromptTemplate.from_template("ä¸‹é¢çš„è¯„è®ºä½¿ç”¨çš„æ˜¯ä»€ä¹ˆè¯­è¨€ï¼Ÿ:\n\n{ch_review}")
        chain_lang = LLMChain(llm=llm, prompt=prompt_lang, output_key="language")

        # å­é“¾4ï¼šç”¨åŸè¯­è¨€ç”Ÿæˆå›å¤
        prompt_reply = PromptTemplate.from_template(
            "ä½¿ç”¨æŒ‡å®šè¯­è¨€ç¼–å†™å¯¹ä»¥ä¸‹æ‘˜è¦çš„åç»­å›å¤ï¼š\n\næ‘˜è¦ï¼š{summary}\n\nè¯­è¨€ï¼š{language}"
        )
        chain_reply = LLMChain(llm=llm, prompt=prompt_reply, output_key="orig_reply")

        # å­é“¾5ï¼šå°†å›å¤ç¿»è¯‘æˆä¸­æ–‡
        prompt_e2z = PromptTemplate.from_template("å°†ä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š\n\n{orig_reply}")
        chain_e2z = LLMChain(llm=llm, prompt=prompt_e2z, output_key="ch_reply")

        # æ„å»ºé¡ºåºé“¾
        overall_chain = SequentialChain(
            chains=[chain_z2e, chain_es, chain_lang, chain_reply, chain_e2z],
            input_variables=["ch_review"],
            output_variables=["en_review", "summary", "language", "orig_reply", "ch_reply"],
            verbose=True
        )

        # æµ‹è¯•
        chinese_review = "å®«å´éªä»¥å¾€çš„ä½œå“å‰§ä½œå·¥æ•´ã€å½¢å¼ç»Ÿä¸€ï¼Œè€Œä¸”å¤§å¤šèƒ½è®©è§‚ä¼—æç‚¼å‡ºå‘å–„å‘ç¾çš„ä¸­å¿ƒæ€æƒ³ã€‚"
        
        try:
            result = overall_chain.invoke({"ch_review": chinese_review})
            print(f"åŸå§‹è¯„è®º: {chinese_review}")
            print(f"è‹±æ–‡ç¿»è¯‘: {result['en_review']}")
            print(f"æ‘˜è¦: {result['summary']}")
            print(f"è¯­è¨€: {result['language']}")
            print(f"åŸè¯­è¨€å›å¤: {result['orig_reply']}")
            print(f"ä¸­æ–‡å›å¤: {result['ch_reply']}")
        except Exception as e:
            print(f"é¡ºåºé“¾æ‰§è¡Œé”™è¯¯: {e}")

    def example_3_document_chain(self):
        """ç¤ºä¾‹3ï¼šæ–‡æ¡£é—®ç­”é“¾ - åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”"""
        print("\nğŸ“Š ç¤ºä¾‹3ï¼šæ–‡æ¡£é—®ç­”é“¾ - åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”")
        print("-" * 50)
        
        # åˆå§‹åŒ–æ¨¡å‹
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0.1
        )
        
        # æ„å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡: {context} \n\n å›ç­”é—®é¢˜: {input}"),
        ])

        # æ„å»ºæ–‡æ¡£é“¾
        document_chain = create_stuff_documents_chain(llm, prompt)

        # å‡†å¤‡æ–‡æ¡£
        docs = [
            Document(page_content="æ°è¥¿å–œæ¬¢çº¢è‰²ï¼Œä½†ä¸å–œæ¬¢é»„è‰²"),
            Document(page_content="è´¾é©¬å°”å–œæ¬¢ç»¿è‰²ï¼Œæœ‰ä¸€ç‚¹å–œæ¬¢çº¢è‰²"),
            Document(page_content="ç›ä¸½å–œæ¬¢ç²‰è‰²å’Œçº¢è‰²")
        ]

        # æµ‹è¯•ç”¨ä¾‹
        questions = [
            "å¤§å®¶å–œæ¬¢ä»€ä¹ˆé¢œè‰²?",
            "è°å–œæ¬¢çº¢è‰²ï¼Ÿ",
            "æ°è¥¿å–œæ¬¢ä»€ä¹ˆé¢œè‰²ï¼Ÿ"
        ]
        
        for question in questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = document_chain.invoke({"input": question, "context": docs})
                print(f"å›ç­”: {result}")
            except Exception as e:
                print(f"æ‰§è¡Œå¤±è´¥: {e}")

    def example_4_extraction_chain(self):
        """ç¤ºä¾‹4ï¼šä¿¡æ¯æå–é“¾ - ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        print("\nğŸ” ç¤ºä¾‹4ï¼šä¿¡æ¯æå–é“¾ - ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯")
        print("-" * 50)
        
        # è®¾ç½®è¾ƒä½temperatureä»¥ä¿è¯å‡†ç¡®æ€§
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0
        )
        
        # å®šä¹‰æå–æ¨¡å¼
        schema = {
            "properties": {
                "name": {"type": "string"},
                "height": {"type": "integer"},
                "hair_color": {"type": "string"},
            },
            "required": ["name", "height"],
        }

        # åˆ›å»ºæå–é“¾
        extraction_chain = create_extraction_chain(schema, llm)

        # æµ‹è¯•ç”¨ä¾‹
        test_texts = [
            "äºšå†å…‹æ–¯èº«é«˜ 5 è‹±å°ºã€‚å…‹åŠ³è¿ªå¨…æ¯”äºšå†å…‹æ–¯é«˜ 1 è‹±å°ºï¼Œå¹¶ä¸”è·³å¾—æ¯”ä»–æ›´é«˜ã€‚å…‹åŠ³è¿ªå¨…æ˜¯é»‘å‘å¥³éƒï¼Œäºšå†å…‹æ–¯æ˜¯é‡‘å‘å¥³éƒã€‚",
            "å°æ˜èº«é«˜180å˜ç±³ï¼Œå°çº¢èº«é«˜165å˜ç±³ï¼Œå°æ˜çš„å¤´å‘æ˜¯é»‘è‰²çš„ã€‚",
            "å¼ ä¸‰å’Œæå››éƒ½æ˜¯å­¦ç”Ÿï¼Œå¼ ä¸‰èº«é«˜175å˜ç±³ï¼Œæå››èº«é«˜170å˜ç±³ï¼Œå¼ ä¸‰çš„å¤´å‘æ˜¯æ£•è‰²çš„ã€‚"
        ]
        
        for text in test_texts:
            print(f"\næ–‡æœ¬: {text}")
            try:
                result = extraction_chain.invoke(text)
                print(f"æå–ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
            except Exception as e:
                print(f"æå–å¤±è´¥: {e}")

    def example_5_math_chain(self):
        """ç¤ºä¾‹5ï¼šæ•°å­¦é“¾ - è§£å†³æ•°å­¦è®¡ç®—é—®é¢˜"""
        print("\nğŸ§® ç¤ºä¾‹5ï¼šæ•°å­¦é“¾ - è§£å†³æ•°å­¦è®¡ç®—é—®é¢˜")
        print("-" * 50)
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ•°å­¦é—®é¢˜éœ€è¦å‡†ç¡®æ€§ï¼‰
        llm = Ollama(
            model="gpt-3.5-turbo:latest",
            base_url="http://localhost:11434",
            temperature=0
        )
        
        # åˆ›å»ºæ•°å­¦é“¾
        llm_math_chain = LLMMathChain.from_llm(llm)

        # æµ‹è¯•ç”¨ä¾‹
        math_questions = [
            "100 * 20 + 100çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ",
            "è®¡ç®—åœ†çš„é¢ç§¯ï¼Œå¦‚æœåŠå¾„æ˜¯5å˜ç±³ã€‚è¯·ä½¿ç”¨3.14ä½œä¸ºåœ†å‘¨ç‡ã€‚",
            "ä¸€ä¸ªé•¿æ–¹å½¢çš„é•¿æ˜¯12ç±³ï¼Œå®½æ˜¯8ç±³ï¼Œæ±‚é¢ç§¯å’Œå‘¨é•¿ã€‚",
            "è§£æ–¹ç¨‹ï¼š2x + 5 = 15"
        ]
        
        for question in math_questions:
            print(f"\né—®é¢˜: {question}")
            try:
                result = llm_math_chain.invoke(question)
                print(f"ç­”æ¡ˆ: {result.get('answer', str(result))}")
            except Exception as e:
                print(f"è®¡ç®—å¤±è´¥: {e}")

    def run_all_examples(self):
        """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
        print("ğŸš€ LangChain Chain ç¤ºä¾‹æ¼”ç¤º")
        print("=" * 60)
        print("ä½¿ç”¨æœ¬åœ°Ollamaçš„gpt-3.5-turbo:latestæ¨¡å‹")
        print("=" * 60)
        
        # æ£€æŸ¥OllamaæœåŠ¡
        try:
            test_response = self.llm.invoke("Hello")
            print("âœ… OllamaæœåŠ¡è¿æ¥æ­£å¸¸")
        except Exception as e:
            print(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨: ollama serve")
            return
        
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        try:
            self.example_1_router_chain()
        except Exception as e:
            print(f"è·¯ç”±é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_2_sequential_chain()
        except Exception as e:
            print(f"é¡ºåºé“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_3_document_chain()
        except Exception as e:
            print(f"æ–‡æ¡£é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_4_extraction_chain()
        except Exception as e:
            print(f"æå–é“¾ç¤ºä¾‹å¤±è´¥: {e}")
            
        try:
            self.example_5_math_chain()
        except Exception as e:
            print(f"æ•°å­¦é“¾ç¤ºä¾‹å¤±è´¥: {e}")

if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„ä¾èµ–
    try:
        import langchain_ollama
        import langchain_core
        import langchain
    except ImportError:
        print("è¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
        print("pip install langchain-ollama langchain-core langchain")
        exit(1)
    
    # è¿è¡Œç¤ºä¾‹
    examples = LangChainExamples()
    examples.run_all_examples()