#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LangChainæ–‡æ¡£è½¬æ¢å™¨å®Œæ•´å®ç°
åŒ…å«æ‰€æœ‰æ ¸å¿ƒè½¬æ¢å™¨çš„ç”Ÿäº§çº§å®ç°
"""

import os
import re
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import ast
import tiktoken

# æ¨¡æ‹ŸDocumentç±»
@dataclass
class Document:
    page_content: str
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

# åŸºç¡€å¼‚å¸¸ç±»
class DocumentTransformerError(Exception):
    """æ–‡æ¡£è½¬æ¢å™¨åŸºç¡€å¼‚å¸¸"""
    pass

# =============================================================================
# æ ¸å¿ƒæ–‡æœ¬åˆ†å‰²å™¨å®ç°
# =============================================================================

class CharacterTextSplitter:
    """å­—ç¬¦çº§æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        separator: str = "\n\n",
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: callable = len,
        is_separator_regex: bool = False
    ):
        self.separator = separator
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function
        self.is_separator_regex = is_separator_regex
    
    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        if self.length_function(text) <= self.chunk_size:
            return [text]
        
        # å¤„ç†åˆ†éš”ç¬¦
        if self.is_separator_regex:
            splits = re.split(self.separator, text)
        else:
            splits = text.split(self.separator)
        
        # åˆå¹¶å°ç‰‡æ®µ
        chunks = []
        current_chunk = ""
        separator_len = len(self.separator) if not self.is_separator_regex else 0
        
        for split in splits:
            if self.length_function(current_chunk + split) <= self.chunk_size:
                current_chunk += split + self.separator
            else:
                if current_chunk:
                    chunks.append(current_chunk.rstrip(self.separator))
                current_chunk = split + self.separator
        
        if current_chunk:
            chunks.append(current_chunk.rstrip(self.separator))
        
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in documents:
            texts = self.split_text(doc.page_content)
            for i, text in enumerate(texts):
                new_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_index': i,
                        'total_chunks': len(texts),
                        'splitter_type': 'CharacterTextSplitter'
                    }
                )
                result.append(new_doc)
        return result

class TokenTextSplitter:
    """Tokençº§æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        encoding_name: str = "cl100k_base",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        self.encoding = tiktoken.get_encoding(encoding_name)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text: str) -> List[str]:
        """åŸºäºtokençš„åˆ†å‰²"""
        tokens = self.encoding.encode(text)
        chunks = []
        
        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):
            chunk_tokens = tokens[i:i + self.chunk_size]
            chunk_text = self.encoding.decode(chunk_tokens)
            chunks.append(chunk_text)
        
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in documents:
            texts = self.split_text(doc.page_content)
            for i, text in enumerate(texts):
                new_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_index': i,
                        'total_chunks': len(texts),
                        'token_count': len(self.encoding.encode(text)),
                        'splitter_type': 'TokenTextSplitter'
                    }
                )
                result.append(new_doc)
        return result

class RecursiveCharacterTextSplitter:
    """é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        separators: Optional[List[str]] = None,
        chunk_size: int = 4000,
        chunk_overlap: int = 200,
        length_function: callable = len
    ):
        self.separators = separators or ["\n\n", "\n", ". ", " ", ""]
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function
    
    def split_text(self, text: str) -> List[str]:
        """é€’å½’åˆ†å‰²æ–‡æœ¬"""
        return self._split_text(text, self.separators)
    
    def _split_text(self, text: str, separators: List[str]) -> List[str]:
        """é€’å½’åˆ†å‰²æ ¸å¿ƒç®—æ³•"""
        if self.length_function(text) <= self.chunk_size:
            return [text]
        
        for separator in separators:
            if separator == "":
                # å­—ç¬¦çº§åˆ†å‰²
                return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
            
            splits = text.split(separator)
            if len(splits) > 1:
                chunks = []
                current_chunk = ""
                
                for split in splits:
                    if self.length_function(current_chunk + separator + split) <= self.chunk_size:
                        if current_chunk:
                            current_chunk += separator + split
                        else:
                            current_chunk = split
                    else:
                        if current_chunk:
                            chunks.append(current_chunk)
                        
                        # å¤„ç†å¤§æ®µè½
                        if self.length_function(split) > self.chunk_size:
                            remaining_chunks = self._split_text(split, separators[separators.index(separator)+1:])
                            chunks.extend(remaining_chunks)
                            current_chunk = ""
                        else:
                            current_chunk = split
                
                if current_chunk:
                    chunks.append(current_chunk)
                
                return chunks
        
        return [text]
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in documents:
            texts = self.split_text(doc.page_content)
            for i, text in enumerate(texts):
                new_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_index': i,
                        'total_chunks': len(texts),
                        'splitter_type': 'RecursiveCharacterTextSplitter'
                    }
                )
                result.append(new_doc)
        return result

class MarkdownTextSplitter:
    """Markdownæ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 2000,
        chunk_overlap: int = 200
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text: str) -> List[str]:
        """ä¿æŒMarkdownç»“æ„çš„åˆ†å‰²"""
        # è¯†åˆ«Markdownç»“æ„
        sections = self._parse_markdown_sections(text)
        
        chunks = []
        current_chunk = ""
        
        for section in sections:
            section_text = section['content']
            section_size = len(section_text)
            
            if len(current_chunk + section_text) <= self.chunk_size:
                current_chunk += section_text
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¤„ç†å¤§æ®µè½
                if section_size > self.chunk_size:
                    # åœ¨æ®µè½å†…ç»§ç»­åˆ†å‰²
                    sub_chunks = self._split_large_section(section_text)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = section_text
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _parse_markdown_sections(self, text: str) -> List[Dict[str, Any]]:
        """è§£æMarkdownç»“æ„"""
        sections = []
        lines = text.split('\n')
        
        current_section = {"type": "text", "content": "", "level": 0}
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹æ ‡é¢˜
            if line.startswith('#'):
                if current_section["content"].strip():
                    sections.append(current_section)
                
                level = line.count('#', 0, line.find(' ')) or line.count('#')
                title = line.lstrip('#').strip()
                
                current_section = {
                    "type": "heading",
                    "content": line + '\n',
                    "level": level,
                    "title": title
                }
            
            # æ£€æµ‹ä»£ç å—
            elif line.startswith('```'):
                if current_section["content"].strip():
                    sections.append(current_section)
                
                current_section = {
                    "type": "code_block",
                    "content": line + '\n',
                    "level": 0
                }
            
            else:
                current_section["content"] += line + '\n'
        
        if current_section["content"].strip():
            sections.append(current_section)
        
        return sections
    
    def _split_large_section(self, text: str) -> List[str]:
        """å¤§æ®µè½å†…åˆ†å‰²"""
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= self.chunk_size:
                current_chunk += sentence + '. '
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + '. '
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in documents:
            texts = self.split_text(doc.page_content)
            for i, text in enumerate(texts):
                new_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_index': i,
                        'total_chunks': len(texts),
                        'splitter_type': 'MarkdownTextSplitter'
                    }
                )
                result.append(new_doc)
        return result

# =============================================================================
# ä»£ç ä¸“ç”¨åˆ†å‰²å™¨
# =============================================================================

class PythonCodeTextSplitter:
    """Pythonä»£ç ä¸“ç”¨åˆ†å‰²å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 1500,
        chunk_overlap: int = 150
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, code: str) -> List[str]:
        """åŸºäºä»£ç ç»“æ„çš„åˆ†å‰²"""
        try:
            tree = ast.parse(code)
            blocks = self._extract_code_blocks(code, tree)
            
            chunks = []
            current_chunk = ""
            
            for block in blocks:
                block_text = block['content']
                if len(current_chunk + block_text) <= self.chunk_size:
                    current_chunk += block_text + '\n\n'
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    
                    # å¤„ç†å¤§ä»£ç å—
                    if len(block_text) > self.chunk_size:
                        sub_chunks = self._split_large_block(block_text)
                        chunks.extend(sub_chunks)
                        current_chunk = ""
                    else:
                        current_chunk = block_text + '\n\n'
            
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            return chunks
            
        except SyntaxError:
            # è¯­æ³•é”™è¯¯æ—¶é™çº§åˆ°å­—ç¬¦åˆ†å‰²
            return RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            ).split_text(code)
    
    def _extract_code_blocks(self, code: str, tree: ast.AST) -> List[Dict[str, Any]]:
        """æå–ä»£ç ç»“æ„"""
        lines = code.split('\n')
        blocks = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                start_line = node.lineno - 1
                end_line = node.end_lineno or start_line + 1
                content = '\n'.join(lines[start_line:end_line])
                
                blocks.append({
                    'type': 'function',
                    'name': node.name,
                    'content': content,
                    'start_line': start_line,
                    'end_line': end_line,
                    'docstring': ast.get_docstring(node) or ''
                })
            
            elif isinstance(node, ast.ClassDef):
                start_line = node.lineno - 1
                end_line = node.end_lineno or start_line + 1
                content = '\n'.join(lines[start_line:end_line])
                
                blocks.append({
                    'type': 'class',
                    'name': node.name,
                    'content': content,
                    'start_line': start_line,
                    'end_line': end_line,
                    'methods': [method.name for method in node.body if isinstance(method, ast.FunctionDef)]
                })
        
        return sorted(blocks, key=lambda x: x['start_line'])
    
    def _split_large_block(self, text: str) -> List[str]:
        """å¤§ä»£ç å—åˆ†å‰²"""
        lines = text.split('\n')
        chunks = []
        current_chunk = ""
        
        for line in lines:
            if len(current_chunk + line) <= self.chunk_size:
                current_chunk += line + '\n'
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = line + '\n'
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        result = []
        for doc in documents:
            texts = self.split_text(doc.page_content)
            for i, text in enumerate(texts):
                new_doc = Document(
                    page_content=text,
                    metadata={
                        **doc.metadata,
                        'chunk_index': i,
                        'total_chunks': len(texts),
                        'splitter_type': 'PythonCodeTextSplitter'
                    }
                )
                result.append(new_doc)
        return result

# =============================================================================
# æ€§èƒ½ä¼˜åŒ–å·¥å…·
# =============================================================================

class SplitQualityEvaluator:
    """åˆ†å‰²è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def evaluate_splits(self, original_text: str, chunks: List[str]) -> Dict[str, float]:
        """è¯„ä¼°åˆ†å‰²è´¨é‡"""
        scores = {
            'semantic_coherence': self._calculate_semantic_coherence(chunks),
            'size_uniformity': self._calculate_size_uniformity(chunks),
            'boundary_naturalness': self._calculate_boundary_naturalness(original_text, chunks),
            'compression_ratio': len(''.join(chunks)) / len(original_text)
        }
        return scores
    
    def _calculate_semantic_coherence(self, chunks: List[str]) -> float:
        """è®¡ç®—è¯­ä¹‰è¿è´¯æ€§"""
        if len(chunks) < 2:
            return 1.0
        
        # ç®€å•çš„é‡å è¯è®¡ç®—
        scores = []
        for i in range(1, len(chunks)):
            prev_words = set(chunks[i-1].lower().split())
            curr_words = set(chunks[i].lower().split())
            
            if prev_words and curr_words:
                overlap = len(prev_words.intersection(curr_words))
                score = overlap / max(len(prev_words), len(curr_words))
                scores.append(score)
        
        return np.mean(scores) if scores else 0.0
    
    def _calculate_size_uniformity(self, chunks: List[str]) -> float:
        """è®¡ç®—å¤§å°å‡åŒ€æ€§"""
        sizes = [len(chunk) for chunk in chunks]
        if len(sizes) <= 1:
            return 1.0
        
        mean_size = np.mean(sizes)
        std_size = np.std(sizes)
        
        # å˜å¼‚ç³»æ•°è¶Šå°è¶Šå‡åŒ€
        cv = std_size / mean_size if mean_size > 0 else 0
        return max(0, 1 - cv)
    
    def _calculate_boundary_naturalness(self, original: str, chunks: List[str]) -> float:
        """è®¡ç®—è¾¹ç•Œè‡ªç„¶åº¦"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        boundaries = []
        current_pos = 0
        
        for chunk in chunks[:-1]:  # é™¤äº†æœ€åä¸€ä¸ªchunk
            boundary_pos = current_pos + len(chunk)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­ç»“æŸ
            if boundary_pos < len(original):
                next_char = original[boundary_pos]
                is_sentence_end = next_char in '.!?'
                boundaries.append(1.0 if is_sentence_end else 0.0)
            
            current_pos = boundary_pos
        
        return np.mean(boundaries) if boundaries else 0.0

class DistributedDocumentProcessor:
    """åˆ†å¸ƒå¼æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_documents_parallel(
        self, 
        documents: List[Document], 
        splitter
    ) -> List[Document]:
        """å¹¶è¡Œå¤„ç†æ–‡æ¡£"""
        # å°†æ–‡æ¡£åˆ†ç»„
        batch_size = max(1, len(documents) // self.max_workers)
        batches = [documents[i:i+batch_size] for i in range(0, len(documents), batch_size)]
        
        # å¹¶è¡Œå¤„ç†
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.executor, self._process_batch, batch, splitter)
            for batch in batches
        ]
        
        results = await asyncio.gather(*tasks)
        
        # åˆå¹¶ç»“æœ
        all_chunks = []
        for batch_result in results:
            all_chunks.extend(batch_result)
        
        return all_chunks
    
    def _process_batch(self, documents: List[Document], splitter) -> List[Document]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        chunks = []
        for doc in documents:
            doc_chunks = splitter.split_documents([doc])
            chunks.extend(doc_chunks)
        return chunks

# =============================================================================
# å®é™…åº”ç”¨ç¤ºä¾‹
# =============================================================================

class EnterpriseDocumentProcessor:
    """ä¼ä¸šçº§æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.splitters = {
            'txt': RecursiveCharacterTextSplitter(
                chunk_size=4000,
                chunk_overlap=400,
                separators=["\n\n", "\n", ". ", " ", ""]
            ),
            'md': MarkdownTextSplitter(
                chunk_size=2000,
                chunk_overlap=200
            ),
            'py': PythonCodeTextSplitter(
                chunk_size=1500,
                chunk_overlap=150
            )
        }
        self.evaluator = SplitQualityEvaluator()
    
    def process_file(self, file_path: str, file_type: str = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        if file_type is None:
            file_type = file_path.split('.')[-1].lower()
        
        if file_type not in self.splitters:
            file_type = 'txt'
        
        # è¯»å–æ–‡ä»¶
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='gbk') as f:
                content = f.read()
        
        # åˆ›å»ºæ–‡æ¡£
        doc = Document(
            page_content=content,
            metadata={
                'source': file_path,
                'file_type': file_type,
                'file_size': len(content),
                'processed_at': datetime.now().isoformat()
            }
        )
        
        # åˆ†å‰²æ–‡æ¡£
        splitter = self.splitters[file_type]
        chunks = splitter.split_documents([doc])
        
        # è¯„ä¼°è´¨é‡
        chunk_texts = [chunk.page_content for chunk in chunks]
        quality_scores = self.evaluator.evaluate_splits(content, chunk_texts)
        
        return {
            'original_document': doc,
            'chunks': chunks,
            'chunk_count': len(chunks),
            'quality_scores': quality_scores,
            'processing_summary': {
                'file_path': file_path,
                'file_type': file_type,
                'original_length': len(content),
                'avg_chunk_size': np.mean([len(chunk.page_content) for chunk in chunks]),
                'processing_time': datetime.now().isoformat()
            }
        }
    
    def process_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†æ•´ä¸ªç›®å½•"""
        results = []
        
        for filename in os.listdir(directory_path):
            file_path = os.path.join(directory_path, filename)
            if os.path.isfile(file_path):
                try:
                    result = self.process_file(file_path)
                    results.append(result)
                    print(f"âœ“ å¤„ç†å®Œæˆ: {filename} -> {len(result['chunks'])} ä¸ªåˆ†å—")
                except Exception as e:
                    print(f"âœ— å¤„ç†å¤±è´¥: {filename} - {str(e)}")
        
        return results

# =============================================================================
# æ¼”ç¤ºå’Œæµ‹è¯•
# =============================================================================

class DocumentTransformerDemo:
    """æ–‡æ¡£è½¬æ¢å™¨æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.processor = EnterpriseDocumentProcessor()
    
    def create_sample_documents(self) -> List[Document]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        documents = []
        
        # ç¤ºä¾‹1ï¼šæŠ€æœ¯æ–‡æ¡£
        tech_doc = Document(
            page_content="""
            # LangChainæŠ€æœ¯æ–‡æ¡£
            
            ## æ¦‚è¿°
            LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
            
            ## æ ¸å¿ƒç»„ä»¶
            
            ### 1. æ¨¡å‹I/O
            LangChainæä¾›äº†æ ‡å‡†åŒ–çš„æ¥å£æ¥ä¸å„ç§è¯­è¨€æ¨¡å‹äº¤äº’ã€‚
            
            ### 2. æ•°æ®è¿æ¥
            æ”¯æŒå¤šç§æ•°æ®æºå’Œæ–‡æ¡£æ ¼å¼çš„åŠ è½½å’Œå¤„ç†ã€‚
            
            ## åº”ç”¨åœºæ™¯
            é€‚ç”¨äºé—®ç­”ç³»ç»Ÿã€èŠå¤©æœºå™¨äººã€æ–‡æ¡£åˆ†æç­‰å¤šç§åœºæ™¯ã€‚
            """,
            metadata={'source': 'tech_doc.md', 'type': 'markdown'}
        )
        
        # ç¤ºä¾‹2ï¼šPythonä»£ç 
        python_code = Document(
            page_content="""
            def calculate_similarity(text1: str, text2: str) -> float:
            \"\"\"è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦\"\"\"
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([text1, text2])
            
            return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            
            class DocumentProcessor:
                def __init__(self, chunk_size: int = 1000):
                    self.chunk_size = chunk_size
                
                def process(self, text: str) -> List[str]:
                    return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
            """,
            metadata={'source': 'example.py', 'type': 'python'}
        )
        
        # ç¤ºä¾‹3ï¼šæ™®é€šæ–‡æœ¬
        plain_text = Document(
            page_content="""
            äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
            
            è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚
            
            å¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚
            
            äººå·¥æ™ºèƒ½ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ ·æ€è€ƒã€ä¹Ÿå¯èƒ½è¶…è¿‡äººçš„æ™ºèƒ½ã€‚äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨æå¯ŒæŒ‘æˆ˜æ€§çš„ç§‘å­¦ï¼Œä»äº‹è¿™é¡¹å·¥ä½œçš„äººå¿…é¡»æ‡‚å¾—è®¡ç®—æœºçŸ¥è¯†ã€å¿ƒç†å­¦å’Œå“²å­¦ã€‚
            """,
            metadata={'source': 'plain_text.txt', 'type': 'text'}
        )
        
        documents.extend([tech_doc, python_code, plain_text])
        return documents
    
    def run_performance_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        documents = self.create_sample_documents()
        
        # æµ‹è¯•ä¸åŒçš„åˆ†å‰²å™¨
        splitters = {
            'Character': CharacterTextSplitter(chunk_size=1000, chunk_overlap=100),
            'Token': TokenTextSplitter(chunk_size=200, chunk_overlap=20),
            'Recursive': RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=100,
                separators=["\n\n", "\n", ". ", " ", ""]
            ),
            'Markdown': MarkdownTextSplitter(chunk_size=500, chunk_overlap=50),
            'PythonCode': PythonCodeTextSplitter(chunk_size=800, chunk_overlap=80)
        }
        
        results = {}
        evaluator = SplitQualityEvaluator()
        
        for splitter_name, splitter in splitters.items():
            print(f"\nğŸ§ª æµ‹è¯•åˆ†å‰²å™¨: {splitter_name}")
            
            splitter_results = []
            for doc in documents:
                start_time = datetime.now()
                chunks = splitter.split_documents([doc])
                end_time = datetime.now()
                
                processing_time = (end_time - start_time).total_seconds()
                
                # è¯„ä¼°è´¨é‡
                chunk_texts = [chunk.page_content for chunk in chunks]
                quality_scores = evaluator.evaluate_splits(doc.page_content, chunk_texts)
                
                result = {
                    'document_type': doc.metadata.get('type', 'unknown'),
                    'original_length': len(doc.page_content),
                    'chunk_count': len(chunks),
                    'avg_chunk_size': np.mean([len(c.page_content) for c in chunks]),
                    'processing_time': processing_time,
                    'quality_scores': quality_scores
                }
                
                splitter_results.append(result)
            
            results[splitter_name] = splitter_results
        
        return results
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("LangChainæ–‡æ¡£è½¬æ¢å™¨æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 60)
        
        for splitter_name, splitter_results in results.items():
            report.append(f"\nğŸ“Š {splitter_name} åˆ†å‰²å™¨")
            report.append("-" * 40)
            
            for result in splitter_results:
                report.append(f"æ–‡æ¡£ç±»å‹: {result['document_type']}")
                report.append(f"åŸæ–‡é•¿åº¦: {result['original_length']} å­—ç¬¦")
                report.append(f"åˆ†å—æ•°é‡: {result['chunk_count']}")
                report.append(f"å¹³å‡åˆ†å—å¤§å°: {result['avg_chunk_size']:.1f} å­—ç¬¦")
                report.append(f"å¤„ç†æ—¶é—´: {result['processing_time']:.3f} ç§’")
                report.append(f"è´¨é‡è¯„åˆ†: {json.dumps(result['quality_scores'], indent=2, ensure_ascii=False)}")
                report.append("")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    print("ğŸš€ LangChainæ–‡æ¡£è½¬æ¢å™¨å®Œæ•´å®ç°æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = DocumentTransformerDemo()
    
    # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\nğŸ“ˆ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    benchmark_results = demo.run_performance_benchmark()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = demo.generate_report(benchmark_results)
    print(report)
    
    # ä¿å­˜ç»“æœ
    with open('document_transformer_benchmark.json', 'w', encoding='utf-8') as f:
        json.dump(benchmark_results, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° document_transformer_benchmark.json")
    
    # å®é™…æ–‡ä»¶å¤„ç†ç¤ºä¾‹
    print("\nğŸ“ å®é™…æ–‡ä»¶å¤„ç†ç¤ºä¾‹")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    os.makedirs('sample_docs', exist_ok=True)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    with open('sample_docs/sample.md', 'w', encoding='utf-8') as f:
        f.write("""
# ç¤ºä¾‹Markdownæ–‡æ¡£

## ç¬¬ä¸€éƒ¨åˆ†
è¿™æ˜¯ä¸€ä¸ªå…³äºLangChainçš„ç¤ºä¾‹æ–‡æ¡£ã€‚

### å­éƒ¨åˆ†1
åŒ…å«ä¸€äº›æŠ€æœ¯ç»†èŠ‚å’Œä»£ç ç¤ºä¾‹ã€‚

## ç¬¬äºŒéƒ¨åˆ†
è®¨è®ºäº†å®é™…åº”ç”¨åœºæ™¯ã€‚

### å­éƒ¨åˆ†2
æä¾›äº†ä½¿ç”¨å»ºè®®å’Œæœ€ä½³å®è·µã€‚
""")
    
    with open('sample_docs/sample.py', 'w', encoding='utf-8') as f:
        f.write("""
def hello_world():
    \"\"\"æ‰“å°é—®å€™è¯­\"\"\"
    print("Hello, World!")

class Calculator:
    def __init__(self):
        self.history = []
    
    def add(self, a, b):
        result = a + b
        self.history.append(f"{a} + {b} = {result}")
        return result
""")
    
    # å¤„ç†ç¤ºä¾‹ç›®å½•
    processor = EnterpriseDocumentProcessor()
    results = processor.process_directory('sample_docs')
    
    print(f"\nâœ… å¤„ç†äº† {len(results)} ä¸ªæ–‡ä»¶")
    for result in results:
        summary = result['processing_summary']
        print(f"æ–‡ä»¶: {summary['file_path']}")
        print(f"åˆ†å—æ•°: {result['chunk_count']}")
        print(f"å¹³å‡åˆ†å—å¤§å°: {summary['avg_chunk_size']:.1f} å­—ç¬¦")
        print("-" * 30)

if __name__ == "__main__":
    main()