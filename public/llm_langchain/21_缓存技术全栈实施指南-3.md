# LangChainç¼“å­˜æŠ€æœ¯å…¨æ ˆå®æ–½æ–¹æ¡ˆ

ä½œä¸ºAIä¸LangChainèµ„æ·±ä¸“å®¶ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†é˜è¿°LangChainç”Ÿæ€ä¸­æä¾›çš„**å®˜æ–¹ç¼“å­˜API**çš„æŠ€æœ¯å®æ–½ã€‚è¿™ä¸æ˜¯æ™®é€šç¼“å­˜ï¼Œè€Œæ˜¯LangChainå†…ç½®çš„ã€ä¸“ä¸ºLLMä¼˜åŒ–çš„ç¼“å­˜ä½“ç³»ã€‚

## ğŸ“‹ ç›®å½•
1. [LangChainå®˜æ–¹ç¼“å­˜æ¶æ„](#langchainå®˜æ–¹ç¼“å­˜æ¶æ„)
2. [å†…å­˜ç¼“å­˜ - InMemoryCache](#å†…å­˜ç¼“å­˜-inmemorycache)
3. [SQLiteç¼“å­˜ - SQLiteCache](#sqliteç¼“å­˜-sqlitecache)
4. [SQLæ•°æ®åº“ç¼“å­˜ - SQLAlchemyCache](#sqlæ•°æ®åº“ç¼“å­˜-sqlalchemy)
5. [Redisç¼“å­˜ - RedisCache](#redisç¼“å­˜-rediscache)
6. [MongoDBè‡ªå®šä¹‰ç¼“å­˜](#mongodbè‡ªå®šä¹‰ç¼“å­˜)
7. [è¯­ä¹‰åŒ–ç¼“å­˜ - SemanticCache](#è¯­ä¹‰åŒ–ç¼“å­˜-semanticcache)
8. [GPTCacheé›†æˆ](#gptcacheé›†æˆ)
9. [ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨](#ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨)

---

## LangChainå®˜æ–¹ç¼“å­˜æ¶æ„

### ğŸ¯ å®˜æ–¹ç¼“å­˜ä½“ç³»æ¦‚è§ˆ

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[LangChain LLM]
    B --> C{ç¼“å­˜æ£€æŸ¥}
    C -->|å‘½ä¸­| D[è¿”å›ç¼“å­˜]
    C -->|æœªå‘½ä¸­| E[è°ƒç”¨LLM API]
    E --> F[å­˜å‚¨ç¼“å­˜]
    F --> G[è¿”å›ç»“æœ]
    
    subgraph å®˜æ–¹ç¼“å­˜ç±»å‹
        H[InMemoryCache]
        I[SQLiteCache]
        J[SQLAlchemyCache]
        K[RedisCache]
        L[è‡ªå®šä¹‰ç¼“å­˜]
    end
    
    H -.-> C
    I -.-> C
    J -.-> C
    K -.-> C
    L -.-> C
```

### ğŸ”§ å®˜æ–¹ç¼“å­˜åŸºç±»

æ‰€æœ‰LangChainç¼“å­˜éƒ½ç»§æ‰¿è‡ª`BaseCache`æ¥å£ï¼š

```python
from langchain_core.caches import BaseCache
from typing import Optional, Any

class BaseCache(ABC):
    """LangChainå®˜æ–¹ç¼“å­˜åŸºç±»"""
    
    @abstractmethod
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        pass
    
    @abstractmethod
    def update(self, prompt: str, llm_string: str, return_val: str) -> None:
        """æ›´æ–°ç¼“å­˜"""
        pass
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        pass
```

---

## å†…å­˜ç¼“å­˜ - InMemoryCache

### ğŸš€ å®˜æ–¹å®ç°

```python
from langchain_community.cache import InMemoryCache
from langchain_openai import ChatOpenAI

# å®˜æ–¹å†…å­˜ç¼“å­˜
memory_cache = InMemoryCache()

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=memory_cache
)

# ä½¿ç”¨ç¤ºä¾‹
response1 = llm.invoke("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
response2 = llm.invoke("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")  # ä»ç¼“å­˜è·å–
```

### ğŸ“Š é«˜çº§é…ç½®

```python
from langchain_community.cache import InMemoryCache
import threading
from typing import Optional, Dict, Any
import time

class AdvancedInMemoryCache(InMemoryCache):
    """å¢å¼ºç‰ˆå®˜æ–¹å†…å­˜ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        super().__init__()
        self.max_size = max_size
        self.ttl = ttl
        self._cache: Dict[str, Any] = {}
        self._timestamps: Dict[str, float] = {}
        self._lock = threading.RLock()
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """å¸¦TTLçš„æŸ¥æ‰¾"""
        key = f"{prompt}:{llm_string}"
        
        with self._lock:
            if key in self._cache:
                # æ£€æŸ¥TTL
                if time.time() - self._timestamps[key] > self.ttl:
                    del self._cache[key]
                    del self._timestamps[key]
                    return None
                    
                return self._cache[key]
        return None
        
    def update(self, prompt: str, llm_string: str, return_val: str) -> None:
        """å¸¦LRUçš„æ›´æ–°"""
        key = f"{prompt}:{llm_string}"
        
        with self._lock:
            # LRUæ·˜æ±°
            if len(self._cache) >= self.max_size:
                oldest_key = min(self._timestamps, key=self._timestamps.get)
                del self._cache[oldest_key]
                del self._timestamps[oldest_key]
                
            self._cache[key] = return_val
            self._timestamps[key] = time.time()
```

---

## SQLiteç¼“å­˜ - SQLiteCache

### ğŸ—„ï¸ å®˜æ–¹SQLiteå®ç°

```python
from langchain_community.cache import SQLiteCache

# å®˜æ–¹SQLiteç¼“å­˜
sqlite_cache = SQLiteCache(database_path="./langchain_cache.db")

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=sqlite_cache
)

# æ•°æ®åº“ç»“æ„
# è¡¨: full_llm_cache
# - prompt TEXT
# - llm TEXT
# - response TEXT
# - idx INTEGER PRIMARY KEY
```

### ğŸ”§ è‡ªå®šä¹‰SQLiteç¼“å­˜

```python
from langchain_community.cache import SQLiteCache
import sqlite3
from typing import Optional, Dict, Any
import json

class EnhancedSQLiteCache(SQLiteCache):
    """å¢å¼ºç‰ˆSQLiteç¼“å­˜"""
    
    def __init__(self, database_path: str = "./langchain_cache.db"):
        super().__init__(database_path=database_path)
        self._init_custom_tables()
        
    def _init_custom_tables(self):
        """åˆå§‹åŒ–è‡ªå®šä¹‰è¡¨"""
        conn = sqlite3.connect(self.database_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS cache_metadata (
                idx INTEGER PRIMARY KEY,
                prompt_hash TEXT,
                model_name TEXT,
                temperature REAL,
                max_tokens INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (idx) REFERENCES full_llm_cache(idx)
            )
        """)
        conn.commit()
        conn.close()
        
    def store_with_metadata(self, prompt: str, response: str, 
                          model_name: str, temperature: float = 0.7,
                          max_tokens: int = 1000) -> None:
        """å­˜å‚¨å¸¦å…ƒæ•°æ®çš„ç¼“å­˜"""
        llm_string = f"{model_name}_{temperature}_{max_tokens}"
        self.update(prompt, llm_string, response)
        
        # å­˜å‚¨å…ƒæ•°æ®
        conn = sqlite3.connect(self.database_path)
        cursor = conn.cursor()
        cursor.execute("SELECT last_insert_rowid()")
        idx = cursor.fetchone()[0]
        
        conn.execute("""
            INSERT INTO cache_metadata (idx, prompt_hash, model_name, temperature, max_tokens)
            VALUES (?, ?, ?, ?, ?)
        """, (idx, hash(prompt), model_name, temperature, max_tokens))
        conn.commit()
        conn.close()
```

---

## SQLæ•°æ®åº“ç¼“å­˜ - SQLAlchemyCache

### ğŸ—ï¸ å®˜æ–¹SQLAlchemyå®ç°

```python
from langchain_community.cache import SQLAlchemyCache
from sqlalchemy import create_engine

# MySQLè¿æ¥
mysql_engine = create_engine(
    "mysql+pymysql://user:password@localhost:3306/langchain_cache"
)
mysql_cache = SQLAlchemyCache(engine=mysql_engine)

# PostgreSQLè¿æ¥
pg_engine = create_engine(
    "postgresql://user:password@localhost:5432/langchain_cache"
)
pg_cache = SQLAlchemyCache(engine=pg_engine)

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=mysql_cache
)
```

### ğŸ”§ é«˜çº§SQLAlchemyé…ç½®

```python
from langchain_community.cache import SQLAlchemyCache
from sqlalchemy import create_engine, Column, String, DateTime, Integer, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime

Base = declarative_base()

class CustomCacheModel(Base):
    """è‡ªå®šä¹‰ç¼“å­˜è¡¨æ¨¡å‹"""
    __tablename__ = 'custom_llm_cache'
    
    prompt = Column(String, primary_key=True)
    llm = Column(String, primary_key=True)
    response = Column(String)
    metadata = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    access_count = Column(Integer, default=1)

class AdvancedSQLAlchemyCache(SQLAlchemyCache):
    """é«˜çº§SQLAlchemyç¼“å­˜"""
    
    def __init__(self, connection_string: str):
        engine = create_engine(connection_string)
        Base.metadata.create_all(engine)
        super().__init__(engine)
        
        self.Session = sessionmaker(bind=engine)
        
    def store_with_tags(self, prompt: str, response: str, 
                       model: str, tags: List[str]) -> None:
        """å­˜å‚¨å¸¦æ ‡ç­¾çš„ç¼“å­˜"""
        llm_string = f"{model}_tags_{','.join(tags)}"
        self.update(prompt, llm_string, response)
        
    def search_by_tags(self, tags: List[str]) -> List[Dict[str, Any]]:
        """æŒ‰æ ‡ç­¾æœç´¢ç¼“å­˜"""
        session = self.Session()
        results = session.query(CustomCacheModel).filter(
            CustomCacheModel.llm.contains("tags")
        ).all()
        session.close()
        return [{"prompt": r.prompt, "response": r.response} for r in results]
```

---

## Redisç¼“å­˜ - RedisCache

### ğŸ”— å®˜æ–¹Rediså®ç°

```python
from langchain_community.cache import RedisCache
import redis

# Redisè¿æ¥é…ç½®
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True
)

# å®˜æ–¹Redisç¼“å­˜
redis_cache = RedisCache(redis_client=redis_client)

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=redis_cache
)

# Redisé”®æ ¼å¼: "langchain:cache:{prompt_hash}:{llm_string}"
```

### ğŸ”§ Redisé›†ç¾¤é…ç½®

```python
from langchain_community.cache import RedisCache
from redis.cluster import RedisCluster

class RedisClusterCache(RedisCache):
    """Redisé›†ç¾¤ç¼“å­˜"""
    
    def __init__(self, cluster_nodes: List[Dict[str, Any]]):
        cluster_client = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True
        )
        super().__init__(redis_client=cluster_client)
        
    def get_cluster_stats(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤ç»Ÿè®¡"""
        return {
            "cluster_info": self.redis_client.cluster_info(),
            "node_count": len(self.redis_client.cluster_nodes()),
            "memory_usage": self.redis_client.info("memory")
        }

# ä½¿ç”¨ç¤ºä¾‹
cluster_nodes = [
    {"host": "127.0.0.1", "port": "7000"},
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"}
]
cluster_cache = RedisClusterCache(cluster_nodes)
```

---

## MongoDBè‡ªå®šä¹‰ç¼“å­˜

### ğŸ—ƒï¸ å®˜æ–¹MongoDBé›†æˆ

```python
from langchain_community.cache import MongoDBCache
from pymongo import MongoClient

# MongoDBè¿æ¥
mongo_client = MongoClient("mongodb://localhost:27017/")
mongodb_cache = MongoDBCache(
    client=mongo_client,
    database_name="langchain_cache",
    collection_name="llm_cache"
)

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=mongodb_cache
)

# æ–‡æ¡£ç»“æ„
# {
#   "_id": ObjectId("..."),
#   "prompt": "ç”¨æˆ·æç¤º",
#   "llm": "llm_string",
#   "response": "AIå“åº”",
#   "idx": ObjectId("...")
# }
```

### ğŸ”§ é«˜çº§MongoDBç¼“å­˜

```python
from langchain_community.cache import MongoDBCache
from pymongo import MongoClient, IndexModel, ASCENDING, TEXT
from typing import Optional, Dict, Any
import datetime

class AdvancedMongoDBCache(MongoDBCache):
    """é«˜çº§MongoDBç¼“å­˜"""
    
    def __init__(self, connection_string: str, database_name: str = "langchain_cache"):
        client = MongoClient(connection_string)
        super().__init__(client, database_name, "advanced_cache")
        
        # åˆ›å»ºç´¢å¼•
        indexes = [
            IndexModel([("prompt", TEXT)]),
            IndexModel([("llm", ASCENDING)]),
            IndexModel([("created_at", ASCENDING)]),
            IndexModel([("tags", ASCENDING)])
        ]
        self.collection.create_indexes(indexes)
        
    def store_with_metadata(self, prompt: str, response: str, 
                          model: str, metadata: Dict[str, Any]) -> None:
        """å­˜å‚¨å¸¦å…ƒæ•°æ®"""
        llm_string = f"{model}_{json.dumps(metadata)}"
        self.update(prompt, llm_string, response)
        
        # é¢å¤–å­˜å‚¨å…ƒæ•°æ®
        self.collection.update_one(
            {"prompt": prompt, "llm": llm_string},
            {
                "$set": {
                    "metadata": metadata,
                    "tags": metadata.get("tags", []),
                    "created_at": datetime.datetime.utcnow()
                }
            },
            upsert=True
        )
        
    def search_by_metadata(self, metadata_filter: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æŒ‰å…ƒæ•°æ®æœç´¢"""
        results = self.collection.find({"metadata": {"$elemMatch": metadata_filter}})
        return [{"prompt": r["prompt"], "response": r["response"]} for r in results]
```

---

## è¯­ä¹‰åŒ–ç¼“å­˜ - SemanticCache

### ğŸ§  å®˜æ–¹è¯­ä¹‰ç¼“å­˜å®ç°

```python
from langchain_community.cache import RedisSemanticCache
from langchain.embeddings import OpenAIEmbeddings

# è¯­ä¹‰ç¼“å­˜é…ç½®
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
semantic_cache = RedisSemanticCache(
    redis_url="redis://localhost:6379",
    embedding=embeddings,
    score_threshold=0.85
)

# é›†æˆåˆ°LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    cache=semantic_cache
)

# å·¥ä½œåŸç†:
# 1. å°†promptè½¬æ¢ä¸ºå‘é‡
# 2. è®¡ç®—ä¸ç¼“å­˜ä¸­å‘é‡çš„ç›¸ä¼¼åº¦
# 3. ç›¸ä¼¼åº¦>é˜ˆå€¼æ—¶è¿”å›ç¼“å­˜
```

### ğŸ”§ è‡ªå®šä¹‰è¯­ä¹‰ç¼“å­˜

```python
from langchain_community.cache import RedisSemanticCache
from langchain.embeddings import HuggingFaceEmbeddings
from typing import Optional, List, Dict, Any

class CustomSemanticCache(RedisSemanticCache):
    """è‡ªå®šä¹‰è¯­ä¹‰ç¼“å­˜"""
    
    def __init__(self, 
                 redis_url: str,
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
                 score_threshold: float = 0.85):
        
        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
        super().__init__(redis_url, embeddings, score_threshold)
        
    def add_examples(self, examples: List[Dict[str, str]]) -> None:
        """æ·»åŠ ç¤ºä¾‹åˆ°ç¼“å­˜"""
        for example in examples:
            self.update(
                example["prompt"],
                "custom_llm",
                example["response"]
            )
            
    def batch_lookup(self, prompts: List[str]) -> List[Optional[str]]:
        """æ‰¹é‡æŸ¥æ‰¾"""
        results = []
        for prompt in prompts:
            result = self.lookup(prompt, "custom_llm")
            results.append(result)
        return results
```

---

## GPTCacheé›†æˆ

### ğŸ”— å®˜æ–¹GPTCacheé€‚é…

```python
from gptcache import Cache
from gptcache.adapter.langchain_models import LangChainLLM
from langchain.llms import OpenAI

# åˆå§‹åŒ–GPTCache
cache = Cache()

# é…ç½®GPTCache
from gptcache.manager import CacheBase, VectorBase, get_data_manager
from gptcache.processor.pre import get_prompt
from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation

data_manager = get_data_manager(
    CacheBase("sqlite", sql_url="sqlite:///gptcache.db"),
    VectorBase("faiss", dimension=1536)
)

cache.init(
    pre_func=get_prompt,
    data_manager=data_manager,
    evaluation=SearchDistanceEvaluation()
)

# LangChainé›†æˆ
llm = LangChainLLM(llm=OpenAI(model="gpt-3.5-turbo"), cache_obj=cache)

# ä½¿ç”¨
response = llm("ä»€ä¹ˆæ˜¯LangChainï¼Ÿ")
```

### ğŸ”§ é«˜çº§GPTCacheé…ç½®

```python
from gptcache import Cache
from gptcache.adapter.langchain_models import LangChainLLM
from langchain.chat_models import ChatOpenAI

class AdvancedGPTCache:
    """é«˜çº§GPTCacheé…ç½®"""
    
    def __init__(self):
        self.cache = Cache()
        self._setup_advanced_config()
        
    def _setup_advanced_config(self):
        """é«˜çº§é…ç½®"""
        from gptcache.manager import CacheBase, VectorBase, get_data_manager
        from gptcache.processor.pre import get_prompt
        from gptcache.similarity_evaluation import OnnxModelEvaluation
        
        # ä½¿ç”¨ONNXæ¨¡å‹è¯„ä¼°
        data_manager = get_data_manager(
            CacheBase("mysql", sql_url="mysql://user:pass@localhost/gptcache"),
            VectorBase("milvus", dimension=1536, collection_name="gptcache")
        )
        
        self.cache.init(
            pre_func=get_prompt,
            data_manager=data_manager,
            evaluation=OnnxModelEvaluation(),
            config={
                "threshold": 0.85,
                "k": 5
            }
        )
        
    def create_chat_model(self, model_name: str = "gpt-3.5-turbo"):
        """åˆ›å»ºå¸¦ç¼“å­˜çš„èŠå¤©æ¨¡å‹"""
        return LangChainLLM(
            llm=ChatOpenAI(model=model_name),
            cache_obj=self.cache
        )
```

---

## ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨

### ğŸ¯ å¤šçº§ç¼“å­˜ç®¡ç†å™¨

```python
from langchain_community.cache import (
    InMemoryCache, 
    SQLiteCache, 
    SQLAlchemyCache, 
    RedisCache, 
    MongoDBCache,
    RedisSemanticCache
)
from typing import Dict, Any, Optional, List
import asyncio
from datetime import datetime

class UnifiedLangChainCache:
    """ç»Ÿä¸€LangChainç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.caches = {}
        self.cache_hierarchy = [
            "memory",
            "sqlite", 
            "mysql",
            "redis",
            "mongodb",
            "semantic"
        ]
        
        self._init_caches()
        
    def _init_caches(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®˜æ–¹ç¼“å­˜"""
        cache_configs = {
            "memory": InMemoryCache,
            "sqlite": SQLiteCache,
            "mysql": SQLAlchemyCache,
            "redis": RedisCache,
            "mongodb": MongoDBCache,
            "semantic": RedisSemanticCache
        }
        
        for cache_type in self.cache_hierarchy:
            if cache_type in self.config:
                cache_class = cache_configs[cache_type]
                self.caches[cache_type] = cache_class(**self.config[cache_type])
                
    async def get_with_fallback(self, prompt: str, llm_string: str) -> Optional[str]:
        """å¸¦é™çº§ç­–ç•¥çš„è·å–"""
        for cache_type in self.cache_hierarchy:
            if cache_type not in self.caches:
                continue
                
            try:
                result = self.caches[cache_type].lookup(prompt, llm_string)
                if result:
                    # å›å†™ä¸Šå±‚ç¼“å­˜
                    await self._backfill_cache(prompt, llm_string, result, cache_type)
                    return result
            except Exception as e:
                print(f"Cache {cache_type} error: {e}")
                continue
                
        return None
        
    async def set_all_levels(self, prompt: str, llm_string: str, value: str):
        """è®¾ç½®æ‰€æœ‰å±‚çº§ç¼“å­˜"""
        tasks = []
        for cache_type, cache in self.caches.items():
            task = asyncio.create_task(
                self._set_cache_async(cache_type, prompt, llm_string, value)
            )
            tasks.append(task)
            
        await asyncio.gather(*tasks, return_exceptions=True)
        
    def get_health_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜å¥åº·çŠ¶æ€"""
        status = {}
        for cache_type, cache in self.caches.items():
            try:
                if hasattr(cache, 'redis_client'):
                    status[cache_type] = {
                        "type": cache_type,
                        "connected": cache.redis_client.ping()
                    }
                else:
                    status[cache_type] = {
                        "type": cache_type,
                        "status": "active"
                    }
            except Exception as e:
                status[cache_type] = {
                    "type": cache_type,
                    "status": "error",
                    "error": str(e)
                }
        return status
```

### ğŸ“Š é…ç½®ç¤ºä¾‹

```python
# ç”Ÿäº§é…ç½®
production_config = {
    "memory": {
        "max_size": 1000,
        "ttl": 3600
    },
    "sqlite": {
        "database_path": "./langchain_cache.db"
    },
    "mysql": {
        "connection_string": "mysql+pymysql://user:pass@localhost:3306/langchain_cache"
    },
    "redis": {
        "redis_url": "redis://localhost:6379/0"
    },
    "mongodb": {
        "connection_string": "mongodb://localhost:27017/",
        "database_name": "langchain_cache"
    },
    "semantic": {
        "redis_url": "redis://localhost:6379/1",
        "embedding_model": "text-embedding-ada-002",
        "score_threshold": 0.85
    }
}

# å¼€å‘é…ç½®
development_config = {
    "memory": {
        "max_size": 100
    },
    "sqlite": {
        "database_path": "./dev_cache.db"
    }
}
```

---

## ç›‘æ§ä¸è¿ç»´

### ğŸ“ˆ æ€§èƒ½ç›‘æ§

```python
from prometheus_client import Counter, Histogram, Gauge
import time

class CacheMonitor:
    """ç¼“å­˜ç›‘æ§å™¨"""
    
    def __init__(self):
        self.cache_hits = Counter('langchain_cache_hits_total', 
                                'Total cache hits', ['cache_type'])
        self.cache_misses = Counter('langchain_cache_misses_total', 
                                  'Total cache misses', ['cache_type'])
        self.cache_latency = Histogram('langchain_cache_latency_seconds',
                                     'Cache operation latency', ['cache_type'])
        self.cache_size = Gauge('langchain_cache_size',
                              'Current cache size', ['cache_type'])
        
    def record_hit(self, cache_type: str):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.cache_hits.labels(cache_type=cache_type).inc()
        
    def record_miss(self, cache_type: str):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.cache_misses.labels(cache_type=cache_type).inc()
        
    def record_latency(self, cache_type: str, duration: float):
        """è®°å½•å»¶è¿Ÿ"""
        self.cache_latency.labels(cache_type=cache_type).observe(duration)
```

### ğŸ” è°ƒè¯•å·¥å…·

```python
class CacheDebugger:
    """ç¼“å­˜è°ƒè¯•å·¥å…·"""
    
    def __init__(self, cache_manager: UnifiedLangChainCache):
        self.manager = cache_manager
        
    def debug_cache_flow(self, prompt: str, llm_string: str) -> Dict[str, Any]:
        """è°ƒè¯•ç¼“å­˜æµç¨‹"""
        result = {
            "prompt": prompt,
            "llm_string": llm_string,
            "cache_flow": []
        }
        
        for cache_type in self.manager.cache_hierarchy:
            if cache_type in self.manager.caches:
                cache = self.manager.caches[cache_type]
                try:
                    start_time = time.time()
                    cached = cache.lookup(prompt, llm_string)
                    latency = time.time() - start_time
                    
                    result["cache_flow"].append({
                        "type": cache_type,
                        "hit": cached is not None,
                        "latency": latency,
                        "response": cached[:100] if cached else None
                    })
                except Exception as e:
                    result["cache_flow"].append({
                        "type": cache_type,
                        "error": str(e)
                    })
                    
        return result
```

---

## æœ€ä½³å®è·µæ€»ç»“

### ğŸ¯ é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èç¼“å­˜ | é…ç½®ç¤ºä¾‹ |
|------|----------|----------|
| **æœ¬åœ°å¼€å‘** | InMemoryCache + SQLiteCache | è½»é‡çº§ï¼Œå¿«é€Ÿè¿­ä»£ |
| **å°å‹ç”Ÿäº§** | RedisCache | é«˜æ€§èƒ½ï¼Œæ”¯æŒé›†ç¾¤ |
| **ä¼ä¸šçº§** | SQLAlchemyCache + MongoDBCache | é«˜å¯ç”¨ï¼Œæ•°æ®æŒä¹…åŒ– |
| **LLMä¼˜åŒ–** | RedisSemanticCache + GPTCache | è¯­ä¹‰åŒ¹é…ï¼Œæˆæœ¬ä¼˜åŒ– |

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

```mermaid
graph LR
    A[æ€§èƒ½æµ‹è¯•] --> B[å»¶è¿Ÿå¯¹æ¯”]
    B --> C[å†…å­˜: <1ms]
    B --> D[SQLite: 1-5ms]
    B --> E[Redis: 1-5ms]
    B --> F[MySQL: 5-15ms]
    B --> G[è¯­ä¹‰: 10-50ms]
    
    A --> H[ååé‡å¯¹æ¯”]
    H --> I[å†…å­˜: 100K+ QPS]
    H --> J[Redis: 50K+ QPS]
    H --> K[MySQL: 10K+ QPS]
```

### ğŸš€ éƒ¨ç½²å»ºè®®

1. **å¼€å‘ç¯å¢ƒ**ï¼š
   ```python
   llm = ChatOpenAI(
       model="gpt-3.5-turbo",
       cache=InMemoryCache(max_size=100)
   )
   ```

2. **ç”Ÿäº§ç¯å¢ƒ**ï¼š
   ```python
   cache_config = {
       "memory": InMemoryCache(max_size=1000),
       "redis": RedisCache(redis_client=redis.Redis("localhost")),
       "semantic": RedisSemanticCache(
           redis_url="redis://localhost:6379/1",
           embedding=OpenAIEmbeddings()
       )
   }
   ```

3. **ç›‘æ§é…ç½®**ï¼š
   ```python
   monitor = CacheMonitor()
   debugger = CacheDebugger(cache_manager)
   ```

è¿™å¥—åŸºäºLangChainå®˜æ–¹APIçš„ç¼“å­˜æŠ€æœ¯æ–¹æ¡ˆï¼Œä¸ºæ‚¨çš„LLMåº”ç”¨æä¾›äº†ä»å¼€å‘åˆ°ç”Ÿäº§çš„å®Œæ•´ç¼“å­˜è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒæŒ‰éœ€æ‰©å±•å’Œæ™ºèƒ½ä¼˜åŒ–ã€‚
        