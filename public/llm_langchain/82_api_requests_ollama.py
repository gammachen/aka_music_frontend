#!/usr/bin/env python3
"""
ä½¿ç”¨Ollamaçš„LLMRequestsChainå®Œæ•´æ¼”ç¤º
ä¸“ä¸ºOllama gpt-3.5-turbo:latestä¼˜åŒ–
"""

import json
import time
import threading
import requests
from flask import Flask, jsonify, request
from langchain.chains import LLMChain
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate

# ==================== Mock RESTfulæœåŠ¡ ====================

class MockEnterpriseAPI:
    """ä¼ä¸šçº§Mock APIæœåŠ¡"""
    
    def __init__(self, port=8000):
        self.app = Flask(__name__)
        self.port = port
        self.setup_routes()
        self.mock_data = self._generate_mock_data()
    
    def _generate_mock_data(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        return {
            "employees": {
                "å¼ ä¸‰": {
                    "name": "å¼ ä¸‰",
                    "department": "æŠ€æœ¯éƒ¨",
                    "annual_leave": 15,
                    "used_leave": 7,
                    "remaining_leave": 8,
                    "sick_leave": 5,
                    "maternity_leave_eligible": True
                },
                "æå››": {
                    "name": "æå››",
                    "department": "å¸‚åœºéƒ¨", 
                    "annual_leave": 12,
                    "used_leave": 3,
                    "remaining_leave": 9,
                    "maternity_leave_eligible": False
                }
            },
            "suppliers": {
                "electronics": [
                    {
                        "name": "åå¼ºåŒ—ç”µå­",
                        "contact": "13800138001",
                        "rating": 4.8,
                        "products": ["æ‰‹æœºé…ä»¶", "ç”µè„‘é…ä»¶"],
                        "status": "active"
                    }
                ]
            },
            "policies": {
                "maternity": {
                    "title": "äº§å‡æ”¿ç­–",
                    "content": """
                    1. åŸºç¡€äº§å‡ï¼šå¥³èŒå·¥ç”Ÿè‚²äº«å—98å¤©äº§å‡
                    2. éš¾äº§å¢åŠ ï¼šéš¾äº§å¢åŠ 15å¤©
                    3. å¤šèƒèƒï¼šæ¯å¤š1ä¸ªå©´å„¿å¢åŠ 15å¤©
                    4. é™ªäº§å‡ï¼šç”·èŒå·¥äº«å—15å¤©é™ªäº§å‡
                    """
                }
            }
        }
    
    def setup_routes(self):
        """è®¾ç½®APIè·¯ç”±"""
        
        @self.app.route('/api/employees/<employee_name>/leave-balance', methods=['GET'])
        def get_employee_leave(employee_name):
            employee = self.mock_data["employees"].get(employee_name)
            if not employee:
                return jsonify({"error": "å‘˜å·¥ä¸å­˜åœ¨"}), 404
            return jsonify(employee)
        
        @self.app.route('/api/suppliers', methods=['GET'])
        def get_suppliers():
            category = request.args.get('category', 'all')
            return jsonify({
                "suppliers": self.mock_data["suppliers"],
                "category": category
            })
        
        @self.app.route('/api/policies/<policy_type>', methods=['GET'])
        def get_policy(policy_type):
            policy = self.mock_data["policies"].get(policy_type)
            if not policy:
                return jsonify({"error": "æ”¿ç­–ä¸å­˜åœ¨"}), 404
            return jsonify(policy)
    
    def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

# ==================== LLMRequestsChainå®ç° ====================

class OllamaLLMRequestsChain:
    """å…¼å®¹Ollamaçš„LLMRequestsChain"""
    
    def __init__(self, llm, prompt_template):
        self.llm = llm
        self.prompt_template = prompt_template
        self.llm_chain = LLMChain(llm=llm, prompt=prompt_template)
    
    def invoke(self, inputs):
        """æ‰§è¡Œé“¾å¼è°ƒç”¨"""
        query = inputs["query"]
        url = inputs["url"]
        
        # è·å–APIæ•°æ®
        response = requests.get(url)
        api_data = response.json()
        
        # æ„å»ºæç¤º
        prompt = self.prompt_template.format(
            query=query,
            api_response=json.dumps(api_data, ensure_ascii=False, indent=2)
        )
        
        # è°ƒç”¨LLM
        result = self.llm.invoke(prompt)
        return {"output": result}

class EnterpriseSmartAssistant:
    """ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.llm = Ollama(model="gpt-3.5-turbo:latest", temperature=0.3)
        self.setup_chains()
    
    def setup_chains(self):
        """è®¾ç½®å„ç§æŸ¥è¯¢é“¾"""
        
        # å‘˜å·¥æŸ¥è¯¢é“¾
        employee_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”å›ç­”å‘˜å·¥å‡æœŸé—®é¢˜ï¼š

é—®é¢˜ï¼š{query}
APIå“åº”ï¼š{api_response}

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼ŒåŒ…å«å…³é”®æ•°å­—ã€‚"""
        )
        self.employee_chain = OllamaLLMRequestsChain(self.llm, employee_prompt)
        
        # æ”¿ç­–æŸ¥è¯¢é“¾  
        policy_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”è§£é‡Šæ”¿ç­–ï¼š

é—®é¢˜ï¼š{query}
æ”¿ç­–å†…å®¹ï¼š{api_response}

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡è§£é‡Šæ”¿ç­–è¦ç‚¹ã€‚"""
        )
        self.policy_chain = OllamaLLMRequestsChain(self.llm, policy_prompt)
        
        # ä¾›åº”å•†æŸ¥è¯¢é“¾
        supplier_prompt = PromptTemplate(
            input_variables=["query", "api_response"],
            template="""åŸºäºAPIå“åº”æä¾›ä¾›åº”å•†ä¿¡æ¯ï¼š

é—®é¢˜ï¼š{query}
ä¾›åº”å•†ä¿¡æ¯ï¼š{api_response}

è¯·ç”¨ä¸­æ–‡åˆ—å‡ºä¾›åº”å•†è¯¦ç»†ä¿¡æ¯ã€‚"""
        )
        self.supplier_chain = OllamaLLMRequestsChain(self.llm, supplier_prompt)
    
    def query_employee_leave(self, employee_name):
        """æŸ¥è¯¢å‘˜å·¥å‡æœŸ"""
        return self.employee_chain.invoke({
            "query": f"æŸ¥è¯¢{employee_name}çš„å‡æœŸä½™é¢",
            "url": f"{self.base_url}/api/employees/{employee_name}/leave-balance"
        })
    
    def query_maternity_policy(self):
        """æŸ¥è¯¢äº§å‡æ”¿ç­–"""
        return self.policy_chain.invoke({
            "query": "äº§å‡æ”¿ç­–æ˜¯ä»€ä¹ˆ",
            "url": f"{self.base_url}/api/policies/maternity"
        })
    
    def query_suppliers(self, category="electronics"):
        """æŸ¥è¯¢ä¾›åº”å•†"""
        return self.supplier_chain.invoke({
            "query": f"æŸ¥è¯¢{category}ç±»åˆ«çš„ä¾›åº”å•†",
            "url": f"{self.base_url}/api/suppliers?category={category}"
        })

# ==================== æ¼”ç¤º ====================

def main():
    """ä¸»ç¨‹åº"""
    
    print("ğŸš€ LLMRequestsChainä¼ä¸šæ™ºèƒ½åŠ©æ‰‹æ¼”ç¤º")
    print("=" * 50)
    
    # å¯åŠ¨MockæœåŠ¡å™¨
    mock_api = MockEnterpriseAPI()
    server_thread = threading.Thread(target=mock_api.start_server, daemon=True)
    server_thread.start()
    time.sleep(3)
    print("âœ… MockæœåŠ¡å™¨å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–åŠ©æ‰‹
    assistant = EnterpriseSmartAssistant()
    
    # æ¼”ç¤ºæŸ¥è¯¢
    print("\n=== æ™ºèƒ½æŸ¥è¯¢æ¼”ç¤º ===")
    
    # 1. å‘˜å·¥å‡æœŸæŸ¥è¯¢
    print("\n1ï¸âƒ£ å‘˜å·¥å‡æœŸæŸ¥è¯¢ï¼š")
    result = assistant.query_employee_leave("å¼ ä¸‰")
    print(f"ç»“æœï¼š{result['output']}")
    
    # 2. äº§å‡æ”¿ç­–æŸ¥è¯¢
    print("\n2ï¸âƒ£ äº§å‡æ”¿ç­–æŸ¥è¯¢ï¼š")
    result = assistant.query_maternity_policy()
    print(f"ç»“æœï¼š{result['output']}")
    
    # 3. ä¾›åº”å•†æŸ¥è¯¢
    print("\n3ï¸âƒ£ ä¾›åº”å•†æŸ¥è¯¢ï¼š")
    result = assistant.query_suppliers("electronics")
    print(f"ç»“æœï¼š{result['output']}")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()