


          
# LangChain Promptç”Ÿå‘½å‘¨æœŸç®¡ç†å…¨æ ˆæŠ€æœ¯æ–‡æ¡£

è¯¦ç»†é˜è¿°Promptåœ¨LangChainç”Ÿæ€ä¸­çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚è¿™ä¸æ˜¯ç®€å•çš„Promptç¼–å†™ï¼Œè€Œæ˜¯ä¼ä¸šçº§çš„Promptå·¥ç¨‹ä½“ç³»ã€‚

## ğŸ“‹ ç›®å½•
- [LangChain Promptç”Ÿå‘½å‘¨æœŸç®¡ç†å…¨æ ˆæŠ€æœ¯æ–‡æ¡£](#langchain-promptç”Ÿå‘½å‘¨æœŸç®¡ç†å…¨æ ˆæŠ€æœ¯æ–‡æ¡£)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [Promptç”Ÿå‘½å‘¨æœŸæ€»è§ˆ](#promptç”Ÿå‘½å‘¨æœŸæ€»è§ˆ)
    - [ğŸ¯ å®Œæ•´ç”Ÿå‘½å‘¨æœŸæµç¨‹](#-å®Œæ•´ç”Ÿå‘½å‘¨æœŸæµç¨‹)
    - [ğŸ” ç”Ÿå‘½å‘¨æœŸé˜¶æ®µçŸ©é˜µ](#-ç”Ÿå‘½å‘¨æœŸé˜¶æ®µçŸ©é˜µ)
  - [è®¾è®¡é˜¶æ®µ - Prompt Engineering](#è®¾è®¡é˜¶æ®µ---prompt-engineering)
    - [ğŸ—ï¸ Promptè®¾è®¡æ¡†æ¶](#ï¸-promptè®¾è®¡æ¡†æ¶)
    - [ğŸ¯ Promptæ¨¡æ¿åº“](#-promptæ¨¡æ¿åº“)
  - [æµ‹è¯•é˜¶æ®µ - Prompt Testing](#æµ‹è¯•é˜¶æ®µ---prompt-testing)
    - [ğŸ§ª æµ‹è¯•æ¡†æ¶](#-æµ‹è¯•æ¡†æ¶)
    - [ğŸ“Š æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ](#-æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ)
  - [ç»†åŒ–é˜¶æ®µ - Prompt Refinement](#ç»†åŒ–é˜¶æ®µ---prompt-refinement)
    - [ğŸ”§ Promptä¼˜åŒ–å¼•æ“](#-promptä¼˜åŒ–å¼•æ“)
    - [ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
  - [è¿­ä»£é˜¶æ®µ - Prompt Iteration](#è¿­ä»£é˜¶æ®µ---prompt-iteration)
    - [ğŸ”„ ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ](#-ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ)
    - [ğŸ“Š è¿­ä»£åˆ†æä»ªè¡¨æ¿](#-è¿­ä»£åˆ†æä»ªè¡¨æ¿)
  - [éƒ¨ç½²é˜¶æ®µ - Prompt Deployment](#éƒ¨ç½²é˜¶æ®µ---prompt-deployment)
    - [ğŸš€ CI/CDé›†æˆ](#-cicdé›†æˆ)
    - [ğŸ”„ è“ç»¿éƒ¨ç½²](#-è“ç»¿éƒ¨ç½²)
  - [ç»´æŠ¤é˜¶æ®µ - Prompt Maintenance](#ç»´æŠ¤é˜¶æ®µ---prompt-maintenance)
    - [ğŸ”§ ç»´æŠ¤å·¥ä½œæµ](#-ç»´æŠ¤å·¥ä½œæµ)
  - [ç›‘æ§é˜¶æ®µ - Prompt Monitoring](#ç›‘æ§é˜¶æ®µ---prompt-monitoring)
    - [ğŸ“Š å®æ—¶ç›‘æ§ä»ªè¡¨æ¿](#-å®æ—¶ç›‘æ§ä»ªè¡¨æ¿)
  - [é€€å½¹é˜¶æ®µ - Prompt Retirement](#é€€å½¹é˜¶æ®µ---prompt-retirement)
    - [ğŸ—‘ï¸ ä¼˜é›…é€€å½¹æµç¨‹](#ï¸-ä¼˜é›…é€€å½¹æµç¨‹)
  - [ç»Ÿä¸€ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨](#ç»Ÿä¸€ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨)
    - [ğŸ¯ ç»Ÿä¸€ç®¡ç†å¹³å°](#-ç»Ÿä¸€ç®¡ç†å¹³å°)
    - [ğŸ“Š ç”Ÿå‘½å‘¨æœŸä»ªè¡¨æ¿](#-ç”Ÿå‘½å‘¨æœŸä»ªè¡¨æ¿)
  - [æ€»ç»“ä¸æœ€ä½³å®è·µ](#æ€»ç»“ä¸æœ€ä½³å®è·µ)
    - [ğŸ¯ å®æ–½è·¯çº¿å›¾](#-å®æ–½è·¯çº¿å›¾)
    - [ğŸ“‹ æ£€æŸ¥æ¸…å•](#-æ£€æŸ¥æ¸…å•)
      - [âœ… è®¾è®¡é˜¶æ®µ](#-è®¾è®¡é˜¶æ®µ)
      - [âœ… æµ‹è¯•é˜¶æ®µ](#-æµ‹è¯•é˜¶æ®µ)
      - [âœ… éƒ¨ç½²é˜¶æ®µ](#-éƒ¨ç½²é˜¶æ®µ)
      - [âœ… ç›‘æ§é˜¶æ®µ](#-ç›‘æ§é˜¶æ®µ)

---

## Promptç”Ÿå‘½å‘¨æœŸæ€»è§ˆ

### ğŸ¯ å®Œæ•´ç”Ÿå‘½å‘¨æœŸæµç¨‹

```mermaid
graph TD
    A[éœ€æ±‚åˆ†æ] --> B[Promptè®¾è®¡]
    B --> C[Promptæµ‹è¯•]
    C --> D{æµ‹è¯•é€šè¿‡?}
    D -->|å¦| E[Promptç»†åŒ–]
    E --> C
    D -->|æ˜¯| F[Promptè¿­ä»£]
    F --> G[ç‰ˆæœ¬æ§åˆ¶]
    G --> H[Promptéƒ¨ç½²]
    H --> I[å®æ—¶ç›‘æ§]
    I --> J{æ€§èƒ½ä¸‹é™?}
    J -->|æ˜¯| K[Promptç»´æŠ¤]
    K --> F
    J -->|å¦| L[æŒç»­ç›‘æ§]
    L --> M{ä¸šåŠ¡å˜åŒ–?}
    M -->|æ˜¯| F
    M -->|å¦| N[Prompté€€å½¹]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style H fill:#fff3e0
    style N fill:#ffebee
```

### ğŸ” ç”Ÿå‘½å‘¨æœŸé˜¶æ®µçŸ©é˜µ

| é˜¶æ®µ | ç›®æ ‡ | å·¥å…·é“¾ | è¾“å‡ºç‰© | å…³é”®æŒ‡æ ‡ |
|------|------|--------|--------|----------|
| **è®¾è®¡** | åˆ›å»ºé«˜è´¨é‡Prompt | LangChain Templates | Promptæ¨¡æ¿ | æ¸…æ™°åº¦ã€ä¸€è‡´æ€§ |
| **æµ‹è¯•** | éªŒè¯Promptæ•ˆæœ | LangSmith, PromptBench | æµ‹è¯•æŠ¥å‘Š | å‡†ç¡®ç‡ã€å»¶è¿Ÿ |
| **ç»†åŒ–** | ä¼˜åŒ–Promptè¡¨ç° | A/Bæµ‹è¯•å¹³å° | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡å¹…åº¦ |
| **è¿­ä»£** | æŒç»­æ”¹è¿› | ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ | è¿­ä»£è®°å½• | è¿­ä»£é¢‘ç‡ |
| **éƒ¨ç½²** | ç”Ÿäº§ç¯å¢ƒå‘å¸ƒ | CI/CDç®¡é“ | éƒ¨ç½²åŒ… | éƒ¨ç½²æˆåŠŸç‡ |
| **ç»´æŠ¤** | æ€§èƒ½ç›‘æ§ | ç›‘æ§ä»ªè¡¨æ¿ | ç›‘æ§æŠ¥å‘Š | æ€§èƒ½ç¨³å®šæ€§ |
| **é€€å½¹** | ä¼˜é›…ä¸‹çº¿ | é€€å½¹æµç¨‹ | é€€å½¹æŠ¥å‘Š | å½±å“èŒƒå›´ |

---

## è®¾è®¡é˜¶æ®µ - Prompt Engineering

### ğŸ—ï¸ Promptè®¾è®¡æ¡†æ¶

```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from typing import Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class PromptDesignSpec:
    """Promptè®¾è®¡è§„èŒƒ"""
    name: str
    version: str
    description: str
    use_case: str
    constraints: Dict[str, Any]
    expected_output: Dict[str, Any]
    metadata: Dict[str, Any]

class PromptDesigner:
    """ä¼ä¸šçº§Promptè®¾è®¡å™¨"""
    
    def __init__(self):
        self.templates = {}
        self.design_specs = {}
        
    def create_template(self, spec: PromptDesignSpec) -> ChatPromptTemplate:
        """åŸºäºè§„èŒƒåˆ›å»ºPromptæ¨¡æ¿"""
        
        # ç³»ç»ŸPromptæ¨¡æ¿
        system_template = """You are a {role} assistant specialized in {domain}.
        
        Context: {context}
        Task: {task_description}
        
        Constraints:
        {constraints}
        
        Output Format:
        {output_format}
        
        Examples:
        {examples}
        """
        
        # åˆ›å»ºChatPromptTemplate
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_template),
            HumanMessage(content="{user_input}")
        ])
        
        # å­˜å‚¨è®¾è®¡è§„èŒƒ
        self.design_specs[spec.name] = spec
        self.templates[spec.name] = prompt_template
        
        return prompt_template
    
    def validate_template(self, template_name: str, test_inputs: List[Dict]) -> Dict[str, Any]:
        """éªŒè¯Promptæ¨¡æ¿"""
        template = self.templates[template_name]
        spec = self.design_specs[template_name]
        
        validation_results = {
            "template_name": template_name,
            "validation_date": datetime.now().isoformat(),
            "test_cases": [],
            "compliance_score": 0.0,
            "issues": []
        }
        
        for test_input in test_inputs:
            try:
                formatted_prompt = template.format(**test_input)
                validation_results["test_cases"].append({
                    "input": test_input,
                    "formatted_prompt": formatted_prompt,
                    "length": len(formatted_prompt),
                    "valid": True
                })
            except Exception as e:
                validation_results["issues"].append(str(e))
                
        return validation_results

# ä½¿ç”¨ç¤ºä¾‹
designer = PromptDesigner()

# åˆ›å»ºå®¢æœPromptè§„èŒƒ
customer_service_spec = PromptDesignSpec(
    name="customer_service_v1",
    version="1.0.0",
    description="æ™ºèƒ½å®¢æœåŠ©æ‰‹Prompt",
    use_case="ç”µå•†å®¢æœè‡ªåŠ¨å›å¤",
    constraints={
        "max_tokens": 150,
        "tone": "friendly",
        "language": "ä¸­æ–‡"
    },
    expected_output={
        "format": "ç»“æ„åŒ–JSON",
        "fields": ["response", "confidence", "category"]
    },
    metadata={
        "domain": "ç”µå•†",
        "role": "å®¢æœåŠ©æ‰‹"
    }
)

template = designer.create_template(customer_service_spec)
```

### ğŸ¯ Promptæ¨¡æ¿åº“

```python
class PromptTemplateLibrary:
    """Promptæ¨¡æ¿åº“ç®¡ç†"""
    
    def __init__(self):
        self.library = {}
        
    def add_template(self, category: str, template: PromptTemplate, metadata: Dict):
        """æ·»åŠ æ¨¡æ¿åˆ°åº“"""
        self.library[category] = {
            "template": template,
            "metadata": metadata,
            "created_at": datetime.now(),
            "usage_count": 0
        }
        
    def get_template(self, category: str, use_case: str) -> Optional[PromptTemplate]:
        """æŒ‰ç”¨ä¾‹è·å–æ¨¡æ¿"""
        key = f"{category}_{use_case}"
        if key in self.library:
            self.library[key]["usage_count"] += 1
            return self.library[key]["template"]
        return None
        
    def list_categories(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰åˆ†ç±»"""
        return list(self.library.keys())

# é¢„å®šä¹‰æ¨¡æ¿
template_library = PromptTemplateLibrary()

# å®¢æœæ¨¡æ¿
customer_service_template = PromptTemplate(
    input_variables=["user_query", "context"],
    template="""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†å®¢æœåŠ©æ‰‹ã€‚
    
    ç”¨æˆ·é—®é¢˜: {user_query}
    ä¸Šä¸‹æ–‡ä¿¡æ¯: {context}
    
    è¯·æä¾›:
    1. å‹å¥½ä¸”å‡†ç¡®çš„å›ç­”
    2. ç›¸å…³äº§å“æ¨è(å¦‚é€‚ç”¨)
    3. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
    
    å›ç­”é™åˆ¶åœ¨150å­—ä»¥å†…ã€‚
    """
)
```

---

## æµ‹è¯•é˜¶æ®µ - Prompt Testing

### ğŸ§ª æµ‹è¯•æ¡†æ¶

```python
from langchain.evaluation import load_evaluator, EvaluatorType
from langchain.schema import LLMResult
import pandas as pd
from typing import List, Dict, Any
import json

class PromptTestSuite:
    """Promptæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, llm, evaluator_llm=None):
        self.llm = llm
        self.evaluator_llm = evaluator_llm or llm
        self.test_cases = []
        self.results = []
        
    def add_test_case(self, 
                     prompt: str, 
                     expected_output: str,
                     test_type: str,
                     metadata: Dict[str, Any] = None):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        test_case = {
            "id": len(self.test_cases) + 1,
            "prompt": prompt,
            "expected_output": expected_output,
            "test_type": test_type,
            "metadata": metadata or {},
            "created_at": datetime.now().isoformat()
        }
        self.test_cases.append(test_case)
        
    def run_tests(self) -> Dict[str, Any]:
        """æ‰§è¡Œæµ‹è¯•"""
        evaluator = load_evaluator(
            EvaluatorType.CRITERIA,
            criteria={
                "accuracy": "Is the response factually accurate?",
                "relevance": "Is the response relevant to the prompt?",
                "completeness": "Does the response fully address the prompt?",
                "tone": "Is the tone appropriate for the context?"
            },
            llm=self.evaluator_llm
        )
        
        test_results = {
            "summary": {
                "total_tests": len(self.test_cases),
                "passed_tests": 0,
                "failed_tests": 0,
                "average_score": 0.0
            },
            "detailed_results": []
        }
        
        for test_case in self.test_cases:
            # ç”Ÿæˆå“åº”
            response = self.llm.invoke(test_case["prompt"])
            
            # è¯„ä¼°å“åº”
            eval_result = evaluator.evaluate_strings(
                prediction=response,
                reference=test_case["expected_output"],
                input=test_case["prompt"]
            )
            
            result = {
                "test_id": test_case["id"],
                "prompt": test_case["prompt"],
                "response": response,
                "expected": test_case["expected_output"],
                "scores": eval_result,
                "passed": eval_result["score"] >= 0.8,
                "test_type": test_case["test_type"]
            }
            
            test_results["detailed_results"].append(result)
            
            if result["passed"]:
                test_results["summary"]["passed_tests"] += 1
            else:
                test_results["summary"]["failed_tests"] += 1
                
        test_results["summary"]["average_score"] = (
            sum(r["scores"]["score"] for r in test_results["detailed_results"]) 
            / len(test_results["detailed_results"])
        )
        
        return test_results

# A/Bæµ‹è¯•æ¡†æ¶
class ABTestFramework:
    """A/Bæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, llm_a, llm_b):
        self.llm_a = llm_a
        self.llm_b = llm_b
        self.test_results = []
        
    def run_ab_test(self, 
                   prompts: List[str],
                   sample_size: int = 100,
                   metric_func=None) -> Dict[str, Any]:
        """æ‰§è¡ŒA/Bæµ‹è¯•"""
        
        if metric_func is None:
            metric_func = self._default_metric
            
        results = {
            "llm_a": {"responses": [], "metrics": []},
            "llm_b": {"responses": [], "metrics": []},
            "winner": None,
            "significance": 0.0
        }
        
        for prompt in prompts:
            for _ in range(sample_size):
                # éšæœºåˆ†é…
                import random
                if random.choice([True, False]):
                    response = self.llm_a.invoke(prompt)
                    results["llm_a"]["responses"].append(response)
                    results["llm_a"]["metrics"].append(metric_func(response))
                else:
                    response = self.llm_b.invoke(prompt)
                    results["llm_b"]["responses"].append(response)
                    results["llm_b"]["metrics"].append(metric_func(response))
                    
        # ç»Ÿè®¡åˆ†æ
        from scipy import stats
        a_scores = results["llm_a"]["metrics"]
        b_scores = results["llm_b"]["metrics"]
        
        t_stat, p_value = stats.ttest_ind(a_scores, b_scores)
        
        if p_value < 0.05:
            if np.mean(a_scores) > np.mean(b_scores):
                results["winner"] = "llm_a"
            else:
                results["winner"] = "llm_b"
                
        results["significance"] = p_value
        
        return results
        
    def _default_metric(self, response: str) -> float:
        """é»˜è®¤è¯„ä¼°æŒ‡æ ‡"""
        return len(response) / 100  # ç®€å•çš„é•¿åº¦æŒ‡æ ‡
```

### ğŸ“Š æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```python
class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def generate_html_report(self, test_results: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"""
        
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Promptæµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .summary { background: #f0f0f0; padding: 20px; border-radius: 5px; }
                .test-case { margin: 10px 0; padding: 10px; border: 1px solid #ddd; }
                .passed { background-color: #d4edda; }
                .failed { background-color: #f8d7da; }
            </style>
        </head>
        <body>
            <h1>Promptæµ‹è¯•æŠ¥å‘Š</h1>
            <div class="summary">
                <h2>æµ‹è¯•æ‘˜è¦</h2>
                <p>æ€»æµ‹è¯•æ•°: {total_tests}</p>
                <p>é€šè¿‡æµ‹è¯•: {passed_tests}</p>
                <p>å¤±è´¥æµ‹è¯•: {failed_tests}</p>
                <p>å¹³å‡åˆ†æ•°: {avg_score:.2f}</p>
            </div>
            
            <h2>è¯¦ç»†ç»“æœ</h2>
            {test_cases}
        </body>
        </html>
        """
        
        test_cases_html = ""
        for result in test_results["detailed_results"]:
            status_class = "passed" if result["passed"] else "failed"
            test_cases_html += f"""
            <div class="test-case {status_class}">
                <h3>æµ‹è¯• {result["test_id"]}</h3>
                <p><strong>Prompt:</strong> {result["prompt"]}</p>
                <p><strong>å“åº”:</strong> {result["response"]}</p>
                <p><strong>åˆ†æ•°:</strong> {result["scores"]["score"]}</p>
            </div>
            """
            
        return html_template.format(
            total_tests=test_results["summary"]["total_tests"],
            passed_tests=test_results["summary"]["passed_tests"],
            failed_tests=test_results["summary"]["failed_tests"],
            avg_score=test_results["summary"]["average_score"],
            test_cases=test_cases_html
        )
```

---

## ç»†åŒ–é˜¶æ®µ - Prompt Refinement

### ğŸ”§ Promptä¼˜åŒ–å¼•æ“

```python
class PromptRefinementEngine:
    """Promptä¼˜åŒ–å¼•æ“"""
    
    def __init__(self, llm, evaluator_llm=None):
        self.llm = llm
        self.evaluator_llm = evaluator_llm or llm
        self.refinement_history = []
        
    def auto_refine(self, 
                   original_prompt: str,
                   test_cases: List[Dict[str, Any]],
                   refinement_strategy: str = "iterative") -> Dict[str, Any]:
        """è‡ªåŠ¨ä¼˜åŒ–Prompt"""
        
        refinement_result = {
            "original_prompt": original_prompt,
            "refined_prompt": None,
            "iterations": [],
            "improvements": [],
            "final_score": 0.0
        }
        
        current_prompt = original_prompt
        
        for iteration in range(5):  # æœ€å¤š5æ¬¡è¿­ä»£
            # è¯„ä¼°å½“å‰Prompt
            test_results = self._evaluate_prompt(current_prompt, test_cases)
            
            iteration_data = {
                "iteration": iteration + 1,
                "prompt": current_prompt,
                "score": test_results["average_score"],
                "issues": test_results["issues"]
            }
            
            refinement_result["iterations"].append(iteration_data)
            
            # å¦‚æœåˆ†æ•°è¶³å¤Ÿé«˜ï¼Œåœæ­¢è¿­ä»£
            if test_results["average_score"] >= 0.9:
                break
                
            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            suggestions = self._generate_suggestions(
                current_prompt, 
                test_results["issues"]
            )
            
            # åº”ç”¨æ”¹è¿›
            current_prompt = self._apply_suggestions(current_prompt, suggestions)
            
        refinement_result["refined_prompt"] = current_prompt
        refinement_result["final_score"] = test_results["average_score"]
        
        return refinement_result
        
    def _evaluate_prompt(self, prompt: str, test_cases: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°Promptæ•ˆæœ"""
        scores = []
        issues = []
        
        for test_case in test_cases:
            response = self.llm.invoke(prompt.format(**test_case["input"]))
            
            # è¯„ä¼°å“åº”è´¨é‡
            score = self._calculate_quality_score(response, test_case["expected"])
            scores.append(score)
            
            if score < 0.8:
                issues.append({
                    "test_case": test_case,
                    "response": response,
                    "score": score,
                    "issue_type": self._identify_issue_type(response, test_case["expected"])
                })
                
        return {
            "average_score": sum(scores) / len(scores),
            "issues": issues
        }
        
    def _generate_suggestions(self, prompt: str, issues: List[Dict]) -> List[str]:
        """åŸºäºé—®é¢˜ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        suggestions = []
        
        for issue in issues:
            issue_type = issue["issue_type"]
            
            if issue_type == "incomplete":
                suggestions.append("Add more specific instructions")
            elif issue_type == "off_topic":
                suggestions.append("Clarify the scope and context")
            elif issue_type == "format_error":
                suggestions.append("Specify output format requirements")
            elif issue_type == "tone_mismatch":
                suggestions.append("Adjust tone specifications")
                
        return suggestions
        
    def _apply_suggestions(self, prompt: str, suggestions: List[str]) -> str:
        """åº”ç”¨æ”¹è¿›å»ºè®®"""
        
        # ä½¿ç”¨LLMç”Ÿæˆæ”¹è¿›åçš„Prompt
        improvement_prompt = f"""
        è¯·åŸºäºä»¥ä¸‹å»ºè®®æ”¹è¿›è¿™ä¸ªPromptï¼š
        
        å½“å‰Prompt: {prompt}
        
        æ”¹è¿›å»ºè®®: {', '.join(suggestions)}
        
        è¯·æä¾›ä¸€ä¸ªæ”¹è¿›åçš„ç‰ˆæœ¬ï¼Œä¿æŒæ ¸å¿ƒåŠŸèƒ½ä½†è§£å†³ä¸Šè¿°é—®é¢˜ã€‚
        """
        
        improved_prompt = self.evaluator_llm.invoke(improvement_prompt)
        return improved_prompt

# ä½¿ç”¨ç¤ºä¾‹
refinement_engine = PromptRefinementEngine(llm=ChatOpenAI())

test_cases = [
    {
        "input": {"user_query": "å¦‚ä½•é€€è´§ï¼Ÿ"},
        "expected": "åŒ…å«é€€è´§æ”¿ç­–ã€æµç¨‹ã€æ—¶é—´é™åˆ¶çš„å®Œæ•´å›ç­”"
    }
]

result = refinement_engine.auto_refine(
    "å›ç­”ç”¨æˆ·çš„å®¢æœé—®é¢˜",
    test_cases
)
```

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

```python
class PromptOptimizer:
    """Promptæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def optimize_for_cost(self, prompt: str, target_cost: float) -> str:
        """ä¼˜åŒ–Tokenæˆæœ¬"""
        
        optimization_prompt = f"""
        ä¼˜åŒ–ä»¥ä¸‹Promptä»¥å‡å°‘tokenä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒæ•ˆæœï¼š
        
        åŸPrompt: {prompt}
        ç›®æ ‡æˆæœ¬: {target_cost} tokens
        
        è¯·æä¾›ä¼˜åŒ–åçš„ç‰ˆæœ¬ã€‚
        """
        
        return self.llm.invoke(optimization_prompt)
        
    def optimize_for_latency(self, prompt: str, target_latency: float) -> str:
        """ä¼˜åŒ–å“åº”å»¶è¿Ÿ"""
        
        # åˆ†æPromptå¤æ‚åº¦
        complexity = self._analyze_complexity(prompt)
        
        # åŸºäºå¤æ‚åº¦ä¼˜åŒ–
        if complexity > target_latency:
            return self._simplify_prompt(prompt)
            
        return prompt
        
    def _analyze_complexity(self, prompt: str) -> float:
        """åˆ†æPromptå¤æ‚åº¦"""
        # åŸºäºtokenæ•°é‡ã€æŒ‡ä»¤æ•°é‡ã€åµŒå¥—å±‚çº§ç­‰
        return len(prompt.split()) / 50
```

---

## è¿­ä»£é˜¶æ®µ - Prompt Iteration

### ğŸ”„ ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ

```python
from git import Repo
import os
from typing import Dict, Any, List
import json

class PromptVersionControl:
    """Promptç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ"""
    
    def __init__(self, repo_path: str):
        self.repo_path = repo_path
        self.repo = Repo.init(repo_path)
        self.prompts_dir = os.path.join(repo_path, "prompts")
        os.makedirs(self.prompts_dir, exist_ok=True)
        
    def save_prompt(self, 
                   prompt_name: str, 
                   prompt_content: str,
                   metadata: Dict[str, Any]) -> str:
        """ä¿å­˜Promptç‰ˆæœ¬"""
        
        version = self._generate_version(prompt_name)
        filename = f"{prompt_name}_v{version}.json"
        filepath = os.path.join(self.prompts_dir, filename)
        
        prompt_data = {
            "name": prompt_name,
            "version": version,
            "content": prompt_content,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat(),
            "hash": self._calculate_hash(prompt_content)
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(prompt_data, f, indent=2, ensure_ascii=False)
            
        # Gitæäº¤
        self.repo.index.add([filepath])
        self.repo.index.commit(f"Add prompt {prompt_name} v{version}")
        
        return version
        
    def load_prompt(self, prompt_name: str, version: str = None) -> Dict[str, Any]:
        """åŠ è½½ç‰¹å®šç‰ˆæœ¬Prompt"""
        
        if version is None:
            # è·å–æœ€æ–°ç‰ˆæœ¬
            version = self._get_latest_version(prompt_name)
            
        filename = f"{prompt_name}_v{version}.json"
        filepath = os.path.join(self.prompts_dir, filename)
        
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
            
    def compare_versions(self, 
                        prompt_name: str, 
                        version1: str, 
                        version2: str) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬"""
        
        prompt1 = self.load_prompt(prompt_name, version1)
        prompt2 = self.load_prompt(prompt_name, version2)
        
        return {
            "diff": self._calculate_diff(prompt1["content"], prompt2["content"]),
            "metadata_changes": self._compare_metadata(
                prompt1["metadata"], 
                prompt2["metadata"]
            ),
            "performance_comparison": self._compare_performance(
                prompt1["metadata"], 
                prompt2["metadata"]
            )
        }
        
    def _generate_version(self, prompt_name: str) -> str:
        """ç”Ÿæˆæ–°ç‰ˆæœ¬å·"""
        existing = [f for f in os.listdir(self.prompts_dir) 
                   if f.startswith(f"{prompt_name}_v")]
        
        if not existing:
            return "1.0.0"
            
        versions = [f.split("_v")[1].split(".json")[0] for f in existing]
        latest = max(versions, key=lambda v: [int(x) for x in v.split(".")])
        
        parts = latest.split(".")
        parts[-1] = str(int(parts[-1]) + 1)
        return ".".join(parts)

# ä½¿ç”¨ç¤ºä¾‹
version_control = PromptVersionControl("./prompt_repo")

# ä¿å­˜æ–°ç‰ˆæœ¬
version = version_control.save_prompt(
    "customer_service",
    "ä¼˜åŒ–åçš„å®¢æœPrompt...",
    {
        "accuracy": 0.95,
        "latency": 1.2,
        "cost": 0.05,
        "tags": ["å®¢æœ", "ä¼˜åŒ–"]
    }
)
```

### ğŸ“Š è¿­ä»£åˆ†æä»ªè¡¨æ¿

```python
class IterationAnalytics:
    """è¿­ä»£åˆ†æä»ªè¡¨æ¿"""
    
    def __init__(self, version_control: PromptVersionControl):
        self.version_control = version_control
        
    def generate_iteration_report(self, prompt_name: str) -> Dict[str, Any]:
        """ç”Ÿæˆè¿­ä»£æŠ¥å‘Š"""
        
        versions = self._get_all_versions(prompt_name)
        
        report = {
            "prompt_name": prompt_name,
            "total_versions": len(versions),
            "iteration_timeline": [],
            "performance_trends": {},
            "key_improvements": []
        }
        
        for version in versions:
            prompt_data = self.version_control.load_prompt(prompt_name, version)
            
            report["iteration_timeline"].append({
                "version": version,
                "date": prompt_data["timestamp"],
                "metadata": prompt_data["metadata"]
            })
            
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        report["performance_trends"] = self._analyze_trends(
            report["iteration_timeline"]
        )
        
        return report
        
    def _analyze_trends(self, timeline: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        
        if len(timeline) < 2:
            return {"message": "Insufficient data for trend analysis"}
            
        # è®¡ç®—è¶‹åŠ¿
        accuracies = [item["metadata"].get("accuracy", 0) for item in timeline]
        latencies = [item["metadata"].get("latency", 0) for item in timeline]
        costs = [item["metadata"].get("cost", 0) for item in timeline]
        
        return {
            "accuracy_trend": {
                "start": accuracies[0],
                "end": accuracies[-1],
                "change": accuracies[-1] - accuracies[0]
            },
            "latency_trend": {
                "start": latencies[0],
                "end": latencies[-1],
                "change": latencies[-1] - latencies[0]
            },
            "cost_trend": {
                "start": costs[0],
                "end": costs[-1],
                "change": costs[-1] - costs[0]
            }
        }
```

---

## éƒ¨ç½²é˜¶æ®µ - Prompt Deployment

### ğŸš€ CI/CDé›†æˆ

```python
from langchain_core.runnables import Runnable
import docker
import kubernetes
from typing import Dict, Any
import yaml

class PromptDeploymentPipeline:
    """Promptéƒ¨ç½²ç®¡é“"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.docker_client = docker.from_env()
        
    def package_prompt(self, 
                      prompt_name: str, 
                      version: str,
                      environment: str) -> Dict[str, Any]:
        """æ‰“åŒ…Prompt"""
        
        prompt_data = self._load_prompt_data(prompt_name, version)
        
        package = {
            "name": prompt_name,
            "version": version,
            "environment": environment,
            "prompt_content": prompt_data["content"],
            "metadata": prompt_data["metadata"],
            "dependencies": self._resolve_dependencies(prompt_data),
            "docker_config": self._generate_docker_config(prompt_data),
            "k8s_config": self._generate_k8s_config(prompt_data)
        }
        
        return package
        
    def deploy_to_environment(self, 
                            package: Dict[str, Any], 
                            target_env: str) -> Dict[str, Any]:
        """éƒ¨ç½²åˆ°ç›®æ ‡ç¯å¢ƒ"""
        
        deployment_result = {
            "status": "success",
            "deployment_id": self._generate_deployment_id(),
            "environment": target_env,
            "timestamp": datetime.now().isoformat(),
            "logs": []
        }
        
        try:
            # æ„å»ºDockeré•œåƒ
            image = self._build_docker_image(package)
            deployment_result["logs"].append(f"Built image: {image.id}")
            
            # éƒ¨ç½²åˆ°Kubernetes
            if target_env in ["staging", "production"]:
                deployment = self._deploy_to_k8s(package, target_env)
                deployment_result["logs"].append(f"K8s deployment: {deployment.metadata.name}")
                
            # æ›´æ–°é…ç½®ä¸­å¿ƒ
            self._update_config_center(package, target_env)
            deployment_result["logs"].append("Updated config center")
            
        except Exception as e:
            deployment_result["status"] = "failed"
            deployment_result["error"] = str(e)
            
        return deployment_result
        
    def _build_docker_image(self, package: Dict[str, Any]) -> docker.models.images.Image:
        """æ„å»ºDockeré•œåƒ"""
        
        dockerfile_content = f"""
        FROM python:3.9-slim
        
        WORKDIR /app
        
        COPY requirements.txt .
        RUN pip install -r requirements.txt
        
        COPY prompt_config.json .
        COPY prompt_runner.py .
        
        ENV PROMPT_NAME={package["name"]}
        ENV PROMPT_VERSION={package["version"]}
        
        CMD ["python", "prompt_runner.py"]
        """
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            dockerfile_path = os.path.join(temp_dir, "Dockerfile")
            with open(dockerfile_path, "w") as f:
                f.write(dockerfile_content)
                
            # æ„å»ºé•œåƒ
            image, logs = self.docker_client.images.build(
                path=temp_dir,
                tag=f"prompt-{package['name']}:{package['version']}",
                rm=True
            )
            
            return image
            
    def _generate_k8s_config(self, package: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆKubernetesé…ç½®"""
        
        return {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": f"prompt-{package['name']}",
                "labels": {
                    "app": package["name"],
                    "version": package["version"]
                }
            },
            "spec": {
                "replicas": 3,
                "selector": {
                    "matchLabels": {
                        "app": package["name"]
                    }
                },
                "template": {
                    "metadata": {
                        "labels": {
                            "app": package["name"],
                            "version": package["version"]
                        }
                    },
                    "spec": {
                        "containers": [{
                            "name": "prompt-service",
                            "image": f"prompt-{package['name']}:{package['version']}",
                            "ports": [{"containerPort": 8000}],
                            "env": [
                                {"name": "ENVIRONMENT", "value": package["environment"]},
                                {"name": "PROMPT_CONFIG", "value": json.dumps(package)}
                            ]
                        }]
                    }
                }
            }
        }

# éƒ¨ç½²é…ç½®
deployment_config = {
    "environments": {
        "development": {
            "replicas": 1,
            "resources": {"memory": "512Mi", "cpu": "500m"}
        },
        "staging": {
            "replicas": 2,
            "resources": {"memory": "1Gi", "cpu": "1000m"}
        },
        "production": {
            "replicas": 3,
            "resources": {"memory": "2Gi", "cpu": "2000m"}
        }
    }
}
```

### ğŸ”„ è“ç»¿éƒ¨ç½²

```python
class BlueGreenDeployment:
    """è“ç»¿éƒ¨ç½²ç®¡ç†"""
    
    def __init__(self, k8s_client):
        self.k8s_client = k8s_client
        
    def deploy_blue_green(self, 
                         prompt_name: str, 
                         new_version: str,
                         health_check_func) -> Dict[str, Any]:
        """æ‰§è¡Œè“ç»¿éƒ¨ç½²"""
        
        # è·å–å½“å‰ç‰ˆæœ¬
        current_version = self._get_current_version(prompt_name)
        
        # éƒ¨ç½²æ–°ç‰ˆæœ¬ï¼ˆç»¿è‰²ï¼‰
        green_deployment = self._deploy_version(
            prompt_name, 
            new_version, 
            "green"
        )
        
        # å¥åº·æ£€æŸ¥
        if health_check_func(green_deployment):
            # åˆ‡æ¢æµé‡åˆ°ç»¿è‰²
            self._switch_traffic(prompt_name, "green")
            
            # ç­‰å¾…ç¨³å®š
            time.sleep(30)
            
            # åˆ é™¤è“è‰²ç‰ˆæœ¬
            self._delete_deployment(prompt_name, "blue")
            
            return {
                "status": "success",
                "new_version": new_version,
                "old_version": current_version
            }
        else:
            # å›æ»š
            self._delete_deployment(prompt_name, "green")
            return {
                "status": "failed",
                "error": "Health check failed"
            }
```

---

## ç»´æŠ¤é˜¶æ®µ - Prompt Maintenance

### ğŸ”§ ç»´æŠ¤å·¥ä½œæµ

```python
class PromptMaintenanceEngine:
    """Promptç»´æŠ¤å¼•æ“"""
    
    def __init__(self, llm, monitor_client):
        self.llm = llm
        self.monitor = monitor_client
        
    def scheduled_maintenance(self, prompt_name: str) -> Dict[str, Any]:
        """å®šæœŸç»´æŠ¤æ£€æŸ¥"""
        
        maintenance_report = {
            "prompt_name": prompt_name,
            "maintenance_date": datetime.now().isoformat(),
            "checks": [],
            "actions": []
        }
        
        # æ€§èƒ½æ£€æŸ¥
        performance_metrics = self._check_performance(prompt_name)
        maintenance_report["checks"].append({
            "type": "performance",
            "metrics": performance_metrics
        })
        
        # å‡†ç¡®æ€§æ£€æŸ¥
        accuracy_metrics = self._check_accuracy(prompt_name)
        maintenance_report["checks"].append({
            "type": "accuracy",
            "metrics": accuracy_metrics
        })
        
        # æˆæœ¬æ£€æŸ¥
        cost_metrics = self._check_cost(prompt_name)
        maintenance_report["checks"].append({
            "type": "cost",
            "metrics": cost_metrics
        })
        
        # ç”Ÿæˆç»´æŠ¤å»ºè®®
        recommendations = self._generate_maintenance_recommendations(
            maintenance_report["checks"]
        )
        
        maintenance_report["recommendations"] = recommendations
        
        # æ‰§è¡Œå¿…è¦çš„ç»´æŠ¤æ“ä½œ
        for recommendation in recommendations:
            if recommendation["priority"] == "high":
                action_result = self._execute_maintenance_action(recommendation)
                maintenance_report["actions"].append(action_result)
                
        return maintenance_report
        
    def _check_performance(self, prompt_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡"""
        
        # è·å–ç›‘æ§æ•°æ®
        metrics = self.monitor.get_metrics(prompt_name, days=7)
        
        return {
            "average_latency": metrics.get("avg_latency", 0),
            "p95_latency": metrics.get("p95_latency", 0),
            "error_rate": metrics.get("error_rate", 0),
            "throughput": metrics.get("throughput", 0)
        }
        
    def _generate_maintenance_recommendations(self, checks: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆç»´æŠ¤å»ºè®®"""
        
        recommendations = []
        
        for check in checks:
            if check["type"] == "performance":
                if check["metrics"]["average_latency"] > 2.0:
                    recommendations.append({
                        "type": "optimize_latency",
                        "priority": "high",
                        "description": "å¹³å‡å»¶è¿Ÿè¶…è¿‡2ç§’ï¼Œéœ€è¦ä¼˜åŒ–"
                    })
                    
            elif check["type"] == "accuracy":
                if check["metrics"]["accuracy"] < 0.85:
                    recommendations.append({
                        "type": "retrain_prompt",
                        "priority": "high",
                        "description": "å‡†ç¡®ç‡ä½äº85%ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ"
                    })
                    
        return recommendations

# è‡ªåŠ¨ç»´æŠ¤è°ƒåº¦
class MaintenanceScheduler:
    """ç»´æŠ¤è°ƒåº¦å™¨"""
    
    def __init__(self, maintenance_engine: PromptMaintenanceEngine):
        self.engine = maintenance_engine
        
    def schedule_maintenance(self, prompt_name: str, schedule: str):
        """è°ƒåº¦ç»´æŠ¤ä»»åŠ¡"""
        
        if schedule == "daily":
            # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
            self._schedule_cron_job(prompt_name, "0 2 * * *")
        elif schedule == "weekly":
            # æ¯å‘¨æ—¥å‡Œæ™¨æ‰§è¡Œ
            self._schedule_cron_job(prompt_name, "0 0 * * 0")
        elif schedule == "monthly":
            # æ¯æœˆ1æ—¥å‡Œæ™¨æ‰§è¡Œ
            self._schedule_cron_job(prompt_name, "0 0 1 * *")
```

---

## ç›‘æ§é˜¶æ®µ - Prompt Monitoring

### ğŸ“Š å®æ—¶ç›‘æ§ä»ªè¡¨æ¿

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from typing import Dict, Any, List
import asyncio

class PromptMonitoringSystem:
    """Promptç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, port: int = 8000):
        self.port = port
        
        # æŒ‡æ ‡å®šä¹‰
        self.prompt_requests = Counter(
            'prompt_requests_total',
            'Total number of prompt requests',
            ['prompt_name', 'version', 'status']
        )
        
        self.prompt_latency = Histogram(
            'prompt_latency_seconds',
            'Prompt response latency',
            ['prompt_name', 'version']
        )
        
        self.prompt_accuracy = Gauge(
            'prompt_accuracy_score',
            'Prompt accuracy score',
            ['prompt_name', 'version']
        )
        
        self.prompt_cost = Gauge(
            'prompt_cost_per_request',
            'Cost per prompt request',
            ['prompt_name', 'version']
        )
        
        # å¯åŠ¨ç›‘æ§æœåŠ¡å™¨
        start_http_server(port)
        
    def record_request(self, prompt_name: str, version: str, status: str):
        """è®°å½•è¯·æ±‚"""
        self.prompt_requests.labels(
            prompt_name=prompt_name,
            version=version,
            status=status
        ).inc()
        
    def record_latency(self, prompt_name: str, version: str, duration: float):
        """è®°å½•å»¶è¿Ÿ"""
        self.prompt_latency.labels(
            prompt_name=prompt_name,
            version=version
        ).observe(duration)
        
    def record_accuracy(self, prompt_name: str, version: str, accuracy: float):
        """è®°å½•å‡†ç¡®æ€§"""
        self.prompt_accuracy.labels(
            prompt_name=prompt_name,
            version=version
        ).set(accuracy)
        
    def record_cost(self, prompt_name: str, version: str, cost: float):
        """è®°å½•æˆæœ¬"""
        self.prompt_cost.labels(
            prompt_name=prompt_name,
            version=version
        ).set(cost)
        
    def create_alert_rule(self, prompt_name: str, threshold: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå‘Šè­¦è§„åˆ™"""
        
        alert_rule = {
            "alert": f"PromptPerformanceAlert_{prompt_name}",
            "expr": f"prompt_latency_seconds{{prompt_name='{prompt_name}'}} > {threshold['latency']}",
            "for": "5m",
            "labels": {
                "severity": "warning",
                "prompt_name": prompt_name
            },
            "annotations": {
                "summary": f"Prompt {prompt_name} latency is high",
                "description": f"Prompt {prompt_name} has latency above {threshold['latency']}s"
            }
        }
        
        return alert_rule

class RealTimeDashboard:
    """å®æ—¶ä»ªè¡¨æ¿"""
    
    def __init__(self, monitor_system: PromptMonitoringSystem):
        self.monitor = monitor_system
        
    def generate_dashboard_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆä»ªè¡¨æ¿æ•°æ®"""
        
        return {
            "overview": {
                "total_prompts": self._get_total_prompts(),
                "active_versions": self._get_active_versions(),
                "overall_health": self._calculate_overall_health()
            },
            "performance": {
                "top_slowest_prompts": self._get_top_slowest_prompts(),
                "error_rates": self._get_error_rates(),
                "cost_analysis": self._get_cost_analysis()
            },
            "alerts": self._get_active_alerts()
        }
        
    def _calculate_overall_health(self) -> float:
        """è®¡ç®—æ•´ä½“å¥åº·åº¦"""
        
        # åŸºäºå¤šä¸ªæŒ‡æ ‡è®¡ç®—å¥åº·åº¦
        metrics = {
            "latency": 0.4,
            "accuracy": 0.3,
            "cost": 0.2,
            "availability": 0.1
        }
        
        health_score = 0.0
        
        for metric, weight in metrics.items():
            value = self._get_metric_value(metric)
            health_score += value * weight
            
        return min(health_score, 1.0)
```

---

## é€€å½¹é˜¶æ®µ - Prompt Retirement

### ğŸ—‘ï¸ ä¼˜é›…é€€å½¹æµç¨‹

```python
class PromptRetirementManager:
    """Prompté€€å½¹ç®¡ç†å™¨"""
    
    def __init__(self, notification_client, migration_client):
        self.notification = notification_client
        self.migration = migration_client
        
    def initiate_retirement(self, 
                          prompt_name: str, 
                          retirement_plan: Dict[str, Any]) -> Dict[str, Any]:
        """å¯åŠ¨é€€å½¹æµç¨‹"""
        
        retirement_process = {
            "prompt_name": prompt_name,
            "retirement_id": self._generate_retirement_id(),
            "status": "initiated",
            "timeline": retirement_plan["timeline"],
            "migration_strategy": retirement_plan["migration_strategy"],
            "notifications": []
        }
        
        # å‘é€é€€å½¹é€šçŸ¥
        notification = self.notification.send_retirement_notice(
            prompt_name,
            retirement_plan["affected_systems"],
            retirement_plan["timeline"]
        )
        
        retirement_process["notifications"].append(notification)
        
        # æ‰§è¡Œæ•°æ®è¿ç§»
        if retirement_plan["migration_strategy"] == "gradual":
            migration_result = self.migration.start_gradual_migration(
                prompt_name,
                retirement_plan["replacement_prompt"]
            )
        else:
            migration_result = self.migration.start_immediate_migration(
                prompt_name,
                retirement_plan["replacement_prompt"]
            )
            
        retirement_process["migration_result"] = migration_result
        
        return retirement_process
        
    def execute_retirement(self, retirement_id: str) -> Dict[str, Any]:
        """æ‰§è¡Œé€€å½¹"""
        
        # éªŒè¯æ‰€æœ‰ä¾èµ–å·²è¿ç§»
        validation_result = self._validate_retirement_readiness(retirement_id)
        
        if not validation_result["ready"]:
            return {
                "status": "failed",
                "reason": "Dependencies not ready",
                "details": validation_result["issues"]
            }
            
        # åœæ­¢æœåŠ¡
        shutdown_result = self._shutdown_prompt_service(retirement_id)
        
        # æ¸…ç†èµ„æº
        cleanup_result = self._cleanup_resources(retirement_id)
        
        # ç”Ÿæˆé€€å½¹æŠ¥å‘Š
        retirement_report = {
            "retirement_id": retirement_id,
            "completion_date": datetime.now().isoformat(),
            "final_status": "completed",
            "affected_systems": self._get_affected_systems(retirement_id),
            "data_migrated": cleanup_result["data_migrated"],
            "resources_released": cleanup_result["resources_released"]
        }
        
        return retirement_report
        
    def _validate_retirement_readiness(self, retirement_id: str) -> Dict[str, Any]:
        """éªŒè¯é€€å½¹å‡†å¤‡å°±ç»ª"""
        
        return {
            "ready": True,
            "issues": []
        }
```

---

## ç»Ÿä¸€ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨

### ğŸ¯ ç»Ÿä¸€ç®¡ç†å¹³å°

```python
class UnifiedPromptLifecycleManager:
    """ç»Ÿä¸€Promptç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # åˆå§‹åŒ–å„ä¸ªé˜¶æ®µçš„ç®¡ç†å™¨
        self.designer = PromptDesigner()
        self.test_suite = PromptTestSuite()
        self.refinement_engine = PromptRefinementEngine()
        self.version_control = PromptVersionControl(config["repo_path"])
        self.deployment = PromptDeploymentPipeline(config)
        self.maintenance = PromptMaintenanceEngine()
        self.monitoring = PromptMonitoringSystem()
        self.retirement = PromptRetirementManager()
        
        # ç”Ÿå‘½å‘¨æœŸçŠ¶æ€æœº
        self.state_machine = self._initialize_state_machine()
        
    def create_lifecycle(self, prompt_spec: PromptDesignSpec) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæ•´çš„Promptç”Ÿå‘½å‘¨æœŸ"""
        
        lifecycle_id = self._generate_lifecycle_id()
        
        lifecycle = {
            "id": lifecycle_id,
            "prompt_spec": prompt_spec,
            "current_stage": "design",
            "history": [],
            "artifacts": {}
        }
        
        # è®¾è®¡é˜¶æ®µ
        template = self.designer.create_template(prompt_spec)
        lifecycle["artifacts"]["template"] = template
        
        # æµ‹è¯•é˜¶æ®µ
        test_results = self.test_suite.run_tests()
        lifecycle["artifacts"]["test_results"] = test_results
        
        # ç‰ˆæœ¬æ§åˆ¶
        version = self.version_control.save_prompt(
            prompt_spec.name,
            template.template,
            {"stage": "initial", "test_results": test_results}
        )
        
        lifecycle["version"] = version
        
        return lifecycle
        
    def transition_stage(self, lifecycle_id: str, target_stage: str) -> Dict[str, Any]:
        """è½¬æ¢ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ"""
        
        lifecycle = self._get_lifecycle(lifecycle_id)
        current_stage = lifecycle["current_stage"]
        
        # éªŒè¯é˜¶æ®µè½¬æ¢
        if not self._validate_stage_transition(current_stage, target_stage):
            return {
                "error": "Invalid stage transition",
                "from": current_stage,
                "to": target_stage
            }
            
        # æ‰§è¡Œé˜¶æ®µè½¬æ¢
        transition_result = self._execute_stage_transition(
            lifecycle,
            current_stage,
            target_stage
        )
        
        # è®°å½•å†å²
        lifecycle["history"].append({
            "from": current_stage,
            "to": target_stage,
            "timestamp": datetime.now().isoformat(),
            "result": transition_result
        })
        
        lifecycle["current_stage"] = target_stage
        
        return transition_result
        
    def get_lifecycle_status(self, lifecycle_id: str) -> Dict[str, Any]:
        """è·å–ç”Ÿå‘½å‘¨æœŸçŠ¶æ€"""
        
        lifecycle = self._get_lifecycle(lifecycle_id)
        
        return {
            "id": lifecycle_id,
            "current_stage": lifecycle["current_stage"],
            "stage_duration": self._calculate_stage_duration(lifecycle),
            "next_possible_stages": self._get_next_stages(lifecycle["current_stage"]),
            "health_status": self._calculate_health_status(lifecycle),
            "metrics": self._get_lifecycle_metrics(lifecycle)
        }
        
    def _initialize_state_machine(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–çŠ¶æ€æœº"""
        
        return {
            "design": ["test", "retire"],
            "test": ["refine", "deploy", "retire"],
            "refine": ["test", "deploy", "retire"],
            "deploy": ["monitor", "retire"],
            "monitor": ["maintain", "retire"],
            "maintain": ["deploy", "retire"],
            "retire": []  # ç»ˆæ­¢çŠ¶æ€
        }

# ä½¿ç”¨ç¤ºä¾‹
lifecycle_manager = UnifiedPromptLifecycleManager({
    "repo_path": "./prompt_lifecycle_repo",
    "environments": ["dev", "staging", "prod"],
    "monitoring": {"enabled": True, "port": 9090}
})

# åˆ›å»ºå®Œæ•´çš„Promptç”Ÿå‘½å‘¨æœŸ
spec = PromptDesignSpec(
    name="advanced_qa_system",
    version="1.0.0",
    description="é«˜çº§é—®ç­”ç³»ç»Ÿ",
    use_case="ä¼ä¸šçŸ¥è¯†åº“é—®ç­”",
    constraints={"max_tokens": 500, "language": "ä¸­æ–‡"},
    expected_output={"format": "ç»“æ„åŒ–", "confidence_threshold": 0.9}
)

lifecycle = lifecycle_manager.create_lifecycle(spec)
```

### ğŸ“Š ç”Ÿå‘½å‘¨æœŸä»ªè¡¨æ¿

```python
class LifecycleDashboard:
    """ç”Ÿå‘½å‘¨æœŸä»ªè¡¨æ¿"""
    
    def __init__(self, lifecycle_manager: UnifiedPromptLifecycleManager):
        self.manager = lifecycle_manager
        
    def generate_overview_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¦‚è§ˆæŠ¥å‘Š"""
        
        return {
            "summary": {
                "total_prompts": self._get_total_prompts(),
                "active_lifecycles": self._get_active_lifecycles(),
                "stage_distribution": self._get_stage_distribution(),
                "health_overview": self._get_health_overview()
            },
            "performance": {
                "average_time_to_production": self._calculate_avg_time_to_prod(),
                "success_rate_by_stage": self._get_success_rates(),
                "bottleneck_analysis": self._analyze_bottlenecks()
            },
            "recommendations": self._generate_recommendations()
        }
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### ğŸ¯ å®æ–½è·¯çº¿å›¾

```mermaid
timeline
    title Promptç”Ÿå‘½å‘¨æœŸå®æ–½è·¯çº¿å›¾
    
    ç¬¬1å‘¨ : è®¾è®¡é˜¶æ®µ
          : å»ºç«‹è§„èŒƒ
          : åˆ›å»ºæ¨¡æ¿åº“
    
    ç¬¬2-3å‘¨ : æµ‹è¯•é˜¶æ®µ
            : æ„å»ºæµ‹è¯•å¥—ä»¶
            : A/Bæµ‹è¯•æ¡†æ¶
    
    ç¬¬4-5å‘¨ : ç»†åŒ–é˜¶æ®µ
            : ä¼˜åŒ–å¼•æ“
            : æ€§èƒ½è°ƒä¼˜
    
    ç¬¬6-8å‘¨ : éƒ¨ç½²é˜¶æ®µ
            : CI/CDé›†æˆ
            : è“ç»¿éƒ¨ç½²
    
    ç¬¬9-10å‘¨ : ç›‘æ§é˜¶æ®µ
             : ä»ªè¡¨æ¿
             : å‘Šè­¦ç³»ç»Ÿ
    
    æŒç»­ : ç»´æŠ¤é˜¶æ®µ
         : å®šæœŸä¼˜åŒ–
         : ç‰ˆæœ¬è¿­ä»£
```

### ğŸ“‹ æ£€æŸ¥æ¸…å•

#### âœ… è®¾è®¡é˜¶æ®µ
- [ ] Promptè§„èŒƒæ–‡æ¡£
- [ ] æ¨¡æ¿åº“å»ºç«‹
- [ ] éªŒè¯æœºåˆ¶

#### âœ… æµ‹è¯•é˜¶æ®µ
- [ ] æµ‹è¯•ç”¨ä¾‹è¦†ç›–
- [ ] A/Bæµ‹è¯•æ¡†æ¶
- [ ] æ€§èƒ½åŸºå‡†

#### âœ… éƒ¨ç½²é˜¶æ®µ
- [ ] CI/CDç®¡é“
- [ ] ç¯å¢ƒé…ç½®
- [ ] å›æ»šç­–ç•¥

#### âœ… ç›‘æ§é˜¶æ®µ
- [ ] å®æ—¶ä»ªè¡¨æ¿
- [ ] å‘Šè­¦è§„åˆ™
- [ ] ç»´æŠ¤è®¡åˆ’

è¿™å¥—å®Œæ•´çš„Promptç”Ÿå‘½å‘¨æœŸç®¡ç†ä½“ç³»ä¸ºæ‚¨çš„LangChainåº”ç”¨æä¾›äº†ä»è®¾è®¡åˆ°é€€å½¹çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œç¡®ä¿Promptçš„è´¨é‡ã€æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚
        