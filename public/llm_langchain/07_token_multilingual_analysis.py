#!/usr/bin/env python3
"""
å¤šè¯­è¨€Tokenæ•ˆçŽ‡æ·±åº¦åˆ†æž
å±•ç¤ºä¸åŒè¯­è¨€åœ¨GPTæ¨¡åž‹ä¸­çš„tokenä½¿ç”¨æ•ˆçŽ‡
"""

import tiktoken
import pandas as pd
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

class MultilingualTokenAnalyzer:
    """å¤šè¯­è¨€Tokenåˆ†æžå™¨"""
    
    def __init__(self):
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        # æµ‹è¯•æ–‡æœ¬ï¼šç›¸åŒè¯­ä¹‰çš„ä¸åŒè¯­è¨€è¡¨è¾¾
        self.test_texts = {
            "english": "Artificial intelligence is transforming the world through machine learning and deep learning technologies.",
            "chinese": "äººå·¥æ™ºèƒ½æ­£åœ¨é€šè¿‡æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ”¹å˜ä¸–ç•Œã€‚",
            "japanese": "äººå·¥çŸ¥èƒ½ã¯æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’æŠ€è¡“ã‚’é€šã˜ã¦ä¸–ç•Œã‚’å¤‰é©ã—ã¦ã„ã¾ã™ã€‚",
            "korean": "ì¸ê³µì§€ëŠ¥ì€ ê¸°ê³„ í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í†µí•´ ì„¸ê³„ë¥¼ ë³€í™”ì‹œí‚¤ê³  ìžˆìŠµë‹ˆë‹¤.",
            "french": "L'intelligence artificielle transforme le monde grÃ¢ce aux technologies d'apprentissage automatique et d'apprentissage profond.",
            "german": "KÃ¼nstliche Intelligenz verÃ¤ndert die Welt durch Technologien des maschinellen Lernens und des Deep Learning.",
            "spanish": "La inteligencia artificial estÃ¡ transformando el mundo a travÃ©s de tecnologÃ­as de aprendizaje automÃ¡tico y aprendizaje profundo.",
            "russian": "Ð˜ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ð¼Ð¸Ñ€ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ.",
            "emoji": "ðŸ¤– AI ðŸ¤– is ðŸ¤– transforming ðŸ¤– the ðŸ¤– world ðŸ¤– through ðŸ¤– ML ðŸ¤–"
        }
    
    def analyze_efficiency(self) -> pd.DataFrame:
        """åˆ†æžå„è¯­è¨€tokenæ•ˆçŽ‡"""
        results = []
        
        for language, text in self.test_texts.items():
            tokens = self.encoding.encode(text)
            decoded_tokens = [self.encoding.decode([t]) for t in tokens]
            
            analysis = {
                "language": language,
                "text": text,
                "char_count": len(text),
                "token_count": len(tokens),
                "byte_length": len(text.encode('utf-8')),
                "tokens_per_char": len(tokens) / len(text),
                "chars_per_token": len(text) / len(tokens),
                "compression_ratio": len(text.encode('utf-8')) / (len(tokens) * 4),
                "tokens": decoded_tokens
            }
            
            results.append(analysis)
        
        return pd.DataFrame(results)
    
    def visualize_efficiency(self, df: pd.DataFrame):
        """å¯è§†åŒ–tokenæ•ˆçŽ‡"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Tokenæ•°é‡å¯¹æ¯”
        languages = [lang.capitalize() for lang in df['language']]
        token_counts = df['token_count']
        
        ax1.bar(languages, token_counts, color='skyblue')
        ax1.set_title('ä¸åŒè¯­è¨€çš„Tokenæ•°é‡å¯¹æ¯”', fontsize=14, pad=20)
        ax1.set_ylabel('Tokenæ•°é‡')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. å­—ç¬¦ä¸ŽTokenæ¯”ä¾‹
        ax2.scatter(df['char_count'], df['token_count'], 
                   s=df['chars_per_token']*50, alpha=0.7, color='coral')
        
        for i, lang in enumerate(df['language']):
            ax2.annotate(lang.capitalize(), 
                        (df['char_count'].iloc[i], df['token_count'].iloc[i]),
                        xytext=(5, 5), textcoords='offset points', fontsize=10)
        
        ax2.set_xlabel('å­—ç¬¦æ•°')
        ax2.set_ylabel('Tokenæ•°')
        ax2.set_title('å­—ç¬¦æ•° vs Tokenæ•°å…³ç³»', fontsize=14)
        
        # 3. åŽ‹ç¼©æ•ˆçŽ‡
        ax3.bar(languages, df['compression_ratio'], color='lightgreen')
        ax3.set_title('åŽ‹ç¼©æ•ˆçŽ‡å¯¹æ¯”', fontsize=14, pad=20)
        ax3.set_ylabel('åŽ‹ç¼©æ¯”')
        ax3.tick_params(axis='x', rotation=45)
        
        # 4. Tokenå¯†åº¦çƒ­åŠ›å›¾
        efficiency_data = df[['language', 'tokens_per_char', 'chars_per_token']]
        sns.heatmap(efficiency_data.set_index('language').T, 
                   annot=True, fmt='.2f', cmap='YlOrRd', ax=ax4)
        ax4.set_title('Tokenå¯†åº¦çƒ­åŠ›å›¾', fontsize=14)
        
        plt.tight_layout()
        plt.savefig('multilingual_token_efficiency.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†åˆ†æžæŠ¥å‘Š"""
        
        df = self.analyze_efficiency()
        
        report = f"""
# å¤šè¯­è¨€Tokenæ•ˆçŽ‡åˆ†æžæŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦
- **æµ‹è¯•æ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æµ‹è¯•æ¨¡åž‹**: GPT-3.5-Turbo
- **æµ‹è¯•æ–‡æœ¬é•¿åº¦**: å„è¯­è¨€è¯­ä¹‰ç›¸åŒ

## è¯¦ç»†æ•°æ®

{df.to_string(index=False)}

## å…³é”®å‘çŽ°

### 1. Tokenæ•ˆçŽ‡æŽ’åï¼ˆä»Žä½Žåˆ°é«˜ï¼‰
"""
        
        # æŒ‰tokenæ•°é‡æŽ’åº
        df_sorted = df.sort_values('token_count')
        
        for idx, row in df_sorted.iterrows():
            efficiency = "é«˜" if row['chars_per_token'] > 3 else "ä¸­" if row['chars_per_token'] > 2 else "ä½Ž"
            report += f"""
**{row['language'].capitalize()}**: 
- Tokenæ•°: {row['token_count']}
- å­—ç¬¦/tokenæ¯”: {row['chars_per_token']:.2f}
- æ•ˆçŽ‡ç­‰çº§: {efficiency}
"""
        
        report += f"""
### 2. æˆæœ¬å½±å“åˆ†æž
- **æœ€ç»æµŽè¯­è¨€**: {df.loc[df['token_count'].idxmin(), 'language'].capitalize()}
- **æœ€æ˜‚è´µè¯­è¨€**: {df.loc[df['token_count'].idxmax(), 'language'].capitalize()}
- **æˆæœ¬å·®å¼‚**: {df['token_count'].max() / df['token_count'].min():.2f}x

### 3. ä¼˜åŒ–å»ºè®®
"""
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        high_efficiency = df[df['chars_per_token'] > 3]['language'].tolist()
        low_efficiency = df[df['chars_per_token'] <= 2]['language'].tolist()
        
        report += f"""
- **é«˜æ•ˆè¯­è¨€** ({', '.join([lang.capitalize() for lang in high_efficiency])}):
  é€‚åˆé•¿æ–‡æœ¬å¤„ç†ï¼Œæˆæœ¬æ•ˆç›Šå¥½

- **ä½Žæ•ˆè¯­è¨€** ({', '.join([lang.capitalize() for lang in low_efficiency])}):
  å»ºè®®ç²¾ç®€æ–‡æœ¬ï¼Œä½¿ç”¨ç¼©å†™å’Œç®€å†™

- **ä¸­æ–‡ç‰¹æ®Šå¤„ç†**:
  æ¯ä¸ªæ±‰å­—é€šå¸¸å¯¹åº”1ä¸ªtokenï¼Œæ¯”è‹±æ–‡å•è¯æ›´é«˜æ•ˆ
"""
        
        return report

# å®žé™…è¿è¡Œåˆ†æž
if __name__ == "__main__":
    analyzer = MultilingualTokenAnalyzer()
    
    # è¿è¡Œåˆ†æž
    df = analyzer.analyze_efficiency()
    print("=== å¤šè¯­è¨€Tokenæ•ˆçŽ‡åˆ†æž ===")
    print(df[['language', 'char_count', 'token_count', 'chars_per_token']])
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report()
    print(report)
    
    # å¯è§†åŒ–
    analyzer.visualize_efficiency(df)
    
    # ä¿å­˜ç»“æžœ
    df.to_csv('multilingual_token_analysis.csv', index=False)
    with open('token_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)