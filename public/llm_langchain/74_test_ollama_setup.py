#!/usr/bin/env python3
"""
æµ‹è¯•Ollamaè®¾ç½®å’Œæ¨¡å‹å¯ç”¨æ€§çš„è„šæœ¬
"""

import requests
import subprocess
import time
import os

def check_ollama_service():
    """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get("http://localhost:11434", timeout=5)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

def check_model_available(model_name="gpt-3.5-turbo:latest"):
    """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        response = requests.post(
            "http://localhost:11434/api/show",
            json={"name": model_name},
            timeout=10
        )
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

def pull_model(model_name="gpt-3.5-turbo:latest"):
    """æ‹‰å–æ¨¡å‹"""
    print(f"æ­£åœ¨æ‹‰å–æ¨¡å‹ {model_name}...")
    try:
        result = subprocess.run(
            ["ollama", "pull", model_name],
            capture_output=True,
            text=True,
            timeout=300
        )
        if result.returncode == 0:
            print("âœ… æ¨¡å‹æ‹‰å–æˆåŠŸ")
            return True
        else:
            print(f"âŒ æ¨¡å‹æ‹‰å–å¤±è´¥: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â° æ¨¡å‹æ‹‰å–è¶…æ—¶")
        return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°ollamaå‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£…Ollama")
        return False

def start_ollama_service():
    """å¯åŠ¨OllamaæœåŠ¡"""
    print("æ­£åœ¨å¯åŠ¨OllamaæœåŠ¡...")
    try:
        # æ£€æŸ¥ollamaå‘½ä»¤æ˜¯å¦å­˜åœ¨
        subprocess.run(["ollama", "--version"], check=True, capture_output=True)
        
        # åœ¨åå°å¯åŠ¨æœåŠ¡
        subprocess.Popen(["ollama", "serve"], 
                        stdout=subprocess.DEVNULL, 
                        stderr=subprocess.DEVNULL)
        time.sleep(3)  # ç­‰å¾…æœåŠ¡å¯åŠ¨
        return check_ollama_service()
    except (FileNotFoundError, subprocess.CalledProcessError):
        print("âŒ æœªæ‰¾åˆ°ollamaå‘½ä»¤æˆ–å¯åŠ¨å¤±è´¥")
        return False

def main():
    """ä¸»å‡½æ•°ï¼šæ£€æŸ¥å¹¶è®¾ç½®Ollamaç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥Ollamaç¯å¢ƒ...")
    
    # æ£€æŸ¥æœåŠ¡
    if not check_ollama_service():
        print("ğŸ”„ OllamaæœåŠ¡æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨...")
        if not start_ollama_service():
            print("âŒ æ— æ³•å¯åŠ¨OllamaæœåŠ¡")
            print("è¯·æ‰‹åŠ¨è¿è¡Œ: ollama serve")
            return False
    else:
        print("âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
    
    # æ£€æŸ¥æ¨¡å‹
    model_name = "gpt-3.5-turbo:latest"
    if not check_model_available(model_name):
        print(f"ğŸ”„ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œæ­£åœ¨æ‹‰å–...")
        if not pull_model(model_name):
            print("âŒ æ¨¡å‹æ‹‰å–å¤±è´¥")
            return False
    else:
        print(f"âœ… æ¨¡å‹ {model_name} å¯ç”¨")
    
    print("\nğŸ‰ Ollamaç¯å¢ƒæ£€æŸ¥å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥è¿è¡Œ sequential_chain_test.py äº†")
    return True

if __name__ == "__main__":
    main()