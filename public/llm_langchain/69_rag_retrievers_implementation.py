#!/usr/bin/env python3
"""
LangChain RAGæ£€ç´¢å™¨ç”Ÿäº§çº§å®ç°
åŒ…å«å¤šç§æ£€ç´¢å™¨ç±»å‹ã€ä¼˜åŒ–ç­–ç•¥å’Œå®é™…åº”ç”¨ç¤ºä¾‹
"""

import os
import json
import time
import hashlib
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ¨¡æ‹ŸLangChainç»„ä»¶
class MockDocument:
    """æ¨¡æ‹ŸDocumentç±»"""
    def __init__(self, page_content: str, metadata: Optional[Dict] = None):
        self.page_content = page_content
        self.metadata = metadata or {}
    
    def __repr__(self):
        return f"MockDocument(content='{self.page_content[:50]}...', metadata={self.metadata})"

class MockEmbeddings:
    """æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹"""
    def __init__(self, model: str = "mock-embedding"):
        self.model = model
    
    def embed_query(self, text: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # ä½¿ç”¨ç®€å•çš„å“ˆå¸Œç”Ÿæˆæ¨¡æ‹Ÿå‘é‡
        hash_obj = hashlib.md5(text.encode())
        vector = [int(b) / 255.0 for b in hash_obj.digest()]
        return vector
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æ¡£å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return [self.embed_query(text) for text in texts]

class MockVectorStore:
    """æ¨¡æ‹Ÿå‘é‡å­˜å‚¨"""
    def __init__(self):
        self.documents = []
        self.vectors = []
        self.metadata = []
    
    def add_documents(self, documents: List[MockDocument]) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        ids = []
        for doc in documents:
            doc_id = hashlib.md5(doc.page_content.encode()).hexdigest()
            vector = MockEmbeddings().embed_query(doc.page_content)
            
            self.documents.append(doc)
            self.vectors.append(vector)
            self.metadata.append(doc.metadata)
            ids.append(doc_id)
        return ids
    
    def similarity_search_with_score(
        self, 
        query: str, 
        k: int = 4,
        **kwargs
    ) -> List[Tuple[MockDocument, float]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        query_vector = MockEmbeddings().embed_query(query)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores = []
        for i, doc_vector in enumerate(self.vectors):
            similarity = np.dot(query_vector, doc_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
            )
            scores.append((i, similarity))
        
        # æ’åºå¹¶è¿”å›å‰kä¸ª
        scores.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for idx, score in scores[:k]:
            results.append((self.documents[idx], score))
        
        return results
    
    def similarity_search(self, query: str, k: int = 4) -> List[MockDocument]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆæ— åˆ†æ•°ï¼‰"""
        results = self.similarity_search_with_score(query, k)
        return [doc for doc, _ in results]
    
    def count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        return len(self.documents)

@dataclass
class RetrievalConfig:
    """æ£€ç´¢å™¨é…ç½®"""
    top_k: int = 5
    score_threshold: float = 0.7
    chunk_size: int = 1000
    chunk_overlap: int = 200
    cache_ttl: int = 3600
    enable_compression: bool = True
    enable_reranking: bool = True
    batch_size: int = 100

class BaseRetriever:
    """åŸºç¡€æ£€ç´¢å™¨ç±»"""
    
    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """è·å–ç›¸å…³æ–‡æ¡£"""
        raise NotImplementedError
    
    def log_retrieval(self, query: str, results: List[MockDocument], duration: float):
        """è®°å½•æ£€ç´¢æ—¥å¿—"""
        self.logger.info(
            f"Query: {query[:50]}... | Results: {len(results)} | Duration: {duration:.3f}s"
        )

class VectorStoreRetriever(BaseRetriever):
    """å‘é‡å­˜å‚¨æ£€ç´¢å™¨"""
    
    def __init__(self, vectorstore: MockVectorStore, config: RetrievalConfig):
        super().__init__(config)
        self.vectorstore = vectorstore
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """ä»å‘é‡å­˜å‚¨æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        start_time = time.time()
        
        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        results = self.vectorstore.similarity_search(
            query=query,
            k=self.config.top_k
        )
        
        duration = time.time() - start_time
        self.log_retrieval(query, results, duration)
        
        return results

class BM25Retriever(BaseRetriever):
    """BM25ç¨€ç–æ£€ç´¢å™¨"""
    
    def __init__(self, documents: List[MockDocument], config: RetrievalConfig):
        super().__init__(config)
        self.documents = documents
        self.inverted_index = self._build_inverted_index()
    
    def _build_inverted_index(self) -> Dict[str, List[int]]:
        """æ„å»ºå€’æ’ç´¢å¼•"""
        inverted_index = {}
        
        for idx, doc in enumerate(self.documents):
            words = doc.page_content.lower().split()
            for word in set(words):
                if word not in inverted_index:
                    inverted_index[word] = []
                inverted_index[word].append(idx)
        
        return inverted_index
    
    def _calculate_bm25_score(self, query: str, doc_idx: int) -> float:
        """è®¡ç®—BM25åˆ†æ•°"""
        query_terms = query.lower().split()
        doc = self.documents[doc_idx]
        doc_terms = doc.page_content.lower().split()
        
        score = 0.0
        doc_length = len(doc_terms)
        avg_doc_length = sum(len(d.page_content.split()) for d in self.documents) / len(self.documents)
        
        k1 = 1.5
        b = 0.75
        
        for term in query_terms:
            if term in doc_terms:
                tf = doc_terms.count(term)
                df = len(self.inverted_index.get(term, []))
                idf = np.log((len(self.documents) - df + 0.5) / (df + 0.5))
                
                numerator = tf * (k1 + 1)
                denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)
                score += idf * (numerator / denominator)
        
        return score
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """BM25æ£€ç´¢"""
        start_time = time.time()
        
        # è·å–å€™é€‰æ–‡æ¡£
        candidate_indices = set()
        query_terms = query.lower().split()
        
        for term in query_terms:
            if term in self.inverted_index:
                candidate_indices.update(self.inverted_index[term])
        
        # è®¡ç®—BM25åˆ†æ•°
        scores = []
        for idx in candidate_indices:
            score = self._calculate_bm25_score(query, idx)
            scores.append((idx, score))
        
        # æ’åºå¹¶è¿”å›å‰kä¸ª
        scores.sort(key=lambda x: x[1], reverse=True)
        results = [self.documents[idx] for idx, _ in scores[:self.config.top_k]]
        
        duration = time.time() - start_time
        self.log_retrieval(query, results, duration)
        
        return results

class EnsembleRetriever(BaseRetriever):
    """é›†æˆæ£€ç´¢å™¨ï¼ˆæ··åˆæ£€ç´¢ï¼‰"""
    
    def __init__(self, retrievers: List[BaseRetriever], weights: List[float], config: RetrievalConfig):
        super().__init__(config)
        self.retrievers = retrievers
        self.weights = weights
    
    def _reciprocal_rank_fusion(self, results_list: List[List[MockDocument]], k: int = 60) -> List[MockDocument]:
        """å€’æ•°æ’åèåˆ"""
        scores = {}
        
        for retriever_idx, results in enumerate(results_list):
            weight = self.weights[retriever_idx]
            
            for rank, doc in enumerate(results, 1):
                doc_key = doc.page_content
                if doc_key not in scores:
                    scores[doc_key] = {"doc": doc, "score": 0.0}
                
                # å€’æ•°æ’åèåˆå…¬å¼
                rrf_score = weight / (k + rank)
                scores[doc_key]["score"] += rrf_score
        
        # æ’åºå¹¶è¿”å›
        sorted_results = sorted(scores.values(), key=lambda x: x["score"], reverse=True)
        return [item["doc"] for item in sorted_results[:self.config.top_k]]
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """é›†æˆæ£€ç´¢"""
        start_time = time.time()
        
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ£€ç´¢å™¨
        all_results = []
        for retriever in self.retrievers:
            results = retriever.get_relevant_documents(query)
            all_results.append(results)
        
        # èåˆç»“æœ
        final_results = self._reciprocal_rank_fusion(all_results)
        
        duration = time.time() - start_time
        self.log_retrieval(query, final_results, duration)
        
        return final_results

class ContextualCompressionRetriever(BaseRetriever):
    """ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨"""
    
    def __init__(self, base_retriever: BaseRetriever, config: RetrievalConfig):
        super().__init__(config)
        self.base_retriever = base_retriever
    
    def _compress_document(self, doc: MockDocument, query: str) -> MockDocument:
        """å‹ç¼©æ–‡æ¡£å†…å®¹"""
        content = doc.page_content
        
        # ç®€å•çš„å‹ç¼©é€»è¾‘ï¼šæå–åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„å¥å­
        sentences = content.replace('ã€‚', '.').split('.')
        query_terms = query.lower().split()
        
        relevant_sentences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(term in sentence_lower for term in query_terms):
                relevant_sentences.append(sentence.strip())
        
        compressed_content = '. '.join(relevant_sentences[:3])  # é™åˆ¶å¥å­æ•°é‡
        
        return MockDocument(
            page_content=compressed_content,
            metadata={**doc.metadata, "compressed": True}
        )
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """å‹ç¼©æ£€ç´¢"""
        start_time = time.time()
        
        # åŸºç¡€æ£€ç´¢
        base_results = self.base_retriever.get_relevant_documents(query)
        
        # å‹ç¼©ç»“æœ
        compressed_results = [
            self._compress_document(doc, query) 
            for doc in base_results
        ]
        
        duration = time.time() - start_time
        self.log_retrieval(query, compressed_results, duration)
        
        return compressed_results

class MultiQueryRetriever(BaseRetriever):
    """å¤šæŸ¥è¯¢æ‰©å±•æ£€ç´¢å™¨"""
    
    def __init__(self, base_retriever: BaseRetriever, config: RetrievalConfig):
        super().__init__(config)
        self.base_retriever = base_retriever
    
    def _generate_variations(self, query: str) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢å˜ä½“"""
        variations = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
        
        # ç®€å•çš„æŸ¥è¯¢æ‰©å±•
        keywords = query.split()
        
        # æ·»åŠ åŒä¹‰è¯
        synonym_map = {
            "æœºå™¨å­¦ä¹ ": ["ML", "äººå·¥æ™ºèƒ½"],
            "æ·±åº¦å­¦ä¹ ": ["DL", "ç¥ç»ç½‘ç»œ"],
            "æ¡†æ¶": ["å·¥å…·", "åº“", "å¹³å°"]
        }
        
        for original, synonyms in synonym_map.items():
            if original in query:
                for synonym in synonyms:
                    variations.append(query.replace(original, synonym))
        
        return variations[:3]  # é™åˆ¶å˜ä½“æ•°é‡
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """å¤šæŸ¥è¯¢æ£€ç´¢"""
        start_time = time.time()
        
        # ç”ŸæˆæŸ¥è¯¢å˜ä½“
        variations = self._generate_variations(query)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        all_results = []
        for variation in variations:
            results = self.base_retriever.get_relevant_documents(variation)
            all_results.extend(results)
        
        # å»é‡
        seen = set()
        unique_results = []
        for doc in all_results:
            if doc.page_content not in seen:
                seen.add(doc.page_content)
                unique_results.append(doc)
        
        duration = time.time() - start_time
        self.log_retrieval(query, unique_results, duration)
        
        return unique_results[:self.config.top_k]

class CachedRetriever(BaseRetriever):
    """ç¼“å­˜æ£€ç´¢å™¨åŒ…è£…å™¨"""
    
    def __init__(self, base_retriever: BaseRetriever, cache_ttl: int = 3600):
        super().__init__(RetrievalConfig())
        self.base_retriever = base_retriever
        self.cache = {}
        self.cache_ttl = cache_ttl
        self.stats = {"hits": 0, "misses": 0}
    
    def _get_cache_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(query.encode()).hexdigest()
    
    def _is_cache_valid(self, timestamp: float) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        return time.time() - timestamp < self.cache_ttl
    
    def get_relevant_documents(self, query: str) -> List[MockDocument]:
        """å¸¦ç¼“å­˜çš„æ£€ç´¢"""
        cache_key = self._get_cache_key(query)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            cached_result, timestamp = self.cache[cache_key]
            if self._is_cache_valid(timestamp):
                self.stats["hits"] += 1
                return cached_result
        
        # æ‰§è¡Œæ£€ç´¢
        results = self.base_retriever.get_relevant_documents(query)
        
        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = (results, time.time())
        self.stats["misses"] += 1
        
        return results
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return self.stats.copy()

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "total_queries": 0,
            "total_time": 0.0,
            "error_count": 0,
            "query_times": []
        }
    
    def record_query(self, duration: float, error: bool = False):
        """è®°å½•æŸ¥è¯¢æŒ‡æ ‡"""
        self.metrics["total_queries"] += 1
        self.metrics["total_time"] += duration
        self.metrics["query_times"].append(duration)
        
        if error:
            self.metrics["error_count"] += 1
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.metrics["total_queries"] == 0:
            return {"message": "No queries recorded"}
        
        times = self.metrics["query_times"]
        return {
            "total_queries": self.metrics["total_queries"],
            "total_time": self.metrics["total_time"],
            "average_time": self.metrics["total_time"] / self.metrics["total_queries"],
            "median_time": np.median(times),
            "p95_time": np.percentile(times, 95),
            "error_rate": self.metrics["error_count"] / self.metrics["total_queries"]
        }

class ProductionRAGSystem:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""
    
    def __init__(self, config: RetrievalConfig):
        self.config = config
        self.vectorstore = MockVectorStore()
        self.monitor = PerformanceMonitor()
        self.setup_retrievers()
    
    def setup_retrievers(self):
        """è®¾ç½®æ£€ç´¢å™¨é“¾"""
        # åŸºç¡€å‘é‡æ£€ç´¢å™¨
        self.vector_retriever = VectorStoreRetriever(self.vectorstore, self.config)
        
        # BM25æ£€ç´¢å™¨
        self.bm25_retriever = None  # å°†åœ¨æ·»åŠ æ–‡æ¡£ååˆå§‹åŒ–
        
        # é›†æˆæ£€ç´¢å™¨
        self.ensemble_retriever = None
        
        # å‹ç¼©æ£€ç´¢å™¨
        self.compression_retriever = ContextualCompressionRetriever(
            self.vector_retriever, self.config
        )
        
        # å¤šæŸ¥è¯¢æ£€ç´¢å™¨
        self.multi_query_retriever = MultiQueryRetriever(
            self.vector_retriever, self.config
        )
        
        # ç¼“å­˜åŒ…è£…å™¨
        self.cached_retriever = CachedRetriever(self.vector_retriever)
    
    def add_documents(self, documents: List[MockDocument]) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°ç³»ç»Ÿ"""
        print(f"æ­£åœ¨ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vectorstore.add_documents(documents)
        
        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever(documents, self.config)
        
        # è®¾ç½®é›†æˆæ£€ç´¢å™¨
        self.ensemble_retriever = EnsembleRetriever(
            [self.vector_retriever, self.bm25_retriever],
            weights=[0.7, 0.3],
            config=self.config
        )
        
        print("æ–‡æ¡£ç´¢å¼•å®Œæˆ")
    
    def search(self, query: str, retriever_type: str = "vector") -> List[MockDocument]:
        """æ‰§è¡Œæœç´¢"""
        start_time = time.time()
        
        try:
            # é€‰æ‹©æ£€ç´¢å™¨
            retriever_map = {
                "vector": self.vector_retriever,
                "bm25": self.bm25_retriever,
                "ensemble": self.ensemble_retriever,
                "compression": self.compression_retriever,
                "multi": self.multi_query_retriever,
                "cached": self.cached_retriever
            }
            
            retriever = retriever_map.get(retriever_type, self.vector_retriever)
            
            if retriever is None:
                raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢å™¨ç±»å‹: {retriever_type}")
            
            # æ‰§è¡Œæ£€ç´¢
            results = retriever.get_relevant_documents(query)
            
            duration = time.time() - start_time
            self.monitor.record_query(duration)
            
            return results
            
        except Exception as e:
            duration = time.time() - start_time
            self.monitor.record_query(duration, error=True)
            raise e
    
    async def search_async(self, query: str, retriever_type: str = "vector") -> List[MockDocument]:
        """å¼‚æ­¥æœç´¢"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search, query, retriever_type)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "document_count": self.vectorstore.count(),
            "performance_stats": self.monitor.get_performance_stats(),
            "cache_stats": self.cached_retriever.get_cache_stats() if hasattr(self, 'cached_retriever') else None,
            "config": asdict(self.config)
        }

# ç¤ºä¾‹æ–‡æ¡£
SAMPLE_DOCUMENTS = [
    MockDocument(
        page_content="LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†æ ‡å‡†åŒ–æ¥å£å’Œä¸°å¯Œçš„å·¥å…·é“¾ã€‚",
        metadata={"source": "tech_doc", "category": "framework", "date": "2024-01-15"}
    ),
    MockDocument(
        page_content="RAGï¼ˆRetrieval-Augmented Generationï¼‰é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå‡å°‘å¹»è§‰é—®é¢˜ã€‚",
        metadata={"source": "tech_doc", "category": "rag", "date": "2024-01-16"}
    ),
    MockDocument(
        page_content="å‘é‡æ•°æ®åº“å¦‚Chromaã€Pineconeå’ŒWeaviateä¸ºRAGç³»ç»Ÿæä¾›äº†é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢èƒ½åŠ›ã€‚",
        metadata={"source": "tech_doc", "category": "vector_db", "date": "2024-01-17"}
    ),
    MockDocument(
        page_content="åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œä½¿å¾—è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè®¡ç®—æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚",
        metadata={"source": "tech_doc", "category": "embeddings", "date": "2024-01-18"}
    ),
    MockDocument(
        page_content="BM25æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡æ’åºå‡½æ•°çš„æ£€ç´¢ç®—æ³•ï¼Œå¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“å’Œæ–‡æ¡£æ£€ç´¢ç³»ç»Ÿã€‚",
        metadata={"source": "tech_doc", "category": "retrieval", "date": "2024-01-19"}
    )
]

def demonstrate_retrievers():
    """æ¼”ç¤ºå„ç§æ£€ç´¢å™¨åŠŸèƒ½"""
    print("ğŸš€ LangChain RAGæ£€ç´¢å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    config = RetrievalConfig(top_k=3)
    rag_system = ProductionRAGSystem(config)
    
    # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
    rag_system.add_documents(SAMPLE_DOCUMENTS)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯LangChainï¼Ÿ",
        "RAGç³»ç»Ÿå¦‚ä½•å·¥ä½œï¼Ÿ",
        "å‘é‡æ•°æ®åº“æœ‰å“ªäº›ï¼Ÿ",
        "åµŒå…¥æ¨¡å‹çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
        "BM25ç®—æ³•çš„åŸç†"
    ]
    
    retriever_types = ["vector", "bm25", "compression", "multi", "cached"]
    
    for query in test_queries:
        print(f"\nğŸ“‹ æŸ¥è¯¢: {query}")
        print("-" * 30)
        
        for retriever_type in retriever_types:
            try:
                results = rag_system.search(query, retriever_type)
                print(f"  {retriever_type.upper()}: {len(results)}ä¸ªç»“æœ")
                for i, doc in enumerate(results[:2], 1):
                    print(f"    {i}. {doc.page_content[:60]}...")
            except Exception as e:
                print(f"  {retriever_type.upper()}: é”™è¯¯ - {str(e)}")
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡")
    print("-" * 30)
    stats = rag_system.get_system_stats()
    print(f"æ–‡æ¡£æ€»æ•°: {stats['document_count']}")
    print(f"æ€§èƒ½ç»Ÿè®¡: {stats['performance_stats']}")
    print(f"ç¼“å­˜ç»Ÿè®¡: {stats['cache_stats']}")

def benchmark_retrievers():
    """æ£€ç´¢å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    config = RetrievalConfig(top_k=5)
    rag_system = ProductionRAGSystem(config)
    rag_system.add_documents(SAMPLE_DOCUMENTS)
    
    test_query = "RAGç³»ç»Ÿçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
    iterations = 100
    
    retriever_types = ["vector", "bm25", "compression", "multi", "cached"]
    
    for retriever_type in retriever_types:
        times = []
        
        for _ in range(iterations):
            start = time.time()
            rag_system.search(test_query, retriever_type)
            times.append(time.time() - start)
        
        avg_time = np.mean(times)
        p95_time = np.percentile(times, 95)
        
        print(f"{retriever_type.upper()}:")
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.4f}s")
        print(f"  P95æ—¶é—´: {p95_time:.4f}s")

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    print("ä½¿ç”¨æ¨¡æ‹Ÿå®ç°ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–")
    print("=" * 50)
    
    # è¿è¡Œæ¼”ç¤º
    demonstrate_retrievers()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_retrievers()
    
    # ä¿å­˜ç»“æœ
    with open("rag_retrievers_demo_results.json", "w", encoding="utf-8") as f:
        config = RetrievalConfig()
        rag_system = ProductionRAGSystem(config)
        rag_system.add_documents(SAMPLE_DOCUMENTS)
        
        test_queries = [
            "RAGç³»ç»Ÿçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•æé«˜æ£€ç´¢å‡†ç¡®æ€§ï¼Ÿ",
            "å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿæ•°æ®åº“çš„åŒºåˆ«ï¼Ÿ",
            "LangChainä¸­çš„æ£€ç´¢å™¨ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
            "ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        results = {
            "system_stats": rag_system.get_system_stats(),
            "sample_queries": test_queries,
            "retriever_types": ["vector", "bm25", "compression", "multi", "cached"],
            "config": asdict(config)
        }
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° rag_retrievers_demo_results.json")