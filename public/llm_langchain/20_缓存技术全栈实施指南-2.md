


          
ä½œä¸ºAIä¸LangChainèµ„æ·±ä¸“å®¶ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†é˜è¿°åœ¨LangChainç”Ÿæ€ä¸­çš„ç¼“å­˜æŠ€æœ¯å®æ–½ã€‚è¿™ä¸æ˜¯æ™®é€šçš„ç¼“å­˜ï¼Œè€Œæ˜¯ä¸“ä¸ºLLMåº”ç”¨è®¾è®¡çš„æ™ºèƒ½ç¼“å­˜ä½“ç³»ã€‚

## LangChainç¼“å­˜æŠ€æœ¯å…¨æ ˆå®æ–½æ–¹æ¡ˆ

### ğŸ“‹ ç›®å½•
1. [LangChainç¼“å­˜æ¶æ„è®¾è®¡](#langchainç¼“å­˜æ¶æ„è®¾è®¡)
2. [å†…å­˜ç¼“å­˜ - é«˜é€ŸL1ç¼“å­˜](#å†…å­˜ç¼“å­˜-é«˜é€Ÿl1ç¼“å­˜)
3. [SQLiteç¼“å­˜ - æœ¬åœ°æŒä¹…åŒ–](#sqliteç¼“å­˜-æœ¬åœ°æŒä¹…åŒ–)
4. [MySQLç¼“å­˜ - å…³ç³»å‹æ•°æ®åº“ç¼“å­˜](#mysqlç¼“å­˜-å…³ç³»å‹æ•°æ®åº“ç¼“å­˜)
5. [Redisç¼“å­˜ - åˆ†å¸ƒå¼ç¼“å­˜](#redisç¼“å­˜-åˆ†å¸ƒå¼ç¼“å­˜)
6. [MongoDBç¼“å­˜ - æ–‡æ¡£å­˜å‚¨](#mongodbç¼“å­˜-æ–‡æ¡£å­˜å‚¨)
7. [è¯­ä¹‰åŒ–ç¼“å­˜ - æ™ºèƒ½ç¼“å­˜](#è¯­ä¹‰åŒ–ç¼“å­˜-æ™ºèƒ½ç¼“å­˜)
8. [GPTCache - ç¬¬ä¸‰æ–¹LLMç¼“å­˜](#gptcache-ç¬¬ä¸‰æ–¹llmç¼“å­˜)
9. [ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨](#ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨)

---

## LangChainç¼“å­˜æ¶æ„è®¾è®¡

### ğŸ¯ ä¸ƒå±‚ç¼“å­˜æ¶æ„

```mermaid
graph TD
    A[ç”¨æˆ·è¯·æ±‚] --> B[å†…å­˜ç¼“å­˜ L1]
    B -->|å‘½ä¸­| C[è¿”å›ç»“æœ]
    B -->|æœªå‘½ä¸­| D[SQLite L2]
    D -->|å‘½ä¸­| E[å†™å…¥L1]
    D -->|æœªå‘½ä¸­| F[MySQL L3]
    F -->|å‘½ä¸­| G[å†™å…¥L2/L1]
    F -->|æœªå‘½ä¸­| H[Redis L4]
    H -->|å‘½ä¸­| I[å†™å…¥L3/L2/L1]
    H -->|æœªå‘½ä¸­| J[MongoDB L5]
    J -->|å‘½ä¸­| K[å†™å…¥L4/L3/L2/L1]
    J -->|æœªå‘½ä¸­| L[è¯­ä¹‰ç¼“å­˜ L6]
    L -->|å‘½ä¸­| M[å†™å…¥L5-L1]
    L -->|æœªå‘½ä¸­| N[GPTCache L7]
    N -->|å‘½ä¸­| O[å†™å…¥L6-L1]
    N -->|æœªå‘½ä¸­| P[è°ƒç”¨LLM API]
    P --> Q[å†™å…¥L7-L1]
```

### ğŸ”„ ç¼“å­˜ç­–ç•¥çŸ©é˜µ

| ç¼“å­˜ç±»å‹ | å»¶è¿Ÿ | å®¹é‡ | å¹¶å‘ | æŒä¹…åŒ– | LangChainé€‚é… |
|---------|------|------|------|--------|---------------|
| å†…å­˜ç¼“å­˜ | <1ms | æœ‰é™ | æé«˜ | å¦ | âœ… ç›´æ¥é›†æˆ |
| SQLite | 1-5ms | ä¸­ç­‰ | é«˜ | æ˜¯ | âœ… æœ¬åœ°å¼€å‘ |
| MySQL | 5-15ms | å¤§ | é«˜ | æ˜¯ | âœ… ä¼ä¸šçº§ |
| Redis | 1-5ms | å¤§ | æé«˜ | å¯é€‰ | âœ… åˆ†å¸ƒå¼ |
| MongoDB | 5-15ms | æå¤§ | é«˜ | æ˜¯ | âœ… æ–‡æ¡£å­˜å‚¨ |
| è¯­ä¹‰ç¼“å­˜ | 10-50ms | ä¸­ç­‰ | ä¸­ | å¯é€‰ | âœ… æ™ºèƒ½åŒ¹é… |
| GPTCache | 5-20ms | å¤§ | é«˜ | å¯é€‰ | âœ… LLMä¸“ç”¨ |

---

## å†…å­˜ç¼“å­˜ - é«˜é€ŸL1ç¼“å­˜

### ğŸš€ å®ç°æ–¹æ¡ˆ

```python
from langchain.cache import InMemoryCache
from typing import Optional, Dict, Any
import threading
import time
from dataclasses import dataclass

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    value: Any
    timestamp: float
    ttl: Optional[float] = None
    access_count: int = 0

class LangChainMemoryCache:
    """LangChainä¸“ç”¨å†…å­˜ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self._cache: Dict[str, CacheEntry] = {}
        self._lock = threading.RLock()
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """LangChainç¼“å­˜æŸ¥æ‰¾æ¥å£"""
        key = f"{prompt}:{llm_string}"
        with self._lock:
            if key in self._cache:
                entry = self._cache[key]
                # æ£€æŸ¥TTL
                if entry.ttl and time.time() > entry.timestamp + entry.ttl:
                    del self._cache[key]
                    return None
                entry.access_count += 1
                return entry.value
        return None
    
    def update(self, prompt: str, llm_string: str, return_val: str) -> None:
        """LangChainç¼“å­˜æ›´æ–°æ¥å£"""
        key = f"{prompt}:{llm_string}"
        with self._lock:
            # LRUæ·˜æ±°ç­–ç•¥
            if len(self._cache) >= self.max_size:
                # ç§»é™¤æœ€å°‘è®¿é—®çš„æ¡ç›®
                lru_key = min(self._cache.keys(), 
                            key=lambda k: self._cache[k].access_count)
                del self._cache[lru_key]
                
            self._cache[key] = CacheEntry(
                value=return_val,
                timestamp=time.time(),
                ttl=self.default_ttl
            )
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()

# LangChainé›†æˆç¤ºä¾‹
from langchain.llms import OpenAI
from langchain.cache import SQLiteCache

# ä½¿ç”¨è‡ªå®šä¹‰å†…å­˜ç¼“å­˜
memory_cache = LangChainMemoryCache(max_size=1000)
llm = OpenAI(cache=memory_cache)
```

### ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

```python
class AdvancedMemoryCache(LangChainMemoryCache):
    """é«˜çº§å†…å­˜ç¼“å­˜ - æ”¯æŒTTLåˆ†å±‚"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ttl_tiers = {
            'short': 300,    # 5åˆ†é’Ÿ
            'medium': 3600,  # 1å°æ—¶
            'long': 86400    # 1å¤©
        }
        
    def set_ttl_tier(self, prompt: str, tier: str):
        """è®¾ç½®TTLå±‚çº§"""
        key = f"{prompt}:ttl_tier"
        with self._lock:
            if key in self._cache:
                self._cache[key].ttl = self.ttl_tiers.get(tier, 3600)
```

---

## SQLiteç¼“å­˜ - æœ¬åœ°æŒä¹…åŒ–

### ğŸ—„ï¸ æ•°æ®åº“è®¾è®¡

```sql
CREATE TABLE langchain_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    prompt_hash VARCHAR(64) UNIQUE NOT NULL,
    prompt TEXT NOT NULL,
    llm_string VARCHAR(255) NOT NULL,
    response TEXT NOT NULL,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    access_count INTEGER DEFAULT 1,
    ttl INTEGER,
    tags TEXT
);

CREATE INDEX idx_prompt_hash ON langchain_cache(prompt_hash);
CREATE INDEX idx_created_at ON langchain_cache(created_at);
```

### ğŸ’» Pythonå®ç°

```python
import sqlite3
import json
import hashlib
from typing import Optional, Dict, Any
from datetime import datetime

class LangChainSQLiteCache:
    """LangChain SQLiteç¼“å­˜å®ç°"""
    
    def __init__(self, db_path: str = "langchain_cache.db"):
        self.db_path = db_path
        self._init_db()
        
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS langchain_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt_hash VARCHAR(64) UNIQUE NOT NULL,
                prompt TEXT NOT NULL,
                llm_string VARCHAR(255) NOT NULL,
                response TEXT NOT NULL,
                metadata JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                access_count INTEGER DEFAULT 1,
                ttl INTEGER,
                tags TEXT
            )
        """)
        conn.execute("CREATE INDEX IF NOT EXISTS idx_prompt_hash ON langchain_cache(prompt_hash)")
        conn.commit()
        conn.close()
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT response FROM langchain_cache 
            WHERE prompt_hash = ? AND llm_string = ?
            AND (ttl IS NULL OR datetime('now') < datetime(created_at, '+' || ttl || ' seconds'))
        """, (prompt_hash, llm_string))
        
        result = cursor.fetchone()
        if result:
            # æ›´æ–°è®¿é—®è®¡æ•°
            cursor.execute("""
                UPDATE langchain_cache 
                SET access_count = access_count + 1, updated_at = CURRENT_TIMESTAMP
                WHERE prompt_hash = ? AND llm_string = ?
            """, (prompt_hash, llm_string))
            conn.commit()
            conn.close()
            return result[0]
        
        conn.close()
        return None
    
    def update(self, prompt: str, llm_string: str, return_val: str, 
               metadata: Optional[Dict] = None, ttl: Optional[int] = None,
               tags: Optional[list] = None) -> None:
        """æ›´æ–°ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        metadata_json = json.dumps(metadata) if metadata else None
        tags_str = json.dumps(tags) if tags else None
        
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            INSERT OR REPLACE INTO langchain_cache 
            (prompt_hash, prompt, llm_string, response, metadata, ttl, tags)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (prompt_hash, prompt, llm_string, return_val, metadata_json, ttl, tags_str))
        conn.commit()
        conn.close()

# LangChainé›†æˆ
from langchain.cache import SQLiteCache

class LangChainSQLiteAdapter(SQLiteCache):
    """LangChain SQLiteé€‚é…å™¨"""
    
    def __init__(self, db_path: str = "langchain_cache.db"):
        super().__init__(database_path=db_path)
```

---

## MySQLç¼“å­˜ - å…³ç³»å‹æ•°æ®åº“ç¼“å­˜

### ğŸ—ï¸ æ•°æ®åº“æ¶æ„è®¾è®¡

```sql
-- ä¸»ç¼“å­˜è¡¨
CREATE TABLE langchain_cache (
    id BIGINT PRIMARY KEY AUTOINCREMENT,
    prompt_hash CHAR(64) UNIQUE NOT NULL,
    prompt TEXT NOT NULL,
    llm_string VARCHAR(255) NOT NULL,
    response LONGTEXT NOT NULL,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    access_count BIGINT DEFAULT 1,
    ttl INT,
    cache_version INT DEFAULT 1,
    INDEX idx_prompt_hash (prompt_hash),
    INDEX idx_created_at (created_at),
    INDEX idx_llm_string (llm_string),
    FULLTEXT idx_prompt (prompt)
);

-- ç¼“å­˜ç»Ÿè®¡è¡¨
CREATE TABLE cache_stats (
    id BIGINT PRIMARY KEY AUTOINCREMENT,
    cache_date DATE UNIQUE,
    hit_count BIGINT DEFAULT 0,
    miss_count BIGINT DEFAULT 0,
    total_requests BIGINT DEFAULT 0,
    avg_response_time DECIMAL(10,3),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç¼“å­˜æ ‡ç­¾è¡¨
CREATE TABLE cache_tags (
    id BIGINT PRIMARY KEY AUTOINCREMENT,
    cache_id BIGINT,
    tag_name VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (cache_id) REFERENCES langchain_cache(id),
    INDEX idx_tag_name (tag_name)
);
```

### ğŸ”§ Pythonå®ç°ï¼ˆæ”¯æŒè¿æ¥æ± ï¼‰

```python
import pymysql
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool
import json
import hashlib
from typing import Optional, Dict, Any, List

class LangChainMySQLCache:
    """LangChain MySQLç¼“å­˜å®ç°"""
    
    def __init__(self, 
                 host: str = "localhost",
                 port: int = 3306,
                 user: str = "root",
                 password: str = "password",
                 database: str = "langchain_cache",
                 pool_size: int = 10):
        
        self.engine = create_engine(
            f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}",
            poolclass=QueuePool,
            pool_size=pool_size,
            max_overflow=20,
            pool_timeout=30,
            pool_recycle=3600
        )
        
        self.Session = sessionmaker(bind=self.engine)
        self._init_tables()
        
    def _init_tables(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        with self.engine.connect() as conn:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS langchain_cache (
                    id BIGINT PRIMARY KEY AUTO_INCREMENT,
                    prompt_hash CHAR(64) UNIQUE NOT NULL,
                    prompt TEXT NOT NULL,
                    llm_string VARCHAR(255) NOT NULL,
                    response LONGTEXT NOT NULL,
                    metadata JSON,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    access_count BIGINT DEFAULT 1,
                    ttl INT,
                    cache_version INT DEFAULT 1,
                    INDEX idx_prompt_hash (prompt_hash),
                    INDEX idx_created_at (created_at),
                    INDEX idx_llm_string (llm_string),
                    FULLTEXT idx_prompt (prompt)
                )
            """))
            conn.commit()
            
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        
        with self.Session() as session:
            result = session.execute(
                text("""
                    SELECT response FROM langchain_cache 
                    WHERE prompt_hash = :prompt_hash AND llm_string = :llm_string
                    AND (ttl IS NULL OR NOW() < DATE_ADD(created_at, INTERVAL ttl SECOND))
                """),
                {"prompt_hash": prompt_hash, "llm_string": llm_string}
            ).fetchone()
            
            if result:
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                session.execute(
                    text("""
                        UPDATE langchain_cache 
                        SET access_count = access_count + 1 
                        WHERE prompt_hash = :prompt_hash AND llm_string = :llm_string
                    """),
                    {"prompt_hash": prompt_hash, "llm_string": llm_string}
                )
                session.commit()
                return result[0]
        return None
    
    def update(self, prompt: str, llm_string: str, return_val: str,
               metadata: Optional[Dict] = None, ttl: Optional[int] = None,
               tags: Optional[List[str]] = None) -> None:
        """æ›´æ–°ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        metadata_json = json.dumps(metadata) if metadata else None
        
        with self.Session() as session:
            session.execute(
                text("""
                    INSERT INTO langchain_cache 
                    (prompt_hash, prompt, llm_string, response, metadata, ttl)
                    VALUES (:prompt_hash, :prompt, :llm_string, :response, :metadata, :ttl)
                    ON DUPLICATE KEY UPDATE
                    response = VALUES(response),
                    metadata = VALUES(metadata),
                    ttl = VALUES(ttl),
                    cache_version = cache_version + 1
                """),
                {
                    "prompt_hash": prompt_hash,
                    "prompt": prompt,
                    "llm_string": llm_string,
                    "response": return_val,
                    "metadata": metadata_json,
                    "ttl": ttl
                }
            )
            session.commit()
            
            # å¤„ç†æ ‡ç­¾
            if tags:
                cache_id = session.execute(
                    text("SELECT id FROM langchain_cache WHERE prompt_hash = :prompt_hash"),
                    {"prompt_hash": prompt_hash}
                ).fetchone()[0]
                
                # åˆ é™¤æ—§æ ‡ç­¾
                session.execute(
                    text("DELETE FROM cache_tags WHERE cache_id = :cache_id"),
                    {"cache_id": cache_id}
                )
                
                # æ’å…¥æ–°æ ‡ç­¾
                for tag in tags:
                    session.execute(
                        text("INSERT INTO cache_tags (cache_id, tag_name) VALUES (:cache_id, :tag)"),
                        {"cache_id": cache_id, "tag": tag}
                    )
                session.commit()
```

---

## Redisç¼“å­˜ - åˆ†å¸ƒå¼ç¼“å­˜

### ğŸ”„ Redisé›†ç¾¤é…ç½®

```yaml
# redis-cluster.yml
version: '3.8'
services:
  redis-master:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    
  redis-slave-1:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    command: redis-server --slaveof redis-master 6379
    
  redis-slave-2:
    image: redis:7-alpine
    ports:
      - "6381:6379"
    command: redis-server --slaveof redis-master 6379
```

### ğŸ’» Pythonå®ç°ï¼ˆæ”¯æŒé›†ç¾¤ï¼‰

```python
import redis
import json
import hashlib
from typing import Optional, Dict, Any
from redis.sentinel import Sentinel
from redis.cluster import RedisCluster

class LangChainRedisCache:
    """LangChain Redisç¼“å­˜å®ç°"""
    
    def __init__(self, 
                 host: str = "localhost",
                 port: int = 6379,
                 db: int = 0,
                 password: Optional[str] = None,
                 cluster_mode: bool = False,
                 sentinel_hosts: Optional[list] = None):
        
        if cluster_mode:
            # Redisé›†ç¾¤æ¨¡å¼
            self.redis_client = RedisCluster(
                startup_nodes=[{"host": host, "port": port}],
                decode_responses=True
            )
        elif sentinel_hosts:
            # Rediså“¨å…µæ¨¡å¼
            sentinel = Sentinel(sentinel_hosts, socket_timeout=0.1)
            self.redis_client = sentinel.master_for('mymaster', socket_timeout=0.1)
        else:
            # å•èŠ‚ç‚¹æ¨¡å¼
            self.redis_client = redis.Redis(
                host=host, port=port, db=db, password=password,
                decode_responses=True
            )
            
        self.key_prefix = "langchain:cache:"
        
    def _generate_key(self, prompt: str, llm_string: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        return f"{self.key_prefix}{prompt_hash}:{llm_string}"
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        key = self._generate_key(prompt, llm_string)
        try:
            result = self.redis_client.get(key)
            if result:
                # æ›´æ–°è®¿é—®æ—¶é—´
                self.redis_client.expire(key, 3600)  # å»¶é•¿TTL
                return result
        except redis.RedisError as e:
            print(f"Redis lookup error: {e}")
        return None
    
    def update(self, prompt: str, llm_string: str, return_val: str,
               ttl: int = 3600, metadata: Optional[Dict] = None) -> None:
        """æ›´æ–°ç¼“å­˜"""
        key = self._generate_key(prompt, llm_string)
        try:
            cache_data = {
                "response": return_val,
                "metadata": json.dumps(metadata) if metadata else None,
                "timestamp": json.dumps(datetime.now().isoformat())
            }
            self.redis_client.setex(key, ttl, json.dumps(cache_data))
        except redis.RedisError as e:
            print(f"Redis update error: {e}")
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        try:
            keys = self.redis_client.keys(f"{self.key_prefix}*")
            if keys:
                self.redis_client.delete(*keys)
        except redis.RedisError as e:
            print(f"Redis clear error: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        try:
            keys = self.redis_client.keys(f"{self.key_prefix}*")
            return {
                "total_keys": len(keys),
                "memory_usage": self.redis_client.info("memory")["used_memory_human"]
            }
        except redis.RedisError as e:
            return {"error": str(e)}

# é«˜çº§åŠŸèƒ½ï¼šRedisç¼“å­˜é›†ç¾¤ç®¡ç†
class LangChainRedisCluster:
    """Redisé›†ç¾¤ç®¡ç†å™¨"""
    
    def __init__(self, cluster_nodes: list):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            decode_responses=True
        )
        self.sharding = {}
        
    def distribute_cache(self, prompt: str, llm_string: str) -> str:
        """æ ¹æ®å“ˆå¸Œåˆ†å¸ƒç¼“å­˜"""
        key = f"{prompt}:{llm_string}"
        slot = self.cluster.keyslot(key)
        node = self.cluster.get_node_from_slot(slot)
        return node.name
```

---

## MongoDBç¼“å­˜ - æ–‡æ¡£å­˜å‚¨

### ğŸ—‚ï¸ æ–‡æ¡£ç»“æ„è®¾è®¡

```javascript
// MongoDBç¼“å­˜æ–‡æ¡£ç»“æ„
{
  "_id": ObjectId("..."),
  "prompt_hash": "a1b2c3d4...",
  "prompt": "ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯",
  "llm_string": "openai-gpt-3.5-turbo",
  "response": {
    "content": "AIç”Ÿæˆçš„å“åº”å†…å®¹",
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 200,
      "total_tokens": 300
    },
    "model": "gpt-3.5-turbo",
    "created": ISODate("2024-01-01T00:00:00Z")
  },
  "metadata": {
    "temperature": 0.7,
    "max_tokens": 1000,
    "user_id": "user123",
    "session_id": "session456"
  },
  "tags": ["chat", "qa", "technical"],
  "cache_version": 1,
  "created_at": ISODate("2024-01-01T00:00:00Z"),
  "updated_at": ISODate("2024-01-01T00:00:00Z"),
  "access_count": 1,
  "ttl": 3600
}
```

### ğŸ’¾ Pythonå®ç°ï¼ˆæ”¯æŒGridFSï¼‰

```python
from pymongo import MongoClient, IndexModel, ASCENDING, TEXT
from gridfs import GridFS
import pymongo
import hashlib
import json
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

class LangChainMongoDBCache:
    """LangChain MongoDBç¼“å­˜å®ç°"""
    
    def __init__(self, 
                 connection_string: str = "mongodb://localhost:27017/",
                 database: str = "langchain_cache",
                 collection: str = "cache"):
        
        self.client = MongoClient(connection_string)
        self.db = self.client[database]
        self.collection = self.db[collection]
        self.fs = GridFS(self.db)  # å¤§æ–‡ä»¶å­˜å‚¨
        
        self._init_indexes()
        
    def _init_indexes(self):
        """åˆå§‹åŒ–ç´¢å¼•"""
        indexes = [
            IndexModel([("prompt_hash", ASCENDING)], unique=True),
            IndexModel([("llm_string", ASCENDING)]),
            IndexModel([("created_at", ASCENDING)]),
            IndexModel([("tags", ASCENDING)]),
            IndexModel([("prompt", TEXT)]),
            IndexModel([("updated_at", ASCENDING)], expireAfterSeconds=0)
        ]
        self.collection.create_indexes(indexes)
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        
        query = {
            "prompt_hash": prompt_hash,
            "llm_string": llm_string,
            "$or": [
                {"ttl": {"$exists": False}},
                {"ttl": None},
                {"updated_at": {"$gte": datetime.utcnow() - timedelta(seconds="$ttl")}}
            ]
        }
        
        result = self.collection.find_one_and_update(
            query,
            {"$inc": {"access_count": 1}, "$set": {"updated_at": datetime.utcnow()}},
            projection={"response.content": 1}
        )
        
        return result["response"]["content"] if result else None
    
    def update(self, prompt: str, llm_string: str, return_val: str,
               metadata: Optional[Dict] = None, ttl: Optional[int] = None,
               tags: Optional[List[str]] = None) -> None:
        """æ›´æ–°ç¼“å­˜"""
        prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()
        
        cache_doc = {
            "prompt_hash": prompt_hash,
            "prompt": prompt,
            "llm_string": llm_string,
            "response": {
                "content": return_val,
                "created": datetime.utcnow()
            },
            "metadata": metadata or {},
            "tags": tags or [],
            "cache_version": 1,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
            "access_count": 1,
            "ttl": ttl
        }
        
        # å¦‚æœå“åº”å¾ˆå¤§ï¼Œä½¿ç”¨GridFS
        if len(return_val) > 1024 * 1024:  # 1MB
            file_id = self.fs.put(
                return_val.encode('utf-8'),
                filename=f"{prompt_hash}.txt",
                metadata={"llm_string": llm_string}
            )
            cache_doc["response"]["gridfs_id"] = file_id
            cache_doc["response"]["content"] = None
        
        self.collection.replace_one(
            {"prompt_hash": prompt_hash, "llm_string": llm_string},
            cache_doc,
            upsert=True
        )
    
    def find_by_tags(self, tags: List[str], llm_string: Optional[str] = None) -> List[Dict]:
        """æŒ‰æ ‡ç­¾æŸ¥æ‰¾ç¼“å­˜"""
        query = {"tags": {"$in": tags}}
        if llm_string:
            query["llm_string"] = llm_string
            
        return list(self.collection.find(query).limit(100))
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        pipeline = [
            {"$group": {
                "_id": None,
                "total_documents": {"$sum": 1},
                "total_accesses": {"$sum": "$access_count"},
                "avg_response_size": {"$avg": {"$strLenCP": "$response.content"}}
            }}
        ]
        
        result = list(self.collection.aggregate(pipeline))
        return result[0] if result else {}
```

---

## è¯­ä¹‰åŒ–ç¼“å­˜ - æ™ºèƒ½ç¼“å­˜

### ğŸ§  è¯­ä¹‰ç›¸ä¼¼åº¦ç®—æ³•

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import faiss
from typing import List, Tuple, Optional

class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜å®ç°"""
    
    def __init__(self, 
                 model_name: str = "all-MiniLM-L6-v2",
                 similarity_threshold: float = 0.85,
                 index_type: str = "faiss"):
        
        self.model = SentenceTransformer(model_name)
        self.similarity_threshold = similarity_threshold
        self.index_type = index_type
        
        # åˆå§‹åŒ–å‘é‡ç´¢å¼•
        self.dimension = self.model.get_sentence_embedding_dimension()
        if index_type == "faiss":
            self.index = faiss.IndexFlatIP(self.dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
        else:
            self.embeddings = []
            self.prompts = []
            
        self.cache_map = {}  # å­˜å‚¨å®é™…ç¼“å­˜å†…å®¹
        
    def encode_prompt(self, prompt: str) -> np.ndarray:
        """ç¼–ç æç¤ºè¯ä¸ºå‘é‡"""
        embedding = self.model.encode([prompt])
        return embedding[0] / np.linalg.norm(embedding[0])  # å½’ä¸€åŒ–
        
    def add_to_cache(self, prompt: str, llm_string: str, response: str):
        """æ·»åŠ åˆ°è¯­ä¹‰ç¼“å­˜"""
        embedding = self.encode_prompt(prompt)
        key = f"{prompt}:{llm_string}"
        
        if self.index_type == "faiss":
            self.index.add(embedding.reshape(1, -1))
            idx = self.index.ntotal - 1
        else:
            self.embeddings.append(embedding)
            self.prompts.append(key)
            idx = len(self.embeddings) - 1
            
        self.cache_map[idx] = {
            "prompt": prompt,
            "llm_string": llm_string,
            "response": response,
            "embedding": embedding
        }
        
    def find_similar(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„ç¼“å­˜"""
        query_embedding = self.encode_prompt(prompt)
        
        if self.index_type == "faiss" and self.index.ntotal > 0:
            # ä½¿ç”¨FAISSè¿›è¡Œå¿«é€Ÿæœç´¢
            scores, indices = self.index.search(
                query_embedding.reshape(1, -1), k=1
            )
            
            if scores[0][0] >= self.similarity_threshold:
                idx = indices[0][0]
                cached_item = self.cache_map[idx]
                if cached_item["llm_string"] == llm_string:
                    return cached_item["response"]
                    
        elif self.embeddings:
            # ä½¿ç”¨sklearnè®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(
                [query_embedding], 
                self.embeddings
            )[0]
            
            max_idx = np.argmax(similarities)
            if similarities[max_idx] >= self.similarity_threshold:
                cached_item = self.cache_map[max_idx]
                if cached_item["llm_string"] == llm_string:
                    return cached_item["response"]
                    
        return None

# LangChainé›†æˆ
class LangChainSemanticCache:
    """LangChainè¯­ä¹‰ç¼“å­˜é€‚é…å™¨"""
    
    def __init__(self, semantic_cache: SemanticCache):
        self.semantic_cache = semantic_cache
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """è¯­ä¹‰æŸ¥æ‰¾"""
        return self.semantic_cache.find_similar(prompt, llm_string)
        
    def update(self, prompt: str, llm_string: str, return_val: str) -> None:
        """è¯­ä¹‰æ›´æ–°"""
        self.semantic_cache.add_to_cache(prompt, llm_string, return_val)
```

---

## GPTCache - ç¬¬ä¸‰æ–¹LLMç¼“å­˜

### ğŸ”— GPTCacheé›†æˆ

```python
from gptcache import Cache
from gptcache.manager import CacheBase, VectorBase, get_data_manager
from gptcache.processor.pre import get_prompt
from gptcache.adapter.api import init_similar_cache
from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation
import openai

class LangChainGPTCache:
    """LangChain GPTCacheé›†æˆ"""
    
    def __init__(self, 
                 cache_dir: str = "./gptcache",
                 model_name: str = "text-embedding-ada-002"):
        
        # åˆå§‹åŒ–GPTCache
        self.cache = Cache()
        
        # é…ç½®æ•°æ®ç®¡ç†å™¨
        data_manager = get_data_manager(
            CacheBase("sqlite", sql_url=f"sqlite:///{cache_dir}/cache.db"),
            VectorBase("faiss", dimension=1536)  # OpenAI embeddingç»´åº¦
        )
        
        # åˆå§‹åŒ–ç¼“å­˜
        init_similar_cache(
            cache_dir=cache_dir,
            data_manager=data_manager,
            evaluation=SearchDistanceEvaluation(),
            pre_func=get_prompt
        )
        
        # é…ç½®OpenAI
        openai.api_key = "your-api-key"
        
    def query_with_cache(self, prompt: str, model: str = "gpt-3.5-turbo", **kwargs):
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        
        # ä½¿ç”¨GPTCacheè£…é¥°å™¨
        @self.cache
        def _generate_response(prompt, **kwargs):
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            return response.choices[0].message.content
            
        return _generate_response(prompt, **kwargs)
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "hit_count": self.cache.hits,
            "miss_count": self.cache.misses,
            "hit_rate": self.cache.hits / (self.cache.hits + self.cache.misses) if self.cache.hits + self.cache.misses > 0 else 0
        }

# LangChainé€‚é…å™¨
class LangChainGPTCacheAdapter:
    """GPTCache LangChainé€‚é…å™¨"""
    
    def __init__(self, gpt_cache: LangChainGPTCache):
        self.gpt_cache = gpt_cache
        
    def lookup(self, prompt: str, llm_string: str) -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜"""
        try:
            # GPTCacheä¼šè‡ªåŠ¨å¤„ç†æŸ¥æ‰¾
            return None  # GPTCacheåœ¨å†…éƒ¨å¤„ç†
        except Exception:
            return None
            
    def update(self, prompt: str, llm_string: str, return_val: str) -> None:
        """æ›´æ–°ç¼“å­˜"""
        # GPTCacheè‡ªåŠ¨æ›´æ–°
        pass
```

---

## ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨

### ğŸ¯ å¤šçº§ç¼“å­˜ç®¡ç†

```python
from typing import Dict, Any, Optional, List
import asyncio
from datetime import datetime
import logging

class UnifiedCacheManager:
    """ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.caches = {}
        self.cache_hierarchy = [
            "memory",      # L1
            "sqlite",      # L2
            "mysql",       # L3
            "redis",       # L4
            "mongodb",     # L5
            "semantic",    # L6
            "gptcache"     # L7
        ]
        
        self._init_caches()
        self.logger = logging.getLogger(__name__)
        
    def _init_caches(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç¼“å­˜"""
        cache_configs = {
            "memory": LangChainMemoryCache,
            "sqlite": LangChainSQLiteCache,
            "mysql": LangChainMySQLCache,
            "redis": LangChainRedisCache,
            "mongodb": LangChainMongoDBCache,
            "semantic": LangChainSemanticCache,
            "gptcache": LangChainGPTCacheAdapter
        }
        
        for cache_type in self.cache_hierarchy:
            if cache_type in self.config:
                cache_class = cache_configs[cache_type]
                self.caches[cache_type] = cache_class(**self.config[cache_type])
                
    async def get(self, prompt: str, llm_string: str) -> Optional[str]:
        """å¤šçº§ç¼“å­˜è·å–"""
        start_time = datetime.now()
        
        for cache_type in self.cache_hierarchy:
            if cache_type not in self.caches:
                continue
                
            try:
                result = self.caches[cache_type].lookup(prompt, llm_string)
                if result:
                    # ç¼“å­˜å‘½ä¸­ï¼Œå›å†™ä¸Šå±‚ç¼“å­˜
                    await self._backfill_cache(prompt, llm_string, result, cache_type)
                    
                    self.logger.info(
                        f"Cache hit at {cache_type}: {prompt[:50]}..."
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡
                    await self._update_stats(cache_type, "hit")
                    return result
                    
            except Exception as e:
                self.logger.error(f"Cache {cache_type} error: {e}")
                await self._update_stats(cache_type, "error")
                
        # æ‰€æœ‰ç¼“å­˜æœªå‘½ä¸­
        self.logger.info(f"Cache miss for: {prompt[:50]}...")
        return None
        
    async def set(self, prompt: str, llm_string: str, value: str,
                  metadata: Optional[Dict] = None) -> None:
        """è®¾ç½®æ‰€æœ‰å±‚çº§çš„ç¼“å­˜"""
        
        tasks = []
        for cache_type in self.cache_hierarchy:
            if cache_type in self.caches:
                task = asyncio.create_task(
                    self._set_cache_async(cache_type, prompt, llm_string, value, metadata)
                )
                tasks.append(task)
                
        await asyncio.gather(*tasks, return_exceptions=True)
        
    async def _set_cache_async(self, cache_type: str, prompt: str, 
                               llm_string: str, value: str, metadata: Dict):
        """å¼‚æ­¥è®¾ç½®ç¼“å­˜"""
        try:
            cache = self.caches[cache_type]
            if hasattr(cache, 'update'):
                cache.update(prompt, llm_string, value, metadata)
            else:
                cache.set(prompt, llm_string, value, metadata)
        except Exception as e:
            self.logger.error(f"Failed to set cache {cache_type}: {e}")
            
    async def _backfill_cache(self, prompt: str, llm_string: str, 
                              value: str, hit_cache_type: str):
        """ç¼“å­˜å›å†™ä¸Šå±‚"""
        hit_index = self.cache_hierarchy.index(hit_cache_type)
        
        tasks = []
        for i in range(hit_index):
            cache_type = self.cache_hierarchy[i]
            if cache_type in self.caches:
                task = asyncio.create_task(
                    self._set_cache_async(cache_type, prompt, llm_string, value, {})
                )
                tasks.append(task)
                
        await asyncio.gather(*tasks, return_exceptions=True)
        
    async def _update_stats(self, cache_type: str, event_type: str):
        """æ›´æ–°ç¼“å­˜ç»Ÿè®¡"""
        # å®ç°ç»Ÿè®¡é€»è¾‘
        pass
        
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        for cache_type, cache in self.caches.items():
            try:
                if hasattr(cache, 'clear'):
                    cache.clear()
            except Exception as e:
                self.logger.error(f"Failed to clear {cache_type}: {e}")
                
    def get_health_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜å¥åº·çŠ¶æ€"""
        status = {}
        for cache_type, cache in self.caches.items():
            try:
                if hasattr(cache, 'get_stats'):
                    status[cache_type] = cache.get_stats()
                else:
                    status[cache_type] = {"status": "active"}
            except Exception as e:
                status[cache_type] = {"status": "error", "error": str(e)}
        return status

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # é…ç½®
    cache_config = {
        "memory": {"max_size": 1000},
        "sqlite": {"db_path": "./langchain_cache.db"},
        "redis": {"host": "localhost", "port": 6379},
        "mongodb": {"connection_string": "mongodb://localhost:27017/"},
        "semantic": {"similarity_threshold": 0.85}
    }
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = UnifiedCacheManager(cache_config)
    
    # ä½¿ç”¨
    prompt = "What is LangChain?"
    llm_string = "openai-gpt-3.5-turbo"
    
    # æ£€æŸ¥ç¼“å­˜
    cached_result = await manager.get(prompt, llm_string)
    
    if not cached_result:
        # è°ƒç”¨LLM
        response = "LangChain is a framework..."
        
        # å­˜å‚¨åˆ°æ‰€æœ‰ç¼“å­˜
        await manager.set(prompt, llm_string, response)
        
    # è·å–å¥åº·çŠ¶æ€
    health = manager.get_health_status()
    print(health)

if __name__ == "__main__":
    asyncio.run(main())
```

### ğŸ“Š ç›‘æ§ä¸å‘Šè­¦

```python
class CacheMonitor:
    """ç¼“å­˜ç›‘æ§å™¨"""
    
    def __init__(self, manager: UnifiedCacheManager):
        self.manager = manager
        self.metrics = {
            "hit_rates": {},
            "response_times": {},
            "error_counts": {}
        }
        
    async def collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç›‘æ§æŒ‡æ ‡"""
        health = self.manager.get_health_status()
        
        # è®¡ç®—å‘½ä¸­ç‡
        for cache_type in self.manager.caches.keys():
            stats = health.get(cache_type, {})
            if "hit_count" in stats and "miss_count" in stats:
                total = stats["hit_count"] + stats["miss_count"]
                hit_rate = stats["hit_count"] / total if total > 0 else 0
                self.metrics["hit_rates"][cache_type] = hit_rate
                
        return {
            "timestamp": datetime.now().isoformat(),
            "hit_rates": self.metrics["hit_rates"],
            "health_status": health
        }
        
    def generate_alerts(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‘Šè­¦"""
        alerts = []
        
        for cache_type, hit_rate in self.metrics["hit_rates"].items():
            if hit_rate < 0.1:  # å‘½ä¸­ç‡ä½äº10%
                alerts.append({
                    "type": "low_hit_rate",
                    "cache_type": cache_type,
                    "hit_rate": hit_rate,
                    "message": f"Cache {cache_type} hit rate is too low"
                })
                
        return alerts
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### ğŸ¯ å®æ–½å»ºè®®

1. **å¼€å‘ç¯å¢ƒ**ï¼š
   - ä½¿ç”¨å†…å­˜ç¼“å­˜ + SQLite
   - å¿«é€ŸåŸå‹éªŒè¯

2. **ç”Ÿäº§ç¯å¢ƒ**ï¼š
   - å®Œæ•´ä¸ƒå±‚ç¼“å­˜æ¶æ„
   - Redisé›†ç¾¤ + MongoDB
   - è¯­ä¹‰ç¼“å­˜ä¼˜åŒ–

3. **ç›‘æ§å‘Šè­¦**ï¼š
   - å®æ—¶ç›‘æ§å‘½ä¸­ç‡
   - è‡ªåŠ¨æ•…éšœè½¬ç§»
   - æ€§èƒ½æŒ‡æ ‡åˆ†æ

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

```mermaid
graph LR
    A[æ€§èƒ½æµ‹è¯•] --> B[åŸºå‡†æµ‹è¯•]
    B --> C[è´Ÿè½½æµ‹è¯•]
    C --> D[å‹åŠ›æµ‹è¯•]
    D --> E[ä¼˜åŒ–è°ƒæ•´]
    E --> F[ç”Ÿäº§éƒ¨ç½²]
```

è¿™å¥—ç¼“å­˜æŠ€æœ¯æ–¹æ¡ˆä¸ºæ‚¨çš„LangChainåº”ç”¨æä¾›äº†ä»å¼€å‘åˆ°ç”Ÿäº§çš„å®Œæ•´ç¼“å­˜è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒæŒ‰éœ€æ‰©å±•å’Œæ™ºèƒ½ä¼˜åŒ–ã€‚
        